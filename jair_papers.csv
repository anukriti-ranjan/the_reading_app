title,abstract,year
the computational complexity of relu network training parameterized by data dimensionality,"understanding the computational complexity of training simple neural networks with rectified linear units (relus) has recently been a subject of intensive research. closing gaps and complementing results from the literature, we present several results on the parameterized complexity of training two-layer relu networks with respect to various loss functions. after a brief discussion of other parameters, we focus on analyzing the influence of the dimension d of the training data on the computational complexity. we provide running time lower bounds in terms of w[1]-hardness for parameter d and prove that known brute-force strategies are essentially optimal (assuming the exponential time hypothesis). in comparison with previous work, our results hold for a broad(er) range of loss functions, including lp-loss for all p ∈ [0, ∞]. in particular, we improve a known polynomial-time algorithm for constant d and convex loss functions to a more general class of loss functions, matching our running time lower bounds also in these cases.",2022
synthesis and properties of optimally value-aligned normative systems,"the value alignment problem is concerned with the design of systems that provably abide by our human values. one approach to this challenge is through the leverage of prescriptive norms that, if carefully designed, are able to steer a multiagent system away from harmful outcomes and towards more beneficial ones. in this work, we first present a general methodology for the automated synthesis of value aligned normative systems, based on a consequentialist view of values. in the second part, we provide analytical tools to examine such value aligned normative systems, namely the shapley value of individual norms and the compatibility of several values under a fixed set of norms. we illustrate all of our contributions with a running example of a society of agents where taxes are collected and redistributed according to a set of parametrised norms.",2022
c-face: using compare face on face hallucination for low-resolution face recognition,"face hallucination is a task of generating high-resolution (hr) face images from low-resolution (lr) inputs, which is a subfield of the general image super-resolution. however, most of the previous methods only consider the visual effect, ignoring how to maintain the identity of the face. in this work, we propose a novel face hallucination model, called c-face network, which can generate hr images with high visual quality while preserving the identity information. a face recognition network is used to extract the identity features in the training process. in order to make the reconstructed face images keep the identity information to a great extent, a novel metric, i.e., c-face loss, is proposed. we also propose a new training algorithm to deal with the convergence problem. moreover, since our work mainly focuses on the recognition accuracy of the output, we integrate face recognition into the face hallucination process which ensures that the model can be used in real scenarios. extensive experiments on two large scale face datasets demonstrate that our c-face network has the best performance compared with other state-of-the-art methods.",2022
threshold treewidth and hypertree width,"treewidth and hypertree width have proven to be highly successful structural parameters in the context of the constraint satisfaction problem (csp). when either of these parameters is bounded by a constant, then csp becomes solvable in polynomial time. however, here the order of the polynomial in the running time depends on the width, and this is known to be unavoidable; therefore, the problem is not fixed-parameter tractable parameterized by either of these width measures. here we introduce an enhancement of tree and hypertree width through a novel notion of thresholds, allowing the associated decompositions to take into account information about the computational costs associated with solving the given csp instance. aside from introducing these notions, we obtain efficient theoretical as well as empirical algorithms for computing threshold treewidth and hypertree width and show that these parameters give rise to fixed-parameter algorithms for csp as well as other, more general problems. we complement our theoretical results with experimental evaluations in terms of heuristics as well as exact methods based on sat/smt encodings.",2022
classical planning in deep latent space,"current domain-independent, classical planners require symbolic models of the problem domain and instance as input, resulting in a knowledge acquisition bottleneck. meanwhile, although deep learning has achieved significant success in many fields, the knowledge is encoded in a subsymbolic representation which is incompatible with symbolic systems such as planners. we propose latplan, an unsupervised architecture combining deep learning and classical planning. given only an unlabeled set of image pairs showing a subset of transitions allowed in the environment (training inputs), latplan learns a complete propositional pddl action model of the environment. later, when a pair of images representing the initial and the goal states (planning inputs) is given, latplan finds a plan to the goal state in a symbolic latent space and returns a visualized plan execution. we evaluate latplan using image-based versions of 6 planning domains: 8-puzzle, 15-puzzle, blocksworld, sokoban and two variations of lightsout.",2022
joint optimization of concave scalarized multi-objective reinforcement learning with policy gradient based algorithm,"many engineering problems have multiple objectives, and the overall aim is to optimize a non-linear function of these objectives. in this paper, we formulate the problem of maximizing a non-linear concave function of multiple long-term objectives. a policy-gradient based model-free algorithm is proposed for the problem. to compute an estimate of the gradient, an asymptotically biased estimator is proposed. the proposed algorithm is shown to achieve convergence to within an ε of the global optima after sampling o(m4 σ2/(1-γ)8ε4) trajectories where γ is the discount factor and m is the number of the agents, thus achieving the same dependence on ε as the policy gradient algorithm for the standard reinforcement learning.",2022
better decision heuristics in cdcl through local search and target phases,"on practical applications, state-of-the-art sat solvers dominantly use the conflict-driven clause learning (cdcl) paradigm. an alternative for satisfiable instances is local search solvers, which is more successful on random and hard combinatorial instances. although there have been attempts to combine these methods in one framework, a tight integration which improves the state of the art on a broad set of application instances has been missing. we present a combination of techniques that achieves such an improvement. our first contribution is to maximize in a local search fashion the assignment trail in cdcl, by sticking to and extending promising assignments via a technique called target phases. second, we relax the cdcl framework by again extending promising branches to complete assignments while ignoring conflicts. these assignments are then used as starting point of local search which tries to find improved assignments with fewer unsatisfied clauses. third, these improved assignments are imported back to the cdcl loop where they are used to determine the value assigned to decision variables. finally, the conflict frequency of variables in local search can be exploited during variable selection in branching heuristics of cdcl. we implemented these techniques to improve three representative cdcl solvers (glucose, maplelcm distchronobt, and kissat). experiments on benchmarks from the main tracks of the last three sat competitions from 2019 to 2021 and an additional benchmark set from spectrum allocation show that the techniques bring significant improvements, particularly and not surprisingly, on satisfiable real-world application instances. we claim that these techniques were essential to the large increase in performance witnessed in the sat competition 2020 where kissat and relaxed lcmdcbdl newtech were leading the field followed by cryptominisat-ccnr, which also incorporated similar ideas.",2022
improving simulated annealing for clique partitioning problems,"the clique partitioning problem (cpp) is essential in graph theory with a number of important applications. due to its np-hardness, efficient algorithms for solving this problem are very crucial for practical purposes, and simulated annealing is proved to be effective in state-of-the-art cpp algorithms. however, to make simulated annealing more efficient to solve large-scale cpps, in this paper, we propose a new iterated simulated annealing algorithm. several methods are proposed in our algorithm to improve simulated annealing. first, a new configuration checking strategy based on timestamp is presented and incorporated into simulated annealing to avoid search cycles. afterwards, to enhance the local search ability of simulated annealing and speed up convergence, we combine our simulated annealing with a descent search method to solve the cpp. this method further improves solutions found by simulated annealing, and thus compensates for the local search effect. to further accelerate the convergence speed, we introduce a shrinking factor to decline initial temperature and then propose an iterated local search algorithm based on simulated annealing. additionally, a restart strategy is adopted when the search procedure converges. extensive experiments on benchmark instances of the cpp were carried out, and the results suggest that the proposed simulated annealing algorithm outperforms all the existing heuristic algorithms, including five state-of-the-art algorithms. thus the best-known solutions for 34 instances out of 94 are updated. we also conduct comparative analyses of the proposed strategies and show their effectiveness.",2022
metric-distortion bounds under limited information,"in this work, we study the metric distortion problem in voting theory under a limited amount of ordinal information. our primary contribution is threefold. first, we consider mechanisms that perform a sequence of pairwise comparisons between candidates. we show that a popular deterministic mechanism employed in many knockout phases yields distortion o(log m) while eliciting only m − 1 out of the θ(m2 ) possible pairwise comparisons, where m represents the number of candidates. our analysis for this mechanism leverages a powerful technical lemma developed by kempe (aaai ‘20). we also provide a matching lower bound on its distortion. in contrast, we prove that any mechanism which performs fewer than m−1 pairwise comparisons is destined to have unbounded distortion. moreover, we study the power of deterministic mechanisms under incomplete rankings. most notably, when agents provide their k-top preferences we show an upper bound of 6m/k + 1 on the distortion, for any k ∈ {1, 2, . . . , m}. thus, we substantially improve over the previous bound of 12m/k established by kempe (aaai ‘20), and we come closer to matching the best-known lower bound. finally, we are concerned with the sample complexity required to ensure near-optimal distortion with high probability. our main contribution is to show that a random sample of θ(m/ϵ2 ) voters suffices to guarantee distortion 3 + ϵ with high probability, for any sufficiently small ϵ &gt; 0. this result is based on analyzing the sensitivity of the deterministic mechanism introduced by gkatzelis, halpern, and shah (focs ‘20). importantly, all of our sample-complexity bounds are distribution-independent. from an experimental standpoint, we present several empirical findings on real-life voting applications, comparing the scoring systems employed in practice with a mechanism explicitly minimizing (metric) distortion. interestingly, for our case studies, we find that the winner in the actual competition is typically the candidate who minimizes the distortion.",2022
recursion in abstract argumentation is hard --- on the complexity of semantics based on weak admissibility,"we study the computational complexity of abstract argumentation semantics based on weak admissibility, a recently introduced concept to deal with arguments of self-defeating nature. our results reveal that semantics based on weak admissibility are of much higher complexity (under typical assumptions) compared to all argumentation semantics which have been analysed in terms of complexity so far. in fact, we show pspace-completeness of all non-trivial standard decision problems for weak-admissible based semantics. we then investigate potential tractable fragments and show that restricting the frameworks under consideration to certain graph-classes significantly reduces the complexity. we also show that weak-admissibility based extensions can be computed by dividing the given graph into its strongly connected components (sccs). this technique ensures that the bottleneck when computing extensions is the size of the largest scc instead of the size of the graph itself and therefore contributes to the search for fixed-parameter tractable implementations for reasoning with weak admissibility.",2022
crossing the conversational chasm: a primer on natural language processing for multilingual task-oriented dialogue systems,"in task-oriented dialogue (tod), a user holds a conversation with an artificial agent with the aim of completing a concrete task. although this technology represents one of the central objectives of ai and has been the focus of ever more intense research and development efforts, it is currently limited to a few narrow domains (e.g., food ordering, ticket booking) and a handful of languages (e.g., english, chinese). this work provides an extensive overview of existing methods and resources in multilingual tod as an entry point to this exciting and emerging field. we find that the most critical factor preventing the creation of truly multilingual tod systems is the lack of datasets in most languages for both training and evaluation. in fact, acquiring annotations or human feedback for each component of modular systems or for data-hungry end-to-end systems is expensive and tedious. hence, state-of-the-art approaches to multilingual tod mostly rely on (zero- or few-shot) cross-lingual transfer from resource-rich languages (almost exclusively english), either by means of (i) machine translation or (ii) multilingual representations. these approaches are currently viable only for typologically similar languages and languages with parallel / monolingual corpora available. on the other hand, their effectiveness beyond these boundaries is doubtful or hard to assess due to the lack of linguistically diverse benchmarks (especially for natural language generation and end-to-end evaluation). to overcome this limitation, we draw parallels between components of the tod pipeline and other nlp tasks, which can inspire solutions for learning in low-resource scenarios. finally, we list additional challenges that multilinguality poses for related areas (such as speech, fluency in generated text, and human-centred evaluation), and indicate future directions that hold promise to further expand language coverage and dialogue capabilities of current tod systems.",2022
hebo: pushing the limits of sample-efficient hyper-parameter optimisation,"in this work we rigorously analyse assumptions inherent to black-box optimisation hyper-parameter tuning tasks. our results on the bayesmark benchmark indicate that heteroscedasticity and non-stationarity pose significant challenges for black-box optimisers. based on these findings, we propose a heteroscedastic and evolutionary bayesian optimisation solver (hebo). hebo performs non-linear input and output warping, admits exact marginal log-likelihood optimisation and is robust to the values of learned parameters. we demonstrate hebo’s empirical efficacy on the neurips 2020 black-box optimisation challenge, where hebo placed first. upon further analysis, we observe that hebo significantly outperforms existing black-box optimisers on 108 machine learning hyperparameter tuning tasks comprising the bayesmark benchmark. our findings indicate that the majority of hyper-parameter tuning tasks exhibit heteroscedasticity and non-stationarity, multiobjective acquisition ensembles with pareto front solutions improve queried configurations, and robust acquisition maximisers afford empirical advantages relative to their non-robust counterparts. we hope these findings may serve as guiding principles for practitioners of bayesian optimisation.",2022
learning bayesian networks under sparsity constraints: a parameterized complexity analysis,"we study the problem of learning the structure of an optimal bayesian network when additional constraints are posed on the network or on its moralized graph. more precisely, we consider the constraint that the network or its moralized graph are close, in terms of vertex or edge deletions, to a sparse graph class π. for example, we show that learning an optimal network whose moralized graph has vertex deletion distance at most k from a graph with maximum degree 1 can be computed in polynomial time when k is constant. this extends previous work that gave an algorithm with such a running time for the vertex deletion distance to edgeless graphs. we then show that further extensions or improvements are presumably impossible. for example, we show that learning optimal networks where the network or its moralized graph have maximum degree 2 or connected components of size at most c, c ≥ 3, is np-hard. finally, we show that learning an optimal network with at most k edges in the moralized graph presumably has no f(k) · |i|o(1)-time algorithm and that, in contrast, an optimal network with at most k arcs can be computed in 2o(k) · |i|o(1) time where |i| is the total input size.",2022
collie: continual learning of language grounding from language-image embeddings,"this paper presents collie: a simple, yet effective model for continual learning of how language is grounded in vision. given a pre-trained multimodal embedding model, where language and images are projected in the same semantic space (in this case clip by openai), collie learns a transformation function that adjusts the language embeddings when needed to accommodate new language use. this is done by predicting the difference vector that needs to be applied, as well as a scaling factor for this vector, so that the adjustment is only applied when needed. unlike traditional few-shot learning, the model does not just learn new classes and labels, but can also generalize to similar language use and leverage semantic compositionality. we verify the model’s performance on two different tasks of identifying the targets of referring expressions, where it has to learn new language use. the results show that the model can efficiently learn and generalize from only a few examples, with little interference with the model’s original zero-shot performance.",2022
autotelic agents with intrinsically motivated goal-conditioned reinforcement learning: a short survey,"building autonomous machines that can explore open-ended environments, discover possible interactions and build repertoires of skills is a general objective of artificial intelligence. developmental approaches argue that this can only be achieved by autotelic agents: intrinsically motivated learning agents that can learn to represent, generate, select and solve their own problems. in recent years, the convergence of developmental approaches with deep reinforcement learning (rl) methods has been leading to the emergence of a new field: developmental reinforcement learning. developmental rl is concerned with the use of deep rl algorithms to tackle a developmental problem— the intrinsically motivated acquisition of open-ended repertoires of skills. the self-generation of goals requires the learning of compact goal encodings as well as their associated goal-achievement functions. this raises new challenges compared to standard rl algorithms originally designed to tackle pre-defined sets of goals using external reward signals. the present paper introduces developmental rl and proposes a computational framework based on goal-conditioned rl to tackle the intrinsically motivated skills acquisition problem. it proceeds to present a typology of the various goal representations used in the literature, before reviewing existing methods to learn to represent and prioritize goals in autonomous systems. we finally close the paper by discussing some open challenges in the quest of intrinsically motivated skills acquisition.",2022
evolutionary dynamics and phi-regret minimization in games,"regret has been established as a foundational concept in online learning, and likewise has important applications in the analysis of learning dynamics in games. regret quantifies the difference between a learner’s performance against a baseline in hindsight. it is well known that regret-minimizing algorithms converge to certain classes of equilibria in games; however, traditional forms of regret used in game theory predominantly consider baselines that permit deviations to deterministic actions or strategies. in this paper, we revisit our understanding of regret from the perspective of deviations over partitions of the full mixed strategy space (i.e., probability distributions over pure strategies), under the lens of the previously-established φ-regret framework, which provides a continuum of stronger regret measures. importantly, φ-regret enables learning agents to consider deviations from and to mixed strategies, generalizing several existing notions of regret such as external, internal, and swap regret, and thus broadening the insights gained from regret-based analysis of learning algorithms. we prove here that the well-studied evolutionary learning algorithm of replicator dynamics (rd) seamlessly minimizes the strongest possible form of φ-regret in generic 2 × 2 games, without any modification of the underlying algorithm itself. we subsequently conduct experiments validating our theoretical results in a suite of 144 2 × 2 games wherein rd exhibits a diverse set of behaviors. we conclude by providing empirical evidence of φ-regret minimization by rd in some larger games, hinting at further opportunity for φ-regret based study of such algorithms from both a theoretical and empirical perspective.",2022
a comprehensive framework for learning declarative action models,"a declarative action model is a compact representation of the state transitions of dynamic systems that generalizes over world objects. the specification of declarative action models is often a complex hand-crafted task. in this paper we formulate declarative action models via state constraints, and present the learning of such models as a combinatorial search. the comprehensive framework presented here allows us to connect the learning of declarative action models to well-known problem solving tasks. in addition, our framework allows us to characterize the existing work in the literature according to four dimensions: (1) the target action models, in terms of the state transitions they define; (2) the available learning examples; (3) the functions used to guide the learning process, and to evaluate the quality of the learned action models; (4) the learning algorithm. last, the paper lists relevant successful applications of the learning of declarative actions models and discusses some open challenges with the aim of encouraging future research work.",2022
supervised visual attention for simultaneous multimodal machine translation,"there has been a surge in research in multimodal machine translation (mmt), where additional modalities such as images are used to improve translation quality of textual systems. a particular use for such multimodal systems is the task of simultaneous machine translation, where visual context has been shown to complement the partial information provided by the source sentence, especially in the early phases of translation. in this paper, we propose the first transformer-based simultaneous mmt architecture, which has not been previously explored in simultaneous translation. additionally, we extend this model with an auxiliary supervision signal that guides the visual attention mechanism using labelled phrase-region alignments. we perform comprehensive experiments on three language directions and conduct thorough quantitative and qualitative analyses using both automatic metrics and manual inspection. our results show that (i) supervised visual attention consistently improves the translation quality of the simultaneous mmt models, and (ii) fine-tuning the mmt with supervision loss enabled leads to better performance than training the mmt from scratch. compared to the state-of-the-art, our proposed model achieves improvements of up to 2.3 bleu and 3.5 meteor points.",2022
two-phase multi-document event summarization on core event graphs,"succinct event description based on multiple documents is critical to news systems as well as search engines. different from existing summarization or event tasks, multi-document event summarization (mes) aims at the query-level event sequence generation, which has extra constraints on event expression and conciseness. identifying and summarizing the key event from a set of related articles is a challenging task that has not been sufficiently studied, mainly because online articles exhibit characteristics of redundancy and sparsity, and a perfect event summarization needs high level information fusion among diverse sentences and articles. to address these challenges, we propose a two-phase framework for the mes task, that first performs event semantic graph construction and dominant event detection via graph-sequence matching, then summarizes the extracted key event by an event-aware pointer generator. for experiments in the new task, we construct two large-scale real-world datasets for training and assessment. extensive evaluations show that the proposed framework significantly outperforms the related baseline methods, with the most dominant event of the articles effectively identified and correctly summarized.",2022
impact of imputation strategies on fairness in machine learning,"research on fairness and bias mitigation in machine learning often uses a set of reference datasets for the design and evaluation of novel approaches or definitions. while these datasets are well structured and useful for the comparison of various approaches, they do not reflect that datasets commonly used in real-world applications can have missing values. when such missing values are encountered, the use of imputation strategies is commonplace. however, as imputation strategies potentially alter the distribution of data they can also affect the performance, and potentially the fairness, of the resulting predictions, a topic not yet well understood in the fairness literature. in this article, we investigate the impact of different imputation strategies on classical performance and fairness in classification settings. we find that the selected imputation strategy, along with other factors including the type of classification algorithm, can significantly affect performance and fairness outcomes. the results of our experiments indicate that the choice of imputation strategy is an important factor when considering fairness in machine learning. we also provide some insights and guidance for researchers to help navigate imputation approaches for fairness.",2022
admissibility in probabilistic argumentation,"abstract argumentation is a prominent reasoning framework. it comes with a variety of semantics and has lately been enhanced by probabilities to enable a quantitative treatment of argumentation. while admissibility is a fundamental notion for classical reasoning in abstract argumentation frameworks, it has barely been reflected so far in the probabilistic setting. in this paper, we address the quantitative treatment of abstract argumentation based on probabilistic notions of admissibility. our approach follows the natural idea of defining probabilistic semantics for abstract argumentation by systematically imposing constraints on the joint probability distribution on the sets of arguments, rather than on probabilities of single arguments. as a result, there might be either a uniquely defined distribution satisfying the constraints, but also none, many, or even an infinite number of satisfying distributions are possible. we provide probabilistic semantics corresponding to the classical complete and stable semantics and show how labeling schemes provide a bridge from distributions back to argument labelings. in relation to existing work on probabilistic argumentation, we present a taxonomy of semantic notions. enabled by the constraint-based approach, standard reasoning problems for probabilistic semantics can be tackled by smt solvers, as we demonstrate by a proof-of-concept implementation.",2022
path counting for grid-based navigation,"counting the number of shortest paths on a grid is a simple procedure with close ties to pascal’s triangle. we show how path counting can be used to select relatively direct grid paths for ai-related applications involving navigation through spatial environments. typical implementations of dijkstra’s algorithm and a* prioritize grid moves in an arbitrary manner, producing paths which stray conspicuously far from line-of-sight trajectories. we find that by counting the number of paths which traverse each vertex, then selecting the vertices with the highest counts, one obtains a path that is reasonably direct in practice and can be improved by refining the grid resolution. central dijkstra and central a* are introduced as the basic methods for computing these central grid paths. theoretical analysis reveals that the proposed grid-based navigation approach is related to an existing grid-based visibility approach, and establishes that central grid paths converge on clear sightlines as the grid spacing approaches zero. a more general property, that central paths converge on direct paths, is formulated as a conjecture.",2022
fond planning with explicit fairness assumptions,"we consider the problem of reaching a propositional goal condition in fully-observable nondeterministic (fond) planning under a general class of fairness assumptions that are given explicitly. the fairness assumptions are of the form a/b and say that state trajectories that contain infinite occurrences of an action a from a in a state s and finite occurrence of actions from b, must also contain infinite occurrences of action a in s followed by each one of its possible outcomes. the infinite trajectories that violate this condition are deemed as unfair, and the solutions are policies for which all the fair trajectories reach a goal state. we show that strong and strong-cyclic fond planning, as well as qnp planning, a planning model introduced recently for generalized planning, are all special cases of fond planning with fairness assumptions of this form which can also be combined. fond+ planning, as this form of planning is called, combines the syntax of fond planning with some of the versatility of ltl for expressing fairness constraints. a sound and complete fond+ planner is implemented by reducing fond+ planning to answer set programs, and its performance is evaluated in comparison with fond and qnp planners, and ltl synthesis tools. two other fond+ planners are introduced as well which are more scalable but are not complete.",2022
on the tractability of shap explanations,"shap explanations are a popular feature-attribution mechanism for explainable ai. they use game-theoretic notions to measure the influence of individual features on the prediction of a machine learning model. despite a lot of recent interest from both academia and industry, it is not known whether shap explanations of common machine learning models can be computed efficiently. in this paper, we establish the complexity of computing the shap explanation in three important settings. first, we consider fully-factorized data distributions, and show that the complexity of computing the shap explanation is the same as the complexity of computing the expected value of the model. this fully-factorized setting is often used to simplify the shap computation, yet our results show that the computation can be intractable for commonly used models such as logistic regression. going beyond fully-factorized distributions, we show that computing shap explanations is already intractable for a very simple setting: computing shap explanations of trivial classifiers over naive bayes distributions. finally, we show that even computing shap over the empirical distribution is #p-hard.",2022
inductive logic programming at 30: a new introduction,"inductive logic programming (ilp) is a form of machine learning. the goal of ilp is to induce a hypothesis (a set of logical rules) that generalises training examples. as ilp turns 30, we provide a new introduction to the field. we introduce the necessary logical notation and the main learning settings; describe the building blocks of an ilp system; compare several systems on several dimensions; describe four systems (aleph, tilde, aspal, and metagol); highlight key application areas; and, finally, summarise current limitations and directions for future research.",2022
cooperation and learning dynamics under wealth inequality and diversity in individual risk,"we examine how wealth inequality and diversity in the perception of risk of a collective disaster impact cooperation levels in the context of a public goods game with uncertain and non-linear returns. in this game, individuals face a collective-risk dilemma where they may contribute or not to a common pool to reduce their chances of future losses. we draw our conclusions based on social simulations with populations of independent reinforcement learners with diverse levels of risk and wealth. we find that both wealth inequality and diversity in risk assessment can hinder cooperation and augment collective losses. additionally, wealth inequality further exacerbates long term inequality, causing rich agents to become richer and poor agents to become poorer. on the other hand, diversity in risk only amplifies inequality when combined with bias in group assortment—i.e., high probability that agents from the same risk class play together. our results also suggest that taking wealth inequality into account can help to design effective policies aiming at leveraging cooperation in large group sizes, a configuration where collective action is harder to achieve. finally, we characterize the circumstances under which risk perception alignment is crucial and those under which reducing wealth inequality constitutes a deciding factor for collective welfare.",2022
planning with critical section macros: theory and practice,"macro-operators (macros) are a well-known technique for enhancing performance of planning engines by providing “short-cuts” in the state space. existing macro learning systems usually generate macros by considering most frequent action sequences in training plans. unfortunately, frequent action sequences might not capture meaningful activities as a whole, leading to a limited beneficial impact for the planning process. in this paper, inspired by resource locking in critical sections in parallel computing, we propose a technique that generates macros able to capture whole activities in which limited resources (e.g., a robotic hand, or a truck) are used. specifically, such a critical section macro starts by locking the resource (e.g., grabbing an object), continues by using the resource (e.g., manipulating the object) and finishes by releasing the resource (e.g., dropping the object). hence, such a macro bridges states in which the resource is locked and cannot be used. we also introduce versions of critical section macros dealing with multiple resources and phased locks. usefulness of macros is evaluated using a range of state-of-the-art planners, and a large number of benchmarks from the deterministic and learning tracks of recent editions of the international planning competition.",2022
fast adaptive non-monotone submodular maximization subject to a knapsack constraint,"constrained submodular maximization problems encompass a wide variety of applications, including personalized recommendation, team formation, and revenue maximization via viral marketing. the massive instances occurring in modern-day applications can render existing algorithms prohibitively slow. moreover, frequently those instances are also inherently stochastic. focusing on these challenges, we revisit the classic problem of maximizing a (possibly non-monotone) submodular function subject to a knapsack constraint. we present a simple randomized greedy algorithm that achieves a 5.83-approximation and runs in o(n log n) time, i.e., at least a factor n faster than other state-of-the-art algorithms. the versatility of our approach allows us to further transfer it to a stochastic version of the problem. there, we obtain a (9 + ε)-approximation to the best adaptive policy, which is the first constant approximation for non-monotone objectives. experimental evaluation of our algorithms showcases their improved performance on real and synthetic data.",2022
out of context: a new clue for context modeling of aspect-based sentiment analysis,"aspect-based sentiment analysis (absa) aims to predict the sentiment expressed in a review with respect to a given aspect. the core of absa is to model the interaction between the context and given aspect to extract aspect-related information. in prior work, attention mechanisms and dependency graph networks are commonly adopted to capture the relations between the context and given aspect. and the weighted sum of context hidden states is used as the final representation fed to the classifier. however, the information related to the given aspect may be already discarded and adverse information may be retained in the context modeling processes of existing models. such a problem cannot be solved by subsequent modules due to two reasons. first, their operations are conducted on the encoder-generated context hidden states, whose value cannot be changed after the encoder. second, existing encoders only consider the context while not the given aspect. to address this problem, we argue the given aspect should be considered as a new clue out of context in the context modeling process. as for solutions, we design three streams of aspect-aware context encoders: an aspect-aware lstm, an aspect-aware gcn, and three aspect-aware berts. they are dedicated to generating aspect-aware hidden states which are tailored for the absa task. in these aspect-aware context encoders, the semantics of the given aspect is used to regulate the information flow. consequently, the aspect-related information can be retained and aspect-irrelevant information can be excluded in the generated hidden states. we conduct extensive experiments on several benchmark datasets with empirical analysis, demonstrating the efficacies and advantages of our proposed aspect-aware context encoders.",2022
finding and recognizing popular coalition structures,"an important aspect of multi-agent systems concerns the formation of coalitions that are stable or optimal in some well-defined way. the notion of popularity has recently received a lot of attention in this context. a partition is popular if there is no other partition in which more agents are better off than worse off. in this paper, we study popularity, strong popularity, and mixed popularity (which is particularly attractive because existence is guaranteed by the minimax theorem) in a variety of coalition formation settings. extending previous work on marriage games, we show that mixed popular partitions in roommate games can be found efficiently via linear programming and a separation oracle. this approach is quite universal, leading to efficient algorithms for verifying whether a given partition is popular and for finding strongly popular partitions (resolving an open problem). by contrast, we prove that both problems become computationally intractable when moving from coalitions of size 2 to coalitions of size 3, even when preferences are strict and globally ranked. moreover, we show that finding popular, strongly popular, and mixed popular partitions in symmetric additively separable hedonic games and symmetric fractional hedonic games is np-hard. together, these results indicate strong boundaries to the tractability of popularity in both ordinal and cardinal models of hedonic games.",2022
automated reinforcement learning (autorl): a survey and open problems,"the combination of reinforcement learning (rl) with deep learning has led to a series of impressive feats, with many believing (deep) rl provides a path towards generally capable agents. however, the success of rl agents is often highly sensitive to design choices in the training process, which may require tedious and error-prone manual tuning. this makes it challenging to use rl for new problems and also limits its full potential. in many other areas of machine learning, automl has shown that it is possible to automate such design choices, and automl has also yielded promising initial results when applied to rl. however, automated reinforcement learning (autorl) involves not only standard applications of automl but also includes additional challenges unique to rl, that naturally produce a different set of methods. as such, autorl has been emerging as an important area of research in rl, providing promise in a variety of applications from rna design to playing games, such as go. given the diversity of methods and environments considered in rl, much of the research has been conducted in distinct subfields, ranging from meta-learning to evolution. in this survey, we seek to unify the field of autorl, provide a common taxonomy, discuss each area in detail and pose open problems of interest to researchers going forward.",2022
core challenges in embodied vision-language planning,"recent advances in the areas of multimodal machine learning and artificial intelligence (ai) have led to the development of challenging tasks at the intersection of computer vision, natural language processing, and embodied ai. whereas many approaches and previous survey pursuits have characterised one or two of these dimensions, there has not been a holistic analysis at the center of all three. moreover, even when combinations of these topics are considered, more focus is placed on describing, e.g., current architectural methods, as opposed to also illustrating high-level challenges and opportunities for the field. in this survey paper, we discuss embodied vision-language planning (evlp) tasks, a family of prominent embodied navigation and manipulation problems that jointly use computer vision and natural language. we propose a taxonomy to unify these tasks and provide an in-depth analysis and comparison of the new and current algorithmic approaches, metrics, simulated environments, as well as the datasets used for evlp tasks. finally, we present the core challenges that we believe new evlp works should seek to address, and we advocate for task construction that enables model generalizability and furthers real-world deployment.",2022
objective bayesian nets for integrating consistent datasets,"this paper addresses a data integration problem: given several mutually consistent datasets each of which measures a subset of the variables of interest, how can one construct a probabilistic model that fits the data and gives reasonable answers to questions which are under-determined by the data? here we show how to obtain a bayesian network model which represents the unique probability function that agrees with the probability distributions measured by the datasets and otherwise has maximum entropy. we provide a general algorithm, obn-cds, which offers substantial efficiency savings over the standard brute-force approach to determining the maximum entropy probability function. furthermore, we develop modifications to the general algorithm which enable further efficiency savings but which are only applicable in particular situations. we show that there are circumstances in which one can obtain the model (i) directly from the data; (ii) by solving algebraic problems; and (iii) by solving relatively simple independent optimisation problems.",2022
ordinal maximin share approximation for goods,"in fair division of indivisible goods, ℓ-out-of-d maximin share (mms) is the value that an agent can guarantee by partitioning the goods into d bundles and choosing the ℓ least preferred bundles. most existing works aim to guarantee to all agents a constant fraction of their 1-out-of-n mms. but this guarantee is sensitive to small perturbation in agents' cardinal valuations. we consider a more robust approximation notion, which depends only on the agents' ordinal rankings of bundles. weprove the existence of ℓ-out-of-⌊(ℓ + 1/2)n⌋ mms allocations of goods for any integer ℓ ≥ 1, and present a polynomial-time algorithm that finds a 1-out-of-⌈3n/2⌉ mms allocation when ℓ=1. we further develop an algorithm that provides a weaker ordinal approximation to mms for any ℓ &gt; 1.",2022
adaptive greedy versus non-adaptive greedy for influence maximization,"we consider the adaptive influence maximization problem: given a network and a budget k, iteratively select k seeds in the network to maximize the expected number of adopters. in the full-adoption feedback model, after selecting each seed, the seed-picker observes all the resulting adoptions. in the myopic feedback model, the seed-picker only observes whether each neighbor of the chosen seed adopts. motivated by the extreme success of greedy-based algorithms/heuristics for influence maximization, we propose the concept of greedy adaptivity gap, which compares the performance of the adaptive greedy algorithm to its non-adaptive counterpart. our first result shows that, for submodular influence maximization, the adaptive greedy algorithm can perform up to a (1 − 1/e)-fraction worse than the non-adaptive greedy algorithm, and that this ratio is tight. more specifically, on one side we provide examples where the performance of the adaptive greedy algorithm is only a (1−1/e) fraction of the performance of the non-adaptive greedy algorithm in four settings: for both feedback models and both the independent cascade model and the linear threshold model. on the other side, we prove that in any submodular cascade, the adaptive greedy algorithm always outputs a (1 − 1/e)-approximation to the expected number of adoptions in the optimal non-adaptive seed choice. our second result shows that, for the general submodular diffusion model with full-adoption feedback, the adaptive greedy algorithm can outperform the non-adaptive greedy algorithm by an unbounded factor. finally, we propose a risk-free variant of the adaptive greedy algorithm that always performs no worse than the non-adaptive greedy algorithm.",2022
constraint solving approaches to the business-to-business meeting scheduling problem,"the business-to-business meeting scheduling problem consists of scheduling a set of meetings between given pairs of participants to an event, while taking into account participants’ availability and accommodation capacity. a crucial aspect of this problem is that breaks in participants’ schedules should be avoided as much as possible. it constitutes a challenging combinatorial problem that needs to be solved for many real world brokerage events. in this paper we present a comparative study of constraint programming (cp), mixedinteger programming (mip) and maximum satisfiability (maxsat) approaches to this problem. the cp approach relies on using global constraints and has been implemented in minizinc to be able to compare cp, lazy clause generation and mip as solving technologies in this setting. we also present a pure mip encoding. finally, an alternative viewpoint is considered under maxsat, showing best performance when considering some implied constraints. experiments conducted on real world instances, as well as on crafted ones, show that the maxsat approach is the one with the best performance for this problem, exhibiting better solving times, sometimes even orders of magnitude smaller than cp and mip.",2022
a few queries go a long way: information-distortion tradeoffs in matching,"we consider the one-sided matching problem, where n agents have preferences over n items, and these preferences are induced by underlying cardinal valuation functions. the goal is to match every agent to a single item so as to maximize the social welfare. most of the related literature, however, assumes that the values of the agents are not a priori known, and only access to the ordinal preferences of the agents over the items is provided. consequently, this incomplete information leads to loss of efficiency, which is measured by the notion of distortion. in this paper, we further assume that the agents can answer a small number of queries, allowing us partial access to their values. we study the interplay between elicited cardinal information (measured by the number of queries per agent) and distortion for one-sided matching, as well as a wide range of well-studied related problems. qualitatively, our results show that with a limited number of queries, it is possible to obtain significant improvements over the classic setting, where only access to ordinal information is given.",2022
proactive dynamic distributed constraint optimization problems,"the distributed constraint optimization problem (dcop) formulation is a powerful tool for modeling multi-agent coordination problems. to solve dcops in a dynamic environment, dynamic dcops (d-dcops) have been proposed to model the inherent dynamism present in many coordination problems. d-dcops solve a sequence of static problems by reacting to changes in the environment as the agents observe them. such reactive approaches ignore knowledge about future changes of the problem. to overcome this limitation, we introduce proactive dynamic dcops (pd-dcops), a novel formalism to model d-dcops in the presence of exogenous uncertainty. in contrast to reactive approaches, pd-dcops are able to explicitly model possible changes of the problem and take such information into account when solving the dynamically changing problem in a proactive manner. the additional expressivity of this formalism allows it to model a wider variety of distributed optimization problems. our work presents both theoretical and practical contributions that advance current dynamic dcop models: (i) we introduce proactive dynamic dcops (pd-dcops), which explicitly model how the dcop will change over time; (ii) we develop exact and heuristic algorithms to solve pd-dcops in a proactive manner; (iii) we provide theoretical results about the complexity of this new class of dcops; and (iv) we empirically evaluate both proactive and reactive algorithms to determine the trade-offs between the two classes. the final contribution is important as our results are the first that identify the characteristics of the problems that the two classes of algorithms excel in.",2022
avoiding negative side effects of autonomous systems in the open world,"autonomous systems that operate in the open world often use incomplete models of their environment. model incompleteness is inevitable due to the practical limitations in precise model specification and data collection about open-world environments. due to the limited fidelity of the model, agent actions may produce negative side effects (nses) when deployed. negative side effects are undesirable, unmodeled effects of agent actions on the environment. nses are inherently challenging to identify at design time and may affect the reliability, usability and safety of the system. we present two complementary approaches to mitigate the nse via: (1) learning from feedback, and (2) environment shaping. the solution approaches target settings with different assumptions and agent responsibilities. in learning from feedback, the agent learns a penalty function associated with a nse. we investigate the efficiency of different feedback mechanisms, including human feedback and autonomous exploration. the problem is formulated as a multi-objective markov decision process such that optimizing the agent’s assigned task is prioritized over mitigating nse. a slack parameter denotes the maximum allowed deviation from the optimal expected reward for the agent’s task in order to mitigate nse. in environment shaping, we examine how a human can assist an agent, beyond providing feedback, and utilize their broader scope of knowledge to mitigate the impacts of nse. we formulate the problem as a human-agent collaboration with decoupled objectives. the agent optimizes its assigned task and may produce nse during its operation. the human assists the agent by performing modest reconfigurations of the environment so as to mitigate the impacts of nse, without affecting the agent’s ability to complete its assigned task. we present an algorithm for shaping and analyze its properties. empirical evaluations demonstrate the trade-offs in the performance of different approaches in mitigating nse in different settings.",2022
fair division of indivisible goods for a class of concave valuations,"we study the fair and efficient allocation of a set of indivisible goods among agents, where each good has several copies, and each agent has an additively separable concave valuation function with a threshold. these valuations capture the property of diminishing marginal returns, and they are more general than the well-studied case of additive valuations. we present a polynomial-time algorithm that approximates the optimal nash social welfare (nsw) up to a factor of e1/e ≈ 1.445. this matches with the state-of-the-art approximation factor for additive valuations. the computed allocation also satisfies the popular fairness guarantee of envy-freeness up to one good (ef1) up to a factor of 2 + ε. for instances without thresholds, it is also approximately pareto-optimal. for instances satisfying a large market property, we show an improved approximation factor. lastly, we show that the upper bounds on the optimal nsw introduced in cole and gkatzelis (2018) and barman et al. (2018) have the same value.",2022
rethinking fairness: an interdisciplinary survey of critiques of hegemonic ml fairness approaches,"this survey article assesses and compares existing critiques of current fairness-enhancing technical interventions in machine learning (ml) that draw from a range of non-computing disciplines, including philosophy, feminist studies, critical race and ethnic studies, legal studies, anthropology, and science and technology studies. it bridges epistemic divides in order to offer an interdisciplinary understanding of the possibilities and limits of hegemonic computational approaches to ml fairness for producing just outcomes for society’s most marginalized. the article is organized according to nine major themes of critique wherein these different fields intersect: 1) how ""fairness"" in ai fairness research gets defined; 2) how problems for ai systems to address get formulated; 3) the impacts of abstraction on how ai tools function and its propensity to lead to technological solutionism; 4) how racial classification operates within ai fairness research; 5) the use of ai fairness measures to avoid regulation and engage in ethics washing; 6) an absence of participatory design and democratic deliberation in ai fairness considerations; 7) data collection practices that entrench “bias,” are non-consensual, and lack transparency; 8) the predatory inclusion of marginalized groups into ai systems; and 9) a lack of engagement with ai’s long-term social and ethical outcomes. drawing from these critiques, the article concludes by imagining future ml fairness research directions that actively disrupt entrenched power dynamics and structural injustices in society.",2022
multi-agent advisor q-learning,"in the last decade, there have been significant advances in multi-agent reinforcement learning (marl) but there are still numerous challenges, such as high sample complexity and slow convergence to stable policies, that need to be overcome before wide-spread deployment is possible. however, many real-world environments already, in practice, deploy sub-optimal or heuristic approaches for generating policies. an interesting question that arises is how to best use such approaches as advisors to help improve reinforcement learning in multi-agent domains. in this paper, we provide a principled framework for incorporating action recommendations from online suboptimal advisors in multi-agent settings. we describe the problem of advising multiple intelligent reinforcement agents (admiral) in nonrestrictive general-sum stochastic game environments and present two novel q-learning based algorithms: admiral - decision making (admiral-dm) and admiral - advisor evaluation (admiral-ae), which allow us to improve learning by appropriately incorporating advice from an advisor (admiral-dm), and evaluate the effectiveness of an advisor (admiral-ae). we analyze the algorithms theoretically and provide fixed point guarantees regarding their learning in general-sum stochastic games. furthermore, extensive experiments illustrate that these algorithms: can be used in a variety of environments, have performances that compare favourably to other related baselines, can scale to large state-action spaces, and are robust to poor advice from advisors.",2022
ranking sets of objects: the complexity of avoiding impossibility results,"the problem of lifting a preference order on a set of objects to a preference order on a family of subsets of this set is a fundamental problem with a wide variety of applications in ai. the process is often guided by axioms postulating properties the lifted order should have. well-known impossibility results by kannai and peleg and by barbera and pattanaik tell us that some desirable axioms – namely dominance and (strict) independence – are not jointly satisfiable for any linear order on the objects if all non-empty sets of objects are to be ordered. on the other hand, if not all non-empty sets of objects are to be ordered, the axioms are jointly satisfiable for all linear orders on the objects for some families of sets. such families are very important for applications as they allow for the use of lifted orders, for example, in combinatorial voting. in this paper, we determine the computational complexity of recognizing such families. we show that it is \pi_2^p-complete to decide for a given family of subsets whether dominance and independence or dominance and strict independence are jointly satisfiable for all linear orders on the objects if the lifted order needs to be total. furthermore, we show that the problem remains conp-complete if the lifted order can be incomplete. additionally, we show that the complexity of these problems can increase exponentially if the family of sets is not given explicitly but via a succinct domain restriction. finally, we show that it is np-complete to decide for a family of subsets whether dominance and independence or dominance and strict independence are jointly satisfiable for at least one linear order on the objects.",2022
"online relaxation refinement for satisficing planning: on partial delete relaxation, complete hill-climbing, and novelty pruning","in classical ai planning, heuristic functions typically base their estimates on a relaxation of the input task. such relaxations can be more or less precise, and many heuristic functions have a refinement procedure that can be iteratively applied until the desired degree of precision is reached. traditionally, such refinement is performed offline to instantiate the heuristic for the search. however, a natural idea is to perform such refinement online instead, in situations where the heuristic is not sufficiently accurate. we introduce several online-refinement search algorithms, based on hill-climbing and greedy best-first search. our hill-climbing algorithms perform a bounded lookahead, proceeding to a state with lower heuristic value than the root state of the lookahead if such a state exists, or refining the heuristic otherwise to remove such a local minimum from the search space surface. these algorithms are complete if the refinement procedure satisfies a suitable convergence property. we transfer the idea of bounded lookaheads to greedy best-first search with a lightweight lookahead after each expansion, serving both as a method to boost search progress and to detect when the heuristic is inaccurate, identifying an opportunity for online refinement. we evaluate our algorithms with the partial delete relaxation heuristic hcff, which can be refined by treating additional conjunctions of facts as atomic, and whose refinement operation satisfies the convergence property required for completeness. on both the ipc domains as well as on the recently published autoscale benchmarks, our online-refinement search algorithms significantly beat state-of-the-art satisficing planners, and are competitive even with complex portfolios.",2022
jointly learning environments and control policies with projected stochastic gradient ascent,"we consider the joint design and control of discrete-time stochastic dynamical systems over a finite time horizon. we formulate the problem as a multi-step optimization problem under uncertainty seeking to identify a system design and a control policy that jointly maximize the expected sum of rewards collected over the time horizon considered. the transition function, the reward function and the policy are all parametrized, assumed known and differentiable with respect to their parameters. we then introduce a deep reinforcement learning algorithm combining policy gradient methods with model-based optimization techniques to solve this problem. in essence, our algorithm iteratively approximates the gradient of the expected return via monte-carlo sampling and automatic differentiation and takes projected gradient ascent steps in the space of environment and policy parameters. this algorithm is referred to as direct environment and policy search (deps). we assess the performance of our algorithm in three environments concerned with the design and control of a mass-spring-damper system, a small-scale off-grid power system and a drone, respectively. in addition, our algorithm is benchmarked against a state-of-the-art deep reinforcement learning algorithm used to tackle joint design and control problems. we show that deps performs at least as well or better in all three environments, consistently yielding solutions with higher returns in fewer iterations. finally, solutions produced by our algorithm are also compared with solutions produced by an algorithm that does not jointly optimize environment and policy parameters, highlighting the fact that higher returns can be achieved when joint optimization is performed.",2022
reward machines: exploiting reward function structure in reinforcement learning,"reinforcement learning (rl) methods usually treat reward functions as black boxes. as such, these methods must extensively interact with the environment in order to discover rewards and optimal policies. in most rl applications, however, users have to program the reward function and, hence, there is the opportunity to make the reward function visible – to show the reward function’s code to the rl agent so it can exploit the function’s internal structure to learn optimal policies in a more sample efficient manner. in this paper, we show how to accomplish this idea in two steps. first, we propose reward machines, a type of finite state machine that supports the specification of reward functions while exposing reward function structure. we then describe different methodologies to exploit this structure to support learning, including automated reward shaping, task decomposition, and counterfactual reasoning with off-policy learning. experiments on tabular and continuous domains, across different tasks and rl agents, show the benefits of exploiting reward structure with respect to sample efficiency and the quality of resultant policies. finally, by virtue of being a form of finite state machine, reward machines have the expressive power of a regular language and as such support loops, sequences and conditionals, as well as the expression of temporally extended properties typical of linear temporal logic and non-markovian reward specification.",2022
doubly robust crowdsourcing,"large-scale labeled dataset is the indispensable fuel that ignites the ai revolution as we see today. most such datasets are constructed using crowdsourcing services such as amazon mechanical turk which provides noisy labels from non-experts at a fair price. the sheer size of such datasets mandates that it is only feasible to collect a few labels per data point. we formulate the problem of test-time label aggregation as a statistical estimation problem of inferring the expected voting score. by imitating workers with supervised learners and using them in a doubly robust estimation framework, we prove that the variance of estimation can be substantially reduced, even if the learner is a poor approximation. synthetic and real-world experiments show that by combining the doubly robust approach with adaptive worker/item selection rules, we often need much lower label cost to achieve nearly the same accuracy as in the ideal world where all workers label all data points.",2022
preferences single-peaked on a tree: multiwinner elections and structural results,"a preference profile is single-peaked on a tree if the candidate set can be equipped with a tree structure so that the preferences of each voter are decreasing from their top candidate along all paths in the tree. this notion was introduced by demange (1982), and subsequently trick (1989b) described an efficient algorithm for deciding if a given profile is single-peaked on a tree. we study the complexity of multiwinner elections under several variants of the chamberlin–courant rule for preferences single-peaked on trees. we show that in this setting the egalitarian version of this rule admits a polynomial-time winner determination algorithm. for the utilitarian version, we prove that winner determination remains np-hard for the borda scoring function; indeed, this hardness results extends to a large family of scoring functions. however, a winning committee can be found in polynomial time if either the number of leaves or the number of internal vertices of the underlying tree is bounded by a constant. to benefit from these positive results, we need a procedure that can determine whether a given profile is single-peaked on a tree that has additional desirable properties (such as, e.g., a small number of leaves). to address this challenge, we develop a structural approach that enables us to compactly represent all trees with respect to which a given profile is single-peaked. we show how to use this representation to efficiently find the best tree for a given profile for use with our winner determination algorithms: given a profile, we can efficiently find a tree with the minimum number of leaves, or a tree with the minimum number of internal vertices among trees on which the profile is single-peaked. we then explore the power and limitations of this framework: we develop polynomial-time algorithms to find trees with the smallest maximum degree, diameter, or pathwidth, but show that it is np-hard to check whether a given profile is single-peaked on a tree that is isomorphic to a given tree, or on a regular tree.",2022
a survey of opponent modeling in adversarial domains,"opponent modeling is the ability to use prior knowledge and observations in order to predict the behavior of an opponent. this survey presents a comprehensive overview of existing opponent modeling techniques for adversarial domains, many of which must address stochastic, continuous, or concurrent actions, and sparse, partially observable payoff structures. we discuss all the components of opponent modeling systems, including feature extraction, learning algorithms, and strategy abstractions. these discussions lead us to propose a new form of analysis for describing and predicting the evolution of game states over time. we then introduce a new framework that facilitates method comparison, analyze a representative selection of techniques using the proposed framework, and highlight common trends among recently proposed methods. finally, we list several open problems and discuss future research directions inspired by ai research on opponent modeling and related research in other disciplines.",2022
explainable deep learning: a field guide for the uninitiated,"deep neural networks (dnns) are an indispensable machine learning tool despite the difficulty of diagnosing what aspects of a model’s input drive its decisions. in countless real-world domains, from legislation and law enforcement to healthcare, such diagnosis is essential to ensure that dnn decisions are driven by aspects appropriate in the context of its use. the development of methods and studies enabling the explanation of a dnn’s decisions has thus blossomed into an active and broad area of research. the field’s complexity is exacerbated by competing definitions of what it means “to explain” the actions of a dnn and to evaluate an approach’s “ability to explain”. this article offers a field guide to explore the space of explainable deep learning for those in the ai/ml field who are uninitiated. the field guide: i) introduces three simple dimensions defining the space of foundational methods that contribute to explainable deep learning, ii) discusses the evaluations for model explanations, iii) places explainability in the context of other related deep learning research areas, and iv) discusses user-oriented explanation design and future directions. we hope the guide is seen as a starting point for those embarking on this research field.",2022
automatic recognition of the general-purpose communicative functions defined by the iso 24617-2 standard for dialog act annotation,"from the perspective of a dialog system, it is important to identify the intention behind the segments in a dialog, since it provides an important cue regarding the information that is present in the segments and how they should be interpreted. iso 24617-2, the standard for dialog act annotation, defines a hierarchically organized set of general-purpose communicative functions which correspond to different intentions that are relevant in the context of a dialog. we explore the automatic recognition of these communicative functions in the dialogbank, which is a reference set of dialogs annotated according to this standard. to do so, we propose adaptations of existing approaches to flat dialog act recognition that allow them to deal with the hierarchical classification problem. more specifically, we propose the use of an end-to-end hierarchical network with cascading outputs and maximum a posteriori path estimation to predict the communicative function at each level of the hierarchy, preserve the dependencies between the functions in the path, and decide at which level to stop. furthermore, since the amount of dialogs in the dialogbank is small, we rely on transfer learning processes to reduce overfitting and improve performance. the results of our experiments show that our approach outperforms both a flat one and hierarchical approaches based on multiple classifiers and that each of its components plays an important role towards the recognition of general-purpose communicative functions.",2022
image captioning as an assistive technology: lessons learned from vizwiz 2020 challenge,"image captioning has recently demonstrated impressive progress largely owing to the introduction of neural network algorithms trained on curated dataset like ms-coco. often work in this field is motivated by the promise of deployment of captioning systems in practical applications. however, the scarcity of data and contexts in many competition datasets renders the utility of systems trained on these datasets limited as an assistive technology in real-world settings, such as helping visually impaired people navigate and accomplish everyday tasks. this gap motivated the introduction of the novel vizwiz dataset, which consists of images taken by the visually impaired and captions that have useful, task-oriented information. in an attempt to help the machine learning computer vision field realize its promise of producing technologies that have positive social impact, the curators of the vizwiz dataset host several competitions, including one for image captioning. this work details the theory and engineering from our winning submission to the 2020 captioning competition. our work provides a step towards improved assistive image captioning systems. this article appears in the special track on ai &amp; society.",2022
neural character-level syntactic parsing for chinese,"in this work, we explore character-level neural syntactic parsing for chinese with two typical syntactic formalisms: the constituent formalism and a dependency formalism based on a newly released character-level dependency treebank. prior works in chinese parsing have struggled with whether to de ne words when modeling character interactions. we choose to integrate full character-level syntactic dependency relationships using neural representations from character embeddings and richer linguistic syntactic information from human-annotated character-level parts-of-speech and dependency labels. this has the potential to better understand the deeper structure of chinese sentences and provides a better structural formalism for avoiding unnecessary structural ambiguities. specifically, we first compare two different character-level syntax annotation styles: constituency and dependency. then, we discuss two key problems for character-level parsing: (1) how to combine constituent and dependency syntactic structure in full character-level trees and (2) how to convert from character-level to word-level for both constituent and dependency trees. in addition, we also explore several other key parsing aspects, including di erent character-level dependency annotations and joint learning of parts-of-speech and syntactic parsing. finally, we evaluate our models on the chinese penn treebank (ctb) and our published shanghai jiao tong university chinese character dependency treebank (scdt). the results show the e effectiveness of our model on both constituent and dependency parsing. we further provide empirical analysis and suggest several directions for future study.",2022
casa: conversational aspect sentiment analysis for dialogue understanding,"dialogue understanding has always been a bottleneck for many conversational tasks, such as dialogue response generation and conversational question answering. to expedite the progress in this area, we introduce the task of conversational aspect sentiment analysis (casa) that can provide useful fine-grained sentiment information for dialogue understanding and planning. overall, this task extends the standard aspect-based sentiment analysis to the conversational scenario with several major adaptations. to aid the training and evaluation of data-driven methods, we annotate 3,000 chit-chat dialogues (27,198 sentences) with fine-grained sentiment information, including all sentiment expressions, their polarities and the corresponding target mentions. we also annotate an out-of-domain test set of 200 dialogues for robustness evaluation. besides, we develop multiple baselines based on either pretrained bert or self-attention for preliminary study. experimental results show that our bert-based model has strong performances for both in-domain and out-of-domain datasets, and thorough analysis indicates several potential directions for further improvements.",2022
sum-of-products with default values: algorithms and complexity results,"weighted counting for constraint satisfaction with default values (#cspd) is a powerful special case of the sum-of-products problem that admits succinct encodings of #csp, #sat, and inference in probabilistic graphical models. we investigate #cspd under the fundamental parameter of incidence treewidth (i.e., the treewidth of the incidence graph of the constraint hypergraph). we show that if the incidence treewidth is bounded, #cspd can be solved in polynomial time. more specifically, we show that the problem is fixed-parameter tractable for the combined parameter incidence treewidth, domain size, and support size (the maximum number of non-default tuples in a constraint). this generalizes known results on the fixed-parameter tractability of #cspd under the combined parameter primal treewidth and domain size. we further prove that the problem is not fixed-parameter tractable if any of the three components is dropped from the parameterization.",2022
migrating techniques from search-based multi-agent path finding solvers to sat-based approach,"in the multi-agent path finding problem (mapf) we are given a set of agents each with respective start and goal positions. the task is to find paths for all agents while avoiding collisions, aiming to minimize a given objective function. many mapf solvers were introduced in the past decade for optimizing two specific objective functions: sum-of-costs and makespan. two prominent categories of solvers can be distinguished: search-based solvers and compilation-based solvers. search-based solvers were developed and tested for the sum-of-costs objective, while the most prominent compilation-based solvers that are built around boolean satisfiability (sat) were designed for the makespan objective. very little is known on the performance and relevance of solvers from the compilation-based approach on the sum-of-costs objective. in this paper, we start to close the gap between these cost functions in the compilation-based approach. our main contribution is a new sat-based mapf solver called mdd-sat, that is directly aimed to optimally solve the mapf problem under the sum-of-costs objective function. using both a lower bound on the sum-of-costs and an upper bound on the makespan, mdd-sat is able to generate a reasonable number of boolean variables in our sat encoding. we then further improve the encoding by borrowing ideas from icts, a search-based solver. in addition, we show that concepts applicable in search-based solvers like icts and icbs are applicable in the sat-based approach as well. specifically, we integrate independence detection, a generic technique for decomposing an mapf instance into independent subproblems, into our sat-based approach, and we design a relaxation of our optimal sat-based solver that results in a bounded suboptimal sat-based solver. experimental evaluation on several domains shows that there are many scenarios where our sat-based methods outperform state-of-the-art sum-of-costs search-based solvers, such as variants of the icts and icbs algorithms.",2022
viewpoint: ethical by designer - how to grow ethical designers of artificial intelligence,"ethical concerns regarding artificial intelligence (ai) technology have fueled discussions around the ethics training received by ai designers. we claim that training designers for ethical behaviour, understood as habitual application of ethical principles in any situation, can make a significant difference in the practice of research, development, and application of ai systems. building on interdisciplinary knowledge and practical experience from computer science, moral psychology and development, and pedagogy, we propose a functional way to provide this training. this article appears in the special track on ai &amp; society.",2022
fine-grained prediction of political leaning on social media with unsupervised deep learning,"predicting the political leaning of social media users is an increasingly popular task, given its usefulness for electoral forecasts, opinion dynamics models and for studying the political dimension of polarization and disinformation. here, we propose a novel unsupervised technique for learning fine-grained political leaning from the textual content of social media posts. our technique leverages a deep neural network for learning latent political ideologies in a representation learning task. then, users are projected in a low-dimensional ideology space where they are subsequently clustered. the political leaning of a user is automatically derived from the cluster to which the user is assigned. we evaluated our technique in two challenging classification tasks and we compared it to baselines and other state-of-the-art approaches. our technique obtains the best results among all unsupervised techniques, with micro f1 = 0.426 in the 8-class task and micro f1 = 0.772 in the 3-class task. other than being interesting on their own, our results also pave the way for the development of new and better unsupervised approaches for the detection of fine-grained political leaning.",2022
"visually grounded models of spoken language: a survey of datasets, architectures and evaluation techniques","this survey provides an overview of the evolution of visually grounded models of spoken language over the last 20 years. such models are inspired by the observation that when children pick up a language, they rely on a wide range of indirect and noisy clues, crucially including signals from the visual modality co-occurring with spoken utterances. several fields have made important contributions to this approach to modeling or mimicking the process of learning language: machine learning, natural language and speech processing, computer vision and cognitive science. the current paper brings together these contributions in order to provide a useful introduction and overview for practitioners in all these areas. we discuss the central research questions addressed, the timeline of developments, and the datasets which enabled much of this work. we then summarize the main modeling architectures and offer an exhaustive overview of the evaluation metrics and analysis techniques.",2022
some inapproximability results of map inference and exponentiated determinantal point processes,"we study the computational complexity of two hard problems on determinantal point processes (dpps). one is maximum a posteriori (map) inference, i.e., to find a principal submatrix having the maximum determinant. the other is probabilistic inference on exponentiated dpps (e-dpps), which can sharpen or weaken the diversity preference of dpps with an exponent parameter p. we present several complexity-theoretic hardness results that explain the difficulty in approximating map inference and the normalizing constant for e-dpps. we first prove that unconstrained map inference for an n × n matrix is np-hard to approximate within a factor of 2βn, where β = 10−1013 . this result improves upon the best-known inapproximability factor of (9/8 − ϵ), and rules out the existence of any polynomial-factor approximation algorithm assuming p = np. we then show that log-determinant maximization is np-hard to approximate within a factor of 5/4 for the unconstrained case and within a factor of 1 + 10−1013 for the size-constrained monotone case. in particular, log-determinant maximization does not admit a polynomial-time approximation scheme unless p = np. as a corollary of the first result, we demonstrate that the normalizing constant for e-dpps of any (fixed) constant exponent p ≥ β-1 = 101013 is np-hard to approximate within a factor of 2βpn, which is in contrast to the case of p ≤ 1 admitting a fully polynomial-time randomized approximation scheme.",2022
samba: a generic framework for secure federated multi-armed bandits,"the multi-armed bandit is a reinforcement learning model where a learning agent repeatedly chooses an action (pull a bandit arm) and the environment responds with a stochastic outcome (reward) coming from an unknown distribution associated with the chosen arm. bandits have a wide-range of application such as web recommendation systems. we address the cumulative reward maximization problem in a secure federated learning setting, where multiple data owners keep their data stored locally and collaborate under the coordination of a central orchestration server. we rely on cryptographic schemes and propose samba, a generic framework for secure federated multi-armed bandits. each data owner has data associated to a bandit arm and the bandit algorithm has to sequentially select which data owner is solicited at each time step. we instantiate samba for five bandit algorithms. we show that samba returns the same cumulative reward as the nonsecure versions of bandit algorithms, while satisfying formally proven security properties. we also show that the overhead due to cryptographic primitives is linear in the size of the input, which is confirmed by our proof-of-concept implementation.",2022
survey and evaluation of causal discovery methods for time series,"we introduce in this survey the major concepts, models, and algorithms proposed so far to infer causal relations from observational time series, a task usually referred to as causal discovery in time series. to do so, after a description of the underlying concepts and modelling assumptions, we present different methods according to the family of approaches they belong to: granger causality, constraint-based approaches, noise-based approaches, score-based approaches, logic-based approaches, topology-based approaches, and difference-based approaches. we then evaluate several representative methods to illustrate the behaviour of different families of approaches. this illustration is conducted on both artificial and real datasets, with different characteristics. the main conclusions one can draw from this survey is that causal discovery in times series is an active research field in which new methods (in every family of approaches) are regularly proposed, and that no family or method stands out in all situations. indeed, they all rely on assumptions that may or may not be appropriate for a particular dataset.",2022
scalable online planning for multi-agent mdps,"we present a scalable tree search planning algorithm for large multi-agent sequential decision problems that require dynamic collaboration. teams of agents need to coordinate decisions in many domains, but naive approaches fail due to the exponential growth of the joint action space with the number of agents. we circumvent this complexity through an approach that allows us to trade computation for approximation quality and dynamically coordinate actions. our algorithm comprises three elements: online planning with monte carlo tree search (mcts), factored representations of local agent interactions with coordination graphs, and the iterative max-plus method for joint action selection. we evaluate our approach on the benchmark sysadmin domain with static coordination graphs and achieve comparable performance with much lower computation cost than our mcts baselines. we also introduce a multi-drone delivery domain with dynamic coordination graphs, and demonstrate how our approach scales to large problems on this domain that are intractable for other mcts methods. we provide an open-source implementation of our algorithm at https://github.com/juliapomdp/factoredvaluemcts.jl.",2022
computational benefits of intermediate rewards for goal-reaching policy learning,"many goal-reaching reinforcement learning (rl) tasks have empirically verified that rewarding the agent on subgoals improves convergence speed and practical performance. we attempt to provide a theoretical framework to quantify the computational benefits of rewarding the completion of subgoals, in terms of the number of synchronous value iterations. in particular, we consider subgoals as one-way intermediate states, which can only be visited once per episode and propose two settings that consider these one-way intermediate states: the one-way single-path (owsp) and the one-way multi-path (owmp) settings. in both owsp and owmp settings, we demonstrate that adding intermediate rewards to subgoals is more computationally efficient than only rewarding the agent once it completes the goal of reaching a terminal state. we also reveal a trade-off between computational complexity and the pursuit of the shortest path in the owmp setting: adding intermediate rewards significantly reduces the computational complexity of reaching the goal but the agent may not find the shortest path, whereas with sparse terminal rewards, the agent finds the shortest path at a significantly higher computational cost. we also corroborate our theoretical results with extensive experiments on the minigrid environments using q-learning and some popular deep rl algorithms.",2022
approximating perfect recall when model checking strategic abilities: theory and applications,"the model checking problem for multi-agent systems against specifications in the alternating-time temporal logic atl, hence atl∗, under perfect recall and imperfect information is known to be undecidable. to tackle this problem, in this paper we investigate a notion of bounded recall under incomplete information. we present a novel three-valued semantics for atl∗ in this setting and analyse the corresponding model checking problem. we show that the three-valued semantics here introduced is an approximation of the classic two-valued semantics, then give a sound, albeit partial, algorithm for model checking two-valued perfect recall via its approximation as three-valued bounded recall. finally, we extend mcmas, an open-source model checker for atl and other agent specifications, to incorporate bounded recall; we illustrate its use and present experimental results.",2022
get out of the bag! silos in ai ethics education: unsupervised topic modeling analysis of global ai curricula,"the domain of artificial intelligence (ai) ethics is not new, with discussions going back at least 40 years. teaching the principles and requirements of ethical ai to students is considered an essential part of this domain, with an increasing number of technical ai courses taught at several higher-education institutions around the globe including content related to ethics. by using latent dirichlet allocation (lda), a generative probabilistic topic model, this study uncovers topics in teaching ethics in ai courses and their trends related to where the courses are taught, by whom, and at what level of cognitive complexity and specificity according to bloom’s taxonomy. in this exploratory study based on unsupervised machine learning, we analyzed a total of 166 courses: 116 from north american universities, 11 from asia, 36 from europe, and 10 from other regions. based on this analysis, we were able to synthesize a model of teaching approaches, which we call bag (build, assess, and govern), that combines specific cognitive levels, course content topics, and disciplines affiliated with the department(s) in charge of the course. we critically assess the implications of this teaching paradigm and provide suggestions about how to move away from these practices. we challenge teaching practitioners and program coordinators to reflect on their usual procedures so that they may expand their methodology beyond the confines of stereotypical thought and traditional biases regarding what disciplines should teach and how. this article appears in the ai &amp; society track.",2022
incremental event calculus for run-time reasoning,"we present a system for online, incremental composite event recognition. in streaming environments, the usual case is for data to arrive with a (variable) delay from, and to be revised by, the underlying sources. we propose rtecinc, an incremental version of rtec, a composite event recognition engine with formal, declarative semantics, that has been shown to scale to several real-world data streams. rtec deals with delayed arrival and revision of events by computing all queries from scratch. this is often inefficient since it results in redundant computations. instead, rtecinc deals with delays and revisions in a more efficient way, by updating only the affected queries. we examine rtecinc theoretically, presenting a complexity analysis, and show the conditions in which it outperforms rtec. moreover, we compare rtecinc and rtec experimentally using real-world and synthetic datasets. the results are compatible with our theoretical analysis and show that rtecinc outperforms rtec in many practical cases.",2022
predicting decisions in language based persuasion games,"sender-receiver interactions, and specifically persuasion games, are widely researched in economic modeling and artificial intelligence, and serve as a solid foundation for powerful applications. however, in the classic persuasion games setting, the messages sent from the expert to the decision-maker are abstract or well-structured application-specific signals rather than natural (human) language messages, although natural language is a very common communication signal in real-world persuasion setups. this paper addresses the use of natural language in persuasion games, exploring its impact on the decisions made by the players and aiming to construct effective models for the prediction of these decisions. for this purpose, we conduct an online repeated interaction experiment. at each trial of the interaction, an informed expert aims to sell an uninformed decision-maker a vacation in a hotel, by sending her a review that describes the hotel. while the expert is exposed to several scored reviews, the decision-maker observes only the single review sent by the expert, and her payoff in case she chooses to take the hotel is a random draw from the review score distribution available to the expert only. the expert’s payoff, in turn, depends on the number of times the decision-maker chooses the hotel. we also compare the behavioral patterns in this experiment to the equivalent patterns in similar experiments where the communication is based on the numerical values of the reviews rather than the reviews’ text, and observe substantial differences which can be explained through an equilibrium analysis of the game. we consider a number of modeling approaches for our verbal communication setup, differing from each other in the model type (deep neural network (dnn) vs. linear classifier), the type of features used by the model (textual, behavioral or both) and the source of the textual features (dnn-based vs. hand-crafted). our results demonstrate that given a prefix of the interaction sequence, our models can predict the future decisions of the decision-maker, particularly when a sequential modeling approach and hand-crafted textual features are applied. further analysis of the hand-crafted textual features allows us to make initial observations about the aspects of text that drive decision making in our setup.",2022
on the indecisiveness of kelly-strategyproof social choice functions,"social choice functions (scfs) map the preferences of a group of agents over some set of alternatives to a non-empty subset of alternatives. the gibbard-satterthwaite theorem has shown that only extremely restrictive scfs are strategyproof when there are more than two alternatives. for set-valued scfs, or so-called social choice correspondences, the situation is less clear. there are miscellaneous -- mostly negative -- results using a variety of strategyproofness notions and additional requirements. the simple and intuitive notion of kelly-strategyproofness has turned out to be particularly compelling because it is weak enough to still allow for positive results. for example, the pareto rule is strategyproof even when preferences are weak, and a number of attractive scfs (such as the top cycle, the uncovered set, and the essential set) are strategyproof for strict preferences. in this paper, we show that, for weak preferences, only indecisive scfs can satisfy strategyproofness. in particular, (i) every strategyproof rank-based scf violates pareto-optimality, (ii) every strategyproof support-based scf (which generalize fishburn's c2 scfs) that satisfies pareto-optimality returns at least one most preferred alternative of every voter, and (iii) every strategyproof non-imposing scf returns the condorcet loser in at least one profile. we also discuss the consequences of these results for randomized social choice.",2022
"neural natural language generation: a survey on multilinguality, multimodality, controllability and learning","developing artificial learning systems that can understand and generate natural language has been one of the long-standing goals of artificial intelligence. recent decades have witnessed an impressive progress on both of these problems, giving rise to a new family of approaches. especially, the advances in deep learning over the past couple of years have led to neural approaches to natural language generation (nlg). these methods combine generative language learning techniques with neural-networks based frameworks. with a wide range of applications in natural language processing, neural nlg (nnlg) is a new and fast growing field of research. in this state-of-the-art report, we investigate the recent developments and applications of nnlg in its full extent from a multidimensional view, covering critical perspectives such as multimodality, multilinguality, controllability and learning strategies. we summarize the fundamental building blocks of nnlg approaches from these aspects and provide detailed reviews of commonly used preprocessing steps and basic neural architectures. this report also focuses on the seminal applications of these nnlg models such as machine translation, description generation, automatic speech recognition, abstractive summarization, text simplification, question answering and generation, and dialogue generation. finally, we conclude with a thorough discussion of the described frameworks by pointing out some open research directions.",2022
multiobjective tree-structured parzen estimator,"practitioners often encounter challenging real-world problems that involve a simultaneous optimization of multiple objectives in a complex search space. to address these problems, we propose a practical multiobjective bayesian optimization algorithm. it is an extension of the widely used tree-structured parzen estimator (tpe) algorithm, called multiobjective tree-structured parzen estimator (motpe). we demonstrate that motpe approximates the pareto fronts of a variety of benchmark problems and a convolutional neural network design problem better than existing methods through the numerical results. we also investigate how the configuration of motpe affects the behavior and the performance of the method and the effectiveness of asynchronous parallelization of the method based on the empirical results.",2022
fairness in influence maximization through randomization,"the influence maximization paradigm has been used by researchers in various fields in order to study how information spreads in social networks. while previously the attention was mostly on efficiency, more recently fairness issues have been taken into account in this scope. in the present paper, we propose to use randomization as a mean for achieving fairness. while this general idea is not new, it has not been applied in this area. similar to previous works like fish et al. (www ’19) and tsang et al. (ijcai ’19), we study the maximin criterion for (group) fairness. in contrast to their work however, we model the problem in such a way that, when choosing the seed sets, probabilistic strategies are possible rather than only deterministic ones. we introduce two different variants of this probabilistic problem, one that entails probabilistic strategies over nodes (node-based problem) and a second one that entails probabilistic strategies over sets of nodes (set-based problem). after analyzing the relation between the two probabilistic problems, we show that, while the original deterministic maximin problem was inapproximable, both probabilistic variants permit approximation algorithms that achieve a constant multiplicative factor of 1 − 1/e minus an additive arbitrarily small error that is due to the simulation of the information spread. for the node-based problem, the approximation is achieved by observing that a polynomial-sized linear program approximates the problem well. for the set-based problem, we show that a multiplicative-weight routine can yield the approximation result. for an experimental study, we provide implementations of multiplicative-weight routines for both the set-based and the node-based problems and compare the achieved fairness values to existing methods. maybe non-surprisingly, we show that the ex-ante values, i.e., minimum expected value of an individual (or group) to obtain the information, of the computed probabilistic strategies are significantly larger than the (ex-post) fairness values of previous methods. this indicates that studying fairness via randomization is a worthwhile path to follow. interestingly and maybe more surprisingly, we observe that even the ex-post fairness values, i.e., fairness values of sets sampled according to the probabilistic strategies computed by our routines, dominate over the fairness achieved by previous methods on many of the instances tested.",2022
the application of machine learning techniques for predicting match results in team sport: a review,"predicting the results of matches in sport is a challenging and interesting task. in this paper, we review a selection of studies from 1996 to 2019 that used machine learning for predicting match results in team sport. considering both invasion sports and striking/fielding sports, we discuss commonly applied machine learning algorithms, as well as common approaches related to data and evaluation. our study considers accuracies that have been achieved across different sports, and explores whether evidence exists to support the notion that outcomes of some sports may be inherently more difficult to predict. we also uncover common themes of future research directions and propose recommendations for future researchers. although there remains a lack of benchmark datasets (apart from in soccer), and the differences between sports, datasets and features makes between-study comparisons difficult, as we discuss, it is possible to evaluate accuracy performance in other ways. artificial neural networks were commonly applied in early studies, however, our findings suggest that a range of models should instead be compared. selecting and engineering an appropriate feature set appears to be more important than having a large number of instances. for feature selection, we see potential for greater inter-disciplinary collaboration between sport performance analysis, a sub-discipline of sport science, and machine learning.",2022
a metric space for point process excitations,"a multivariate hawkes process enables self- and cross-excitations through a triggering matrix that behaves like an asymmetrical covariance structure, characterizing pairwise interactions between the event types. full-rank estimation of all interactions is often infeasible in empirical settings. models that specialize on a spatiotemporal application alleviate this obstacle by exploiting spatial locality, allowing the dyadic relationships between events to depend only on separation in time and relative distances in real euclidean space. here we generalize this framework to any multivariate hawkes process, and harness it as a vessel for embedding arbitrary event types in a hidden metric space. specifically, we propose a hidden hawkes geometry (hhg) model to uncover the hidden geometry between event excitations in a multivariate point process. the low dimensionality of the embedding regularizes the structure of the inferred interactions. we develop a number of estimators and validate the model by conducting several experiments. in particular, we investigate regional infectivity dynamics of covid-19 in an early south korean record and recent los angeles confirmed cases. by additionally performing synthetic experiments on short records as well as explorations into options markets and the ebola epidemic, we demonstrate that learning the embedding alongside a point process uncovers salient interactions in a broad range of applications.",2022
marginal distance and hilbert-schmidt covariances-based independence tests for multivariate functional data,"we study the pairwise and mutual independence testing problem for multivariate functional data. using a basis representation of functional data, we reduce this problem to testing the independence of multivariate data, which may be high-dimensional. for pairwise independence, we apply tests based on distance and hilbert-schmidt covariances as well as their marginal versions, which aggregate these covariances for coordinates of random processes. in the case of mutual independence, we study asymmetric and symmetric aggregating measures of pairwise dependence. a theoretical justification of the test procedures is established. in extensive simulation studies and examples based on a real economic data set, we investigate and compare the performance of the tests in terms of size control and power. an important finding is that tests based on distance and hilbert-schmidt covariances are usually more powerful than their marginal versions under linear dependence, while the reverse is true under non-linear dependence.",2022
agent-based modeling for predicting pedestrian trajectories around an autonomous vehicle,"this paper addresses modeling and simulating pedestrian trajectories when interacting with an autonomous vehicle in a shared space. most pedestrian–vehicle interaction models are not suitable for predicting individual trajectories. data-driven models yield accurate predictions but lack generalizability to new scenarios, usually do not run in real time and produce results that are poorly explainable. current expert models do not deal with the diversity of possible pedestrian interactions with the vehicle in a shared space and lack microscopic validation. we propose an expert pedestrian model that combines the social force model and a new decision model for anticipating pedestrian–vehicle interactions. the proposed model integrates different observed pedestrian behaviors, as well as the behaviors of the social groups of pedestrians, in diverse interaction scenarios with a car. we calibrate the model by fitting the parameters values on a training set. we validate the model and evaluate its predictive potential through qualitative and quantitative comparisons with ground truth trajectories. the proposed model reproduces observed behaviors that have not been replicated by the social force model and outperforms the social force model at predicting pedestrian behavior around the vehicle on the used dataset. the model generates explainable and real-time trajectory predictions. additional evaluation on a new dataset shows that the model generalizes well to new scenarios and can be applied to an autonomous vehicle embedded prediction.",2022
adversarial framework with certified robustness for time-series domain via statistical features,"time-series data arises in many real-world applications (e.g., mobile health) and deep neural networks (dnns) have shown great success in solving them. despite their success, little is known about their robustness to adversarial attacks. in this paper, we propose a novel adversarial framework referred to as time-series attacks via statistical features (tsa-stat). to address the unique challenges of time-series domain, tsa-stat employs constraints on statistical features of the time-series data to construct adversarial examples. optimized polynomial transformations are used to create attacks that are more effective (in terms of successfully fooling dnns) than those based on additive perturbations. we also provide certified bounds on the norm of the statistical features for constructing adversarial examples. our experiments on diverse real-world benchmark datasets show the effectiveness of tsa-stat in fooling dnns for time-series domain and in improving their robustness.",2022
a logic-based explanation generation framework for classical and hybrid planning problems,"in human-aware planning systems, a planning agent might need to explain its plan to a human user when that plan appears to be non-feasible or sub-optimal. a popular approach, called model reconciliation, has been proposed as a way to bring the model of the human user closer to the agent’s model. to do so, the agent provides an explanation that can be used to update the model of human such that the agent’s plan is feasible or optimal to the human user. existing approaches to solve this problem have been based on automated planning methods and have been limited to classical planning problems only. in this paper, we approach the model reconciliation problem from a different perspective, that of knowledge representation and reasoning, and demonstrate that our approach can be applied not only to classical planning problems but also hybrid systems planning problems with durative actions and events/processes. in particular, we propose a logic-based framework for explanation generation, where given a knowledge base kba (of an agent) and a knowledge base kbh (of a human user), each encoding their knowledge of a planning problem, and that kba entails a query q (e.g., that a proposed plan of the agent is valid), the goal is to identify an explanation ε ⊆ kba such that when it is used to update kbh, then the updated kbh also entails q. more specifically, we make the following contributions in this paper: (1) we formally define the notion of logic-based explanations in the context of model reconciliation problems; (2) we introduce a number of cost functions that can be used to reflect preferences between explanations; (3) we present algorithms to compute explanations for both classical planning and hybrid systems planning problems; and (4) we empirically evaluate their performance on such problems. our empirical results demonstrate that, on classical planning problems, our approach is faster than the state of the art when the explanations are long or when the size of the knowledge base is small (e.g., the plans to be explained are short). they also demonstrate that our approach is efficient for hybrid systems planning problems. finally, we evaluate the real-world efficacy of explanations generated by our algorithms through a controlled human user study, where we develop a proof-of-concept visualization system and use it as a medium for explanation communication.",2022
multilingual machine translation: deep analysis of language-specific encoder-decoders,"state-of-the-art multilingual machine translation relies on a shared encoder-decoder. in this paper, we propose an alternative approach based on language-specific encoder-decoders, which can be easily extended to new languages by learning their corresponding modules. to establish a common interlingua representation, we simultaneously train n initial languages. our experiments show that the proposed approach improves over the shared encoder-decoder for the initial languages and when adding new languages, without the need to retrain the remaining modules. all in all, our work closes the gap between shared and language-specific encoder-decoders, advancing toward modular multilingual machine translation systems that can be flexibly extended in lifelong learning settings.",2022
ffci: a framework for interpretable automatic evaluation of summarization,"in this paper, we propose ffci, a framework for fine-grained summarization evaluation that comprises four elements: faithfulness (degree of factual consistency with the source), focus (precision of summary content relative to the reference), coverage (recall of summary content relative to the reference), and inter-sentential coherence (document fluency between adjacent sentences). we construct a novel dataset for focus, coverage, and inter-sentential coherence, and develop automatic methods for evaluating each of the four dimensions of ffci based on cross-comparison of evaluation metrics and model-based evaluation methods, including question answering (qa) approaches, semantic textual similarity (sts), next-sentence prediction (nsp), and scores derived from 19 pre-trained language models. we then apply the developed metrics in evaluating a broad range of summarization models across two datasets, with some surprising findings.",2022
optimizing for interpretability in deep neural networks with tree regularization,"deep models have advanced prediction in many domains, but their lack of interpretability remains a key barrier to the adoption in many real world applications. there exists a large body of work aiming to help humans understand these black box functions to varying levels of granularity – for example, through distillation, gradients, or adversarial examples. these methods however, all tackle interpretability as a separate process after training. in this work, we take a different approach and explicitly regularize deep models so that they are well-approximated by processes that humans can step through in little time. specifically, we train several families of deep neural networks to resemble compact, axis-aligned decision trees without significant compromises in accuracy. the resulting axis-aligned decision functions uniquely make tree regularized models easy for humans to interpret. moreover, for situations in which a single, global tree is a poor estimator, we introduce a regional tree regularizer that encourages the deep model to resemble a compact, axis-aligned decision tree in predefined, human-interpretable contexts. using intuitive toy examples, benchmark image datasets, and medical tasks for patients in critical care and with hiv, we demonstrate that this new family of tree regularizers yield models that are easier for humans to simulate than l1 or l2 penalties without sacrificing predictive power.",2021
a semi-exact algorithm for quickly computing a maximum weight clique in large sparse graphs,"this paper explores techniques to quickly solve the maximum weight clique problem (mwcp) in very large scale sparse graphs. due to their size, and the hardness of mwcp, it is infeasible to solve many of these graphs with exact algorithms. although recent heuristic algorithms make progress in solving mwcp in large graphs, they still need considerable time to get a high-quality solution. in this work, we focus on solving mwcp for large sparse graphs within a short time limit. we propose a new method for mwcp which interleaves clique finding with data reduction rules. we propose novel ideas to make this process efficient, and develop an algorithm called fastwclq. experiments on a broad range of large sparse graphs show that fastwclq finds better solutions than state-of-the-art algorithms while the running time of fastwclq is much shorter than the competitors for most instances. further, fastwclq proves the optimality of its solutions for roughly half of the graphs, all with at least 105 vertices, with an average time of 21 seconds.",2021
finding the hardest formulas for resolution,"a cnf formula is harder than another cnf formula with the same number of clauses if it requires a longer resolution proof. in this paper we introduce resolution hardness numbers; they give for m=1,2,... the length of a shortest proof of a hardest formula on m clauses. we compute the first ten resolution hardness numbers, along with the corresponding hardest formulas. to achieve this, we devise a candidate filtering and symmetry breaking search scheme for limiting the number of potential candidates for hardest for- mulas, and an efficient sat encoding for computing a shortest resolution proof of a given candidate formula.",2021
worst-case bounds on power vs. proportion in weighted voting games with an application to false-name manipulation,"weighted voting games apply to a wide variety of multi-agent settings. they enable the formalization of power indices which quantify the coalitional power of players. we take a novel approach to the study of the power of big vs. small players in these games. we model small (big) players as having single (multiple) votes. the aggregate relative power of big players is measured w.r.t. their votes proportion. for this ratio, we show small constant worst-case bounds for the shapley-shubik and the deegan-packel indices. in sharp contrast, this ratio is unbounded for the banzhaf index. as an application, we define a false-name strategic normal form game where each big player may split its votes between false identities, and study its various properties. together, our results provide foundations for the implications of players’ size, modeled as their ability to split, on their relative power.",2021
on the computational complexity of non-dictatorial aggregation,"we investigate when non-dictatorial aggregation is possible from an algorithmic perspective, where non-dictatorial aggregation means that the votes cast by the members of a society can be aggregated in such a way that there is no single member of the society that always dictates the collective outcome. we consider the setting in which the members of a society take a position on a fixed collection of issues, where for each issue several different alternatives are possible, but the combination of choices must belong to a given set x of allowable voting patterns. such a set x is called a possibility domain if there is an aggregator that is non-dictatorial, operates separately on each issue, and returns values among those cast by the society on each issue. we design a polynomial-time algorithm that decides, given a set x of voting patterns, whether or not x is a possibility domain. furthermore, if x is a possibility domain, then the algorithm constructs in polynomial time a non-dictatorial aggregator for x. furthermore, we show that the question of whether a boolean domain x is a possibility domain is in nlogspace. we also design a polynomial-time algorithm that decides whether x is a uniform possibility domain, that is, whether x admits an aggregator that is non-dictatorial even when restricted to any two positions for each issue. as in the case of possibility domains, the algorithm also constructs in polynomial time a uniform non-dictatorial aggregator, if one exists. then, we turn our attention to the case where x is given implicitly, either as the set of assignments satisfying a propositional formula, or as a set of consistent evaluations of a sequence of propositional formulas. in both cases, we provide bounds to the complexity of deciding if x is a (uniform) possibility domain. finally, we extend our results to four types of aggregators that have appeared in the literature: generalized dictatorships, whose outcome is always an element of their input, anonymous aggregators, whose outcome is not affected by permutations of their input, monotone, whose outcome does not change if more individuals agree with it and systematic, which aggregate every issue in the same way.",2021
pure nash equilibria in resource graph games,"this paper studies the existence of pure nash equilibria in resource graph games, a general class of strategic games succinctly representing the players’ private costs. these games are defined relative to a finite set of resources and the strategy set of each player corresponds to a set of subsets of resources. the cost of a resource is an arbitrary function of the load vector of a certain subset of resources. as our main result, we give complete characterizations of the cost functions guaranteeing the existence of pure nash equilibria for weighted and unweighted players, respectively. for unweighted players, pure nash equilibria are guaranteed to exist for any choice of the players’ strategy space if and only if the cost of each resource is an arbitrary function of the load of the resource itself and linear in the load of all other resources where the linear coefficients of mutual influence of different resources are symmetric. this implies in particular that for any other cost structure there is a resource graph game that does not have a pure nash equilibrium. for weighted games where players have intrinsic weights and the cost of each resource depends on the aggregated weight of its users, pure nash equilibria are guaranteed to exist if and only if the cost of a resource is linear in all resource loads, and the linear factors of mutual influence are symmetric, or there is no interaction among resources and the cost is an exponential function of the local resource load. we further discuss the computational complexity of pure nash equilibria in resource graph games showing that for unweighted games where pure nash equilibria are guaranteed to exist, it is conp-complete to decide for a given strategy profile whether it is a pure nash equilibrium. for general resource graph games, we prove that the decision whether a pure nash equilibrium exists is σ p 2 -complete.",2021
a theoretical perspective on hyperdimensional computing,"hyperdimensional (hd) computing is a set of neurally inspired methods for obtaining highdimensional, low-precision, distributed representations of data. these representations can be combined with simple, neurally plausible algorithms to effect a variety of information processing tasks. hd computing has recently garnered significant interest from the computer hardware community as an energy-efficient, low-latency, and noise-robust tool for solving learning problems. in this review, we present a unified treatment of the theoretical foundations of hd computing with a focus on the suitability of representations for learning.",2021
relevance in belief update,"it has been pointed out by katsuno and mendelzon that the so-called agm revision operators, defined by alchourron, gardenfors and makinson, do not behave well in dynamically-changing applications. on that premise, katsuno and mendelzon formally characterized a different type of belief-change operators, typically referred to as km update operators, which, to this date, constitute a benchmark in belief update. in this article, we show that there exist km update operators that yield the same counter-intuitive results as any agm revision operator. against this non-satisfactory background, we prove that a translation of parikh’s relevance-sensitive axiom (p), in the realm of belief update, suffices to block this liberal behaviour of km update operators. it is shown, both axiomatically and semantically, that axiom (p) for belief update, essentially, encodes a type of relevance that acts at the possible-worlds level, in the context of which each possible world is locally modified, in the light of new information. interestingly, relevance at the possible-worlds level is shown to be equivalent to a form of relevance that acts at the sentential level, by considering the building blocks of relevance to be the sentences of the language. furthermore, we concretely demonstrate that parikh’s notion of relevance in belief update can be regarded as (at least a partial) solution to the frame, ramification and qualification problems, encountered in dynamically-changing worlds. last but not least, a whole new class of well-behaved, relevance-sensitive km update operators is introduced, which generalize forbus’ update operator and are perfectly-suited for real-world implementations.",2021
on quantifying literals in boolean logic and its applications to explainable ai,"quantified boolean logic results from adding operators to boolean logic for existentially and universally quantifying variables. this extends the reach of boolean logic by enabling a variety of applications that have been explored over the decades. the existential quantification of literals (variable states) and its applications have also been studied in the literature. in this paper, we complement this by introducing and studying universal literal quantification and its applications, particularly to explainable ai. we also provide a novel semantics for quantification, discuss the interplay between variable/literal and existential/universal quantification, and identify some classes of boolean formulas and circuits on which quantification can be done efficiently. literal quantification is more fine-grained than variable quantification as the latter can be defined in terms of the former, leading to a refinement of quantified boolean logic with literal quantification as its primitive.",2021
sunny-as2: enhancing sunny for algorithm selection,"sunny is an algorithm selection (as) technique originally tailored for constraint programming (cp). sunny is based on the k-nearest neighbors algorithm and enables one to schedule, from a portfolio of solvers, a subset of solvers to be run on a given cp problem. this approach has proved to be effective for cp problems. in 2015, the aslib benchmarks were released for comparing as systems coming from disparate fields (e.g., asp, qbf, and sat) and sunny was extended to deal with generic as problems. this led to the development of sunny-as, a prototypical algorithm selector based on sunny for aslib scenarios. a major improvement of sunny-as, called sunny-as2, was then submitted to the open algorithm selection challenge (oasc) in 2017, where it turned out to be the best approach for the runtime minimization of decision problems. in this work we present the technical advancements of sunny-as2, by detailing through several empirical evaluations and by providing new insights. its current version, built on the top of the preliminary version submitted to oasc, is able to outperform sunny-as and other state-of-the-art as methods, including those who did not attend the challenge.",2021
a survey of algorithms for black-box safety validation of cyber-physical systems,"autonomous cyber-physical systems (cps) can improve safety and efficiency for safety-critical applications, but require rigorous testing before deployment. the complexity of these systems often precludes the use of formal verification and real-world testing can be too dangerous during development. therefore, simulation-based techniques have been developed that treat the system under test as a black box operating in a simulated environment. safety validation tasks include finding disturbances in the environment that cause the system to fail (falsification), finding the most-likely failure, and estimating the probability that the system fails. motivated by the prevalence of safety-critical artificial intelligence, this work provides a survey of state-of-the-art safety validation techniques for cps with a focus on applied algorithms and their modifications for the safety validation problem. we present and discuss algorithms in the domains of optimization, path planning, reinforcement learning, and importance sampling. problem decomposition techniques are presented to help scale algorithms to large state spaces, which are common for cps. a brief overview of safety-critical applications is given, including autonomous vehicles and aircraft collision avoidance systems. finally, we present a survey of existing academic and commercially available safety validation tools.",2021
nlp methods for extraction of symptoms from unstructured data for use in prognostic covid-19 analytic models,"statistical modeling of outcomes based on a patient's presenting symptoms (symptomatology) can help deliver high quality care and allocate essential resources, which is especially important during the covid-19 pandemic. patient symptoms are typically found in unstructured notes, and thus not readily available for clinical decision making. in an attempt to fill this gap, this study compared two methods for symptom extraction from emergency department (ed) admission notes. both methods utilized a lexicon derived by expanding the center for disease control and prevention's (cdc) symptoms of coronavirus list. the first method utilized a word2vec model to expand the lexicon using a dictionary mapping to the uni ed medical language system (umls). the second method utilized the expanded lexicon as a rule-based gazetteer and the umls. these methods were evaluated against a manually annotated reference (f1-score of 0.87 for umls-based ensemble; and 0.85 for rule-based gazetteer with umls). through analyses of associations of extracted symptoms used as features against various outcomes, salient risks among the population of covid-19 patients, including increased risk of in-hospital mortality (or 1.85, p-value &lt; 0.001), were identified for patients presenting with dyspnea. disparities between english and non-english speaking patients were also identified, the most salient being a concerning finding of opposing risk signals between fatigue and in-hospital mortality (non-english: or 1.95, p-value = 0.02; english: or 0.63, p-value = 0.01). while use of symptomatology for modeling of outcomes is not unique, unlike previous studies this study showed that models built using symptoms with the outcome of in-hospital mortality were not significantly different from models using data collected during an in-patient encounter (auc of 0.9 with 95% ci of [0.88, 0.91] using only vital signs; auc of 0.87 with 95% ci of [0.85, 0.88] using only symptoms). these findings indicate that prognostic models based on symptomatology could aid in extending covid-19 patient care through telemedicine, replacing the need for in-person options. the methods presented in this study have potential for use in development of symptomatology-based models for other diseases, including for the study of post-acute sequelae of covid-19 (pasc).",2021
optimal any-angle pathfinding on a sphere,"pathfinding in euclidean space is a common problem faced in robotics and computer games. for long-distance navigation on the surface of the earth or in outer space however, approximating the geometry as euclidean can be insufficient for real-world applications such as the navigation of spacecraft, aeroplanes, drones and ships. this article describes an any-angle pathfinding algorithm for calculating the shortest path between point pairs over the surface of a sphere. introducing several novel adaptations, it is shown that anya as described by harabor &amp; grastien for euclidean space can be extended to spherical geometry. there, where the shortest-distance line between coordinates is defined instead by a great-circle path, the optimal solution is typically a curved line in euclidean space. in addition the turning points for optimal paths in spherical geometry are not necessarily corner points as they are in euclidean space, as will be shown, making further substantial adaptations to anya necessary. spherical anya returns the optimal path on the sphere, given these different properties of world maps defined in spherical geometry. it preserves all primary benefits of anya in euclidean geometry, namely the spherical anya algorithm always returns an optimal path on a sphere and does so entirely on-line, without any preprocessing or large memory overheads. performance benchmarks are provided for several game maps including starcraft and warcraft iii as well as for sea navigation on earth using the noaa bathymetric dataset. always returning the shorter path compared with the euclidean approximation yielded by anya, spherical anya is shown to be faster than anya for the majority of sea routes and slower for game maps and random maps.",2021
optimally deceiving a learning leader in stackelberg games,"recent results have shown that algorithms for learning the optimal commitment in a stackelberg game are susceptible to manipulation by the follower. these learning algorithms operate by querying the best responses of the follower, who consequently can deceive the algorithm by using fake best responses, typically by responding according to fake payoffs that are different from the actual ones. for this strategic behavior to be successful, the main challenge faced by the follower is to pinpoint the fake payoffs that would make the learning algorithm output a commitment that benefits them the most. while this problem has been considered before, the related literature has only focused on a simple setting where the follower can only choose from a finite set of payoff matrices, thus leaving the general version of the problem unanswered. in this paper, we fill this gap by showing that it is always possible for the follower to efficiently compute (near-)optimal fake payoffs, for various scenarios of learning interaction between the leader and the follower. our results also establish an interesting connection between the follower’s deception and the leader’s maximin utility: through deception, the follower can induce almost any (fake) stackelberg equilibrium if and only if the leader obtains at least their maximin utility in this equilibrium.",2021
contrastive explanations of plans through model restrictions,"in automated planning, the need for explanations arises when there is a mismatch between a proposed plan and the user’s expectation. we frame explainable ai planning as an iterative plan exploration process, in which the user asks a succession of contrastive questions that lead to the generation and solution of hypothetical planning problems that are restrictions of the original problem. the object of the exploration is for the user to understand the constraints that govern the original plan and, ultimately, to arrive at a satisfactory plan. we present the results of a user study that demonstrates that when users ask questions about plans, those questions are usually contrastive, i.e. “why a rather than b?”. we use the data from this study to construct a taxonomy of user questions that often arise during plan exploration. our approach to iterative plan exploration is a process of successive model restriction. each contrastive user question imposes a set of constraints on the planning problem, leading to the construction of a new hypothetical planning problem as a restriction of the original. solving this restricted problem results in a plan that can be compared with the original plan, admitting a contrastive explanation. we formally define model-based compilations in pddl2.1 for each type of constraint derived from a contrastive user question in the taxonomy, and empirically evaluate the compilations in terms of computational complexity. the compilations were implemented as part of an explanation framework supporting iterative model restriction. we demonstrate its benefits in a second user study.",2021
multilabel classification with partial abstention: bayes-optimal prediction under label independence,"in contrast to conventional (single-label) classification, the setting of multilabel classification (mlc) allows an instance to belong to several classes simultaneously. thus, instead of selecting a single class label, predictions take the form of a subset of all labels. in this paper, we study an extension of the setting of mlc, in which the learner is allowed to partially abstain from a prediction, that is, to deliver predictions on some but not necessarily all class labels. this option is useful in cases of uncertainty, where the learner does not feel confident enough on the entire label set. adopting a decision-theoretic perspective, we propose a formal framework of mlc with partial abstention, which builds on two main building blocks: first, the extension of underlying mlc loss functions so as to accommodate abstention in a proper way, and second the problem of optimal prediction, that is, finding the bayes-optimal prediction minimizing this generalized loss in expectation. it is well known that different (generalized) loss functions may have different risk-minimizing predictions, and finding the bayes predictor typically comes down to solving a computationally complexity optimization problem. in the most general case, given a prediction of the (conditional) joint distribution of possible labelings, the minimizer of the expected loss needs to be found over a number of candidates which is exponential in the number of class labels. we elaborate on properties of risk minimizers for several commonly used (generalized) mlc loss functions, show them to have a specific structure, and leverage this structure to devise efficient methods for computing bayes predictors. experimentally, we show mlc with partial abstention to be effective in the sense of reducing loss when being allowed to abstain.",2021
output space entropy search framework for multi-objective bayesian optimization,"we consider the problem of black-box multi-objective optimization (moo) using expensive function evaluations (also referred to as experiments), where the goal is to approximate the true pareto set of solutions by minimizing the total resource cost of experiments. for example, in hardware design optimization, we need to find the designs that trade-off performance, energy, and area overhead using expensive computational simulations. the key challenge is to select the sequence of experiments to uncover high-quality solutions using minimal resources. in this paper, we propose a general framework for solving moo problems based on the principle of output space entropy (ose) search: select the experiment that maximizes the information gained per unit resource cost about the true pareto front. we appropriately instantiate the principle of ose search to derive efficient algorithms for the following four moo problem settings: 1) the most basic single-fidelity setting, where experiments are expensive and accurate; 2) handling black-box constraints which cannot be evaluated without performing experiments; 3) the discrete multi-fidelity setting, where experiments can vary in the amount of resources consumed and their evaluation accuracy; and 4) the continuous-fidelity setting, where continuous function approximations result in a huge space of experiments. experiments on diverse synthetic and real-world benchmarks show that our ose search based algorithms improve over state-of-the-art methods in terms of both computational-efficiency and accuracy of moo solutions.",2021
analysis of the impact of randomization of search-control parameters in monte-carlo tree search,"monte-carlo tree search (mcts) has been applied successfully in many domains, including games. however, its performance is not uniform on all domains, and it also depends on how parameters that control the search are set. parameter values that are optimal for a task might be sub-optimal for another. in a domain that tackles many games with different characteristics, like general game playing (ggp), selecting appropriate parameter settings is not a trivial task. games are unknown to the player, thus, finding optimal parameters for a given game in advance is not feasible. previous work has looked into tuning parameter values online, while the game is being played, showing some promising results. this tuning approach looks for optimal parameter values, balancing exploitation of values that performed well so far in the search and exploration of less sampled values. continuously changing parameter values while performing the search, combined also with exploration of multiple values, introduces some randomization in the process. in addition, previous research indicates that adding randomization to certain components of mcts might increase the diversification of the search and improve the performance. therefore, this article investigates the effect of randomly selecting values for mcts search-control parameters online among predefined sets of reasonable values. for the ggp domain, this article evaluates four different online parameter randomization strategies by comparing them with other methods to set parameter values: online parameter tuning, offline parameter tuning and sub-optimal parameter choices. results on a set of 14 heterogeneous abstract games show that randomizing parameter values before each simulation has a positive effect on the search in some of the tested games, with respect to using fixed offline-tuned parameters. moreover, results show a clear distinction between games for which online parameter tuning works best and games for which online randomization works best. in addition, the overall performance of online parameter randomization is closer to the one of online parameter turning than the one of sub-optimal parameter values, showing that online randomization is a reasonable parameter selection strategy. when analyzing the structure of the search trees generated by agents that use the different parameters selection strategies, it is clear that randomization causes mcts to become more explorative, which is helpful for alignment games that present many winning paths in their trees. online parameter tuning, instead, seems more suitable for games that present narrow winning paths and many losing paths.",2021
multi-label classification neural networks with hard logical constraints,"multi-label classification (mc) is a standard machine learning problem in which a data point can be associated with a set of classes. a more challenging scenario is given by hierarchical multi-label classification (hmc) problems, in which every prediction must satisfy a given set of hard constraints expressing subclass relationships between classes. in this article, we propose c-hmcnn(h), a novel approach for solving hmc problems, which, given a network h for the underlying mc problem, exploits the hierarchy information in order to produce predictions coherent with the constraints and to improve performance. furthermore, we extend the logic used to express hmc constraints in order to be able to specify more complex relations among the classes and propose a new model ccn(h), which extends c-hmcnn(h) and is again able to satisfy and exploit the constraints to improve performance. we conduct an extensive experimental analysis showing the superior performance of both c-hmcnn(h) and ccn(h) when compared to state-of-the-art models in both the hmc and the general mc setting with hard logical constraints.",2021
task-aware verifiable rnn-based policies for partially observable markov decision processes,"partially observable markov decision processes (pomdps) are models for sequential decision-making under uncertainty and incomplete information. machine learning methods typically train recurrent neural networks (rnn) as effective representations of pomdp policies that can efficiently process sequential data. however, it is hard to verify whether the pomdp driven by such rnn-based policies satisfies safety constraints, for instance, given by temporal logic specifications. we propose a novel method that combines techniques from machine learning with the field of formal methods: training an rnn-based policy and then automatically extracting a so-called finite-state controller (fsc) from the rnn. such fscs offer a convenient way to verify temporal logic constraints. implemented on a pomdp, they induce a markov chain, and probabilistic verification methods can efficiently check whether this induced markov chain satisfies a temporal logic specification. using such methods, if the markov chain does not satisfy the specification, a byproduct of verification is diagnostic information about the states in the pomdp that are critical for the specification. the method exploits this diagnostic information to either adjust the complexity of the extracted fsc or improve the policy by performing focused retraining of the rnn. the method synthesizes policies that satisfy temporal logic specifications for pomdps with up to millions of states, which are three orders of magnitude larger than comparable approaches.",2021
experimental comparison and survey of twelve time series anomaly detection algorithms,"the existence of an anomaly detection method that is optimal for all domains is a myth. thus, there exists a plethora of anomaly detection methods which increases every year for a wide variety of domains. but a strength can also be a weakness; given this massive library of methods, how can one select the best method for their application? current literature is focused on creating new anomaly detection methods or large frameworks for experimenting with multiple methods at the same time. however, and especially as the literature continues to expand, an extensive evaluation of every anomaly detection method is simply not feasible. to reduce this evaluation burden, we present guidelines to intelligently choose the optimal anomaly detection methods based on the characteristics the time series displays such as seasonality, trend, level change concept drift, and missing time steps. we provide a comprehensive experimental validation and survey of twelve anomaly detection methods over different time series characteristics to form guidelines based on several metrics: the auc (area under the curve), windowed f-score, and numenta anomaly benchmark (nab) scoring model. applying our methodologies can save time and effort by surfacing the most promising anomaly detection methods instead of experimenting extensively with a rapidly expanding library of anomaly detection methods, especially in an online setting.",2021
flexible bayesian nonlinear model configuration,"regression models are used in a wide range of applications providing a powerful scientific tool for researchers from different fields. linear, or simple parametric, models are often not sufficient to describe complex relationships between input variables and a response. such relationships can be better described through flexible approaches such as neural networks, but this results in less interpretable models and potential overfitting. alternatively, specific parametric nonlinear functions can be used, but the specification of such functions is in general complicated. in this paper, we introduce a flexible approach for the construction and selection of highly flexible nonlinear parametric regression models. nonlinear features are generated hierarchically, similarly to deep learning, but have additional flexibility on the possible types of features to be considered. this flexibility, combined with variable selection, allows us to find a small set of important features and thereby more interpretable models. within the space of possible functions, a bayesian approach, introducing priors for functions based on their complexity, is considered. a genetically modified mode jumping markov chain monte carlo algorithm is adopted to perform bayesian inference and estimate posterior probabilities for model averaging. in various applications, we illustrate how our approach is used to obtain meaningful nonlinear models. additionally, we compare its predictive performance with several machine learning algorithms.",2021
graph kernels: a survey,"graph kernels have attracted a lot of attention during the last decade, and have evolved into a rapidly developing branch of learning on structured data. during the past 20 years, the considerable research activity that occurred in the field resulted in the development of dozens of graph kernels, each focusing on specific structural properties of graphs. graph kernels have proven successful in a wide range of domains, ranging from social networks to bioinformatics. the goal of this survey is to provide a unifying view of the literature on graph kernels. in particular, we present a comprehensive overview of a wide range of graph kernels. furthermore, we perform an experimental evaluation of several of those kernels on publicly available datasets, and provide a comparative study. finally, we discuss key applications of graph kernels, and outline some challenges that remain to be addressed.",2021
steady-state planning in expected reward multichain mdps,"the planning domain has experienced increased interest in the formal synthesis of decision-making policies. this formal synthesis typically entails finding a policy which satisfies formal specifications in the form of some well-defined logic. while many such logics have been proposed with varying degrees of expressiveness and complexity in their capacity to capture desirable agent behavior, their value is limited when deriving decision-making policies which satisfy certain types of asymptotic behavior in general system models. in particular, we are interested in specifying constraints on the steady-state behavior of an agent, which captures the proportion of time an agent spends in each state as it interacts for an indefinite period of time with its environment. this is sometimes called the average or expected behavior of the agent and the associated planning problem is faced with significant challenges unless strong restrictions are imposed on the underlying model in terms of the connectivity of its graph structure. in this paper, we explore this steady-state planning problem that consists of deriving a decision-making policy for an agent such that constraints on its steady-state behavior are satisfied. a linear programming solution for the general case of multichain markov decision processes (mdps) is proposed and we prove that optimal solutions to the proposed programs yield stationary policies with rigorous guarantees of behavior.",2021
teaching people by justifying tree search decisions: an empirical study in curling,"in this research note we show that a simple justification system can be used to teach humans non-trivial strategies of the olympic sport of curling. this is achieved by justifying the decisions of kernel regression uct (kr-uct), a tree search algorithm that derives curling strategies by playing the game with itself. given an action returned by kr-uct and the expected outcome of that action, we use a decision tree to produce a counterfactual justification of kr-uct’s decision. the system samples other possible outcomes and selects for presentation the outcomes that are most similar to the expected outcome in terms of visual features and most different in terms of expected end-game value. a user study with 122 people shows that the participants who had access to the justifications produced by our system achieved much higher scores in a curling test than those who only observed the decision made by kr-uct and those with access to the justifications of a baseline system. this is, to the best of our knowledge, the first work showing that a justification system is able to teach humans non-trivial strategies learned by an algorithm operating in self play.",2021
reasoning with pcp-nets,"we introduce pcp-nets, a formalism to model qualitative conditional preferences with probabilistic uncertainty. pcp-nets generalise cp-nets by allowing for uncertainty over the preference orderings. we define and study both optimality and dominance queries in pcp-nets, and we propose a tractable approximation of dominance which we show to be very accurate in our experimental setting. since pcp-nets can be seen as a way to model a collection of weighted cp-nets, we also explore the use of pcp-nets in a multi-agent context, where individual agents submit cp-nets which are then aggregated into a single pcp-net. we consider various ways to perform such aggregation and we compare them via two notions of scores, based on well known voting theory concepts. experimental results allow us to identify the aggregation method that better represents the given set of cp-nets and the most efficient dominance procedure to be used in the multi-agent context.",2021
learning realistic patterns from visually unrealistic stimuli: generalization and data anonymization,"good training data is a prerequisite to develop useful machine learning applications. however, in many domains existing data sets cannot be shared due to privacy regulations (e.g., from medical studies). this work investigates a simple yet unconventional approach for anonymized data synthesis to enable third parties to benefit from such anonymized data. we explore the feasibility of learning implicitly from visually unrealistic, task-relevant stimuli, which are synthesized by exciting the neurons of a trained deep neural network. as such, neuronal excitation can be used to generate synthetic stimuli. the stimuli data is used to train new classification models. furthermore, we extend this framework to inhibit representations that are associated with specific individuals. we use sleep monitoring data from both an open and a large closed clinical study, and electroencephalogram sleep stage classification data, to evaluate whether (1) end-users can create and successfully use customized classification models, and (2) the identity of participants in the study is protected. extensive comparative empirical investigation shows that different algorithms trained on the stimuli are able to generalize successfully on the same task as the original model. architectural and algorithmic similarity between new and original models play an important role in performance. for similar architectures, the performance is close to that of using the original data (e.g., accuracy difference of 0.56%-3.82%, kappa coefficient difference of 0.02-0.08). further experiments show that the stimuli can provide state-ofthe-art resilience against adversarial association and membership inference attacks.",2021
on the online coalition structure generation problem,"we consider the online version of the coalition structure generation problem, in which agents, corresponding to the vertices of a graph, appear in an online fashion and have to be partitioned into coalitions by an authority (i.e., an online algorithm). when an agent appears, the algorithm has to decide whether to put the agent into an existing coalition or to create a new one containing, at this moment, only her. the decision is irrevocable. the objective is partitioning agents into coalitions so as to maximize the resulting social welfare that is the sum of all coalition values. we consider two cases for the value of a coalition: (1) the sum of the weights of its edges, and (2) the sum of the weights of its edges divided by its size. coalition structures appear in a variety of application in ai, multi-agent systems, networks, as well as in social networks, data analysis, computational biology, game theory, and scheduling. for each of the coalition value functions we consider the bounded and unbounded cases depending on whether or not the size of a coalition can exceed a given value α. furthermore, we consider the case of a limited number of coalitions and various weight functions for the edges, i.e., unrestricted, positive and constant weights. we show tight or nearly tight bounds for the competitive ratio in each case.",2021
learning optimal decision sets and lists with sat,"decision sets and decision lists are two of the most easily explainable machine learning models. given the renewed emphasis on explainable machine learning decisions, both of these machine learning models are becoming increasingly attractive, as they combine small size and clear explainability. in this paper, we define size as the total number of literals in the sat encoding of these rule-based models as opposed to earlier work that concentrates on the number of rules. in this paper, we develop approaches to computing minimum-size “perfect” decision sets and decision lists, which are perfectly accurate on the training data, and minimal in size, making use of modern sat solving technology. we also provide a new method for determining optimal sparse alternatives, which trade off size and accuracy. the experiments in this paper demonstrate that the optimal decision sets computed by the sat-based approach are comparable with the best heuristic methods, but much more succinct, and thus, more explainable. we contrast the size and test accuracy of optimal decisions lists versus optimal decision sets, as well as other state-of-the-art methods for determining optimal decision lists. finally, we examine the size of average explanations generated by decision sets and decision lists.",2021
a word selection method for producing interpretable distributional semantic word vectors,"distributional semantic models represent the meaning of words as vectors. we introduce a selection method to learn a vector space that each of its dimensions is a natural word. the selection method starts from the most frequent words and selects a subset, which has the best performance. the method produces a vector space that each of its dimensions is a word. this is the main advantage of the method compared to fusion methods such as nmf, and neural embedding models. we apply the method to the ukwac corpus and train a vector space of n=1500 basis words. we report tests results on word similarity tasks for men, rg-65, simlex-999, and wordsim353 gold datasets. also, results show that reducing the number of basis vectors from 5000 to 1500 reduces accuracy by about 1.5-2%. so, we achieve good interpretability without a large penalty. interpretability evaluation results indicate that the word vectors obtained by the proposed method using n=1500 are more interpretable than word embedding models, and the baseline method. we report the top 15 words of 1500 selected basis words in this paper.",2021
quantum mathematics in artificial intelligence,"in the decade since 2010, successes in artificial intelligence have been at the forefront of computer science and technology, and vector space models have solidified a position at the forefront of artificial intelligence. at the same time, quantum computers have become much more powerful, and announcements of major advances are frequently in the news. the mathematical techniques underlying both these areas have more in common than is sometimes realized. vector spaces took a position at the axiomatic heart of quantum mechanics in the 1930s, and this adoption was a key motivation for the derivation of logic and probability from the linear geometry of vector spaces. quantum interactions between particles are modelled using the tensor product, which is also used to express objects and operations in artificial neural networks. this paper describes some of these common mathematical areas, including examples of how they are used in artificial intelligence (ai), particularly in automated reasoning and natural language processing (nlp). techniques discussed include vector spaces, scalar products, subspaces and implication, orthogonal projection and negation, dual vectors, density matrices, positive operators, and tensor products. application areas include information retrieval, categorization and implication, modelling word-senses and disambiguation, inference in knowledge bases, and semantic composition. some of these approaches can potentially be implemented on quantum hardware. many of the practical steps in this implementation are in early stages, and some are already realized. explaining some of the common mathematical tools can help researchers in both ai and quantum computing further exploit these overlaps, recognizing and exploring new directions along the way.",2021
the rediscovery hypothesis: language models need to meet linguistics,"there is an ongoing debate in the nlp community whether modern language models contain linguistic knowledge, recovered through so-called probes. in this paper, we study whether linguistic knowledge is a necessary condition for the good performance of modern language models, which we call the rediscovery hypothesis. in the first place, we show that language models that are significantly compressed but perform well on their pretraining objectives retain good scores when probed for linguistic structures. this result supports the rediscovery hypothesis and leads to the second contribution of our paper: an information-theoretic framework that relates language modeling objectives with linguistic information. this framework also provides a metric to measure the impact of linguistic information on the word prediction task. we reinforce our analytical results with various experiments, both on synthetic and on real nlp tasks in english.",2021
learning from disagreement: a survey,"many tasks in natural language processing (nlp) and computer vision (cv) offer evidence that humans disagree, from objective tasks such as part-of-speech tagging to more subjective tasks such as classifying an image or deciding whether a proposition follows from certain premises. while most learning in artificial intelligence (ai) still relies on the assumption that a single (gold) interpretation exists for each item, a growing body of research aims to develop learning methods that do not rely on this assumption. in this survey, we review the evidence for disagreements on nlp and cv tasks, focusing on tasks for which substantial datasets containing this information have been created. we discuss the most popular approaches to training models from datasets containing multiple judgments potentially in disagreement. we systematically compare these different approaches by training them with each of the available datasets, considering several ways to evaluate the resulting models. finally, we discuss the results in depth, focusing on four key research questions, and assess how the type of evaluation and the characteristics of a dataset determine the answers to these questions. our results suggest, first of all, that even if we abandon the assumption of a gold standard, it is still essential to reach a consensus on how to evaluate models. this is because the relative performance of the various training methods is critically affected by the chosen form of evaluation. secondly, we observed a strong dataset effect. with substantial datasets, providing many judgments by high-quality coders for each item, training directly with soft labels achieved better results than training from aggregated or even gold labels. this result holds for both hard and soft evaluation. but when the above conditions do not hold, leveraging both gold and soft labels generally achieved the best results in the hard evaluation. all datasets and models employed in this paper are freely available as supplementary materials.",2021
constraint-based diversification of jop gadgets,"modern software deployment process produces software that is uniform, and hence vulnerable to large-scale code-reuse attacks, such as jump-oriented programming (jop) attacks. compiler-based diversification improves the resilience and security of software systems by automatically generating different assembly code versions of a given program. existing techniques are efficient but do not have a precise control over the quality, such as the code size or speed, of the generated code variants. this paper introduces diversity by construction (divcon), a constraint-based compiler approach to software diversification. unlike previous approaches, divcon allows users to control and adjust the conflicting goals of diversity and code quality. a key enabler is the use of large neighborhood search (lns) to generate highly diverse assembly code efficiently. for larger problems, we propose a combination of lns with a structural decomposition of the problem. to further improve the diversification efficiency of divcon against jop attacks, we propose an application-specific distance measure tailored to the characteristics of jop attacks. we evaluate divcon with 20 functions from a popular benchmark suite for embedded systems. these experiments show that divcon's combination of lns and our application-specific distance measure generates binary programs that are highly resilient against jop attacks (they share between 0.15% to 8% of jop gadgets) with an optimality gap of 10%. our results confirm that there is a trade-off between the quality of each assembly code version and the diversity of the entire pool of versions. in particular, the experiments show that divcon is able to generate binary programs that share a very small number of gadgets, while delivering near-optimal code. for constraint programming researchers and practitioners, this paper demonstrates that lns is a valuable technique for finding diverse solutions. for security researchers and software engineers, divcon extends the scope of compiler-based diversification to performance-critical and resource-constrained applications.",2021
on the cluster admission problem for cloud computing,"cloud computing providers face the problem of matching heterogeneous customer workloads to resources that will serve them. this is particularly challenging if customers, who are already running a job on a cluster, scale their resource usage up and down over time. the provider therefore has to continuously decide whether she can add additional workloads to a given cluster or if doing so would impact existing workloads’ ability to scale. currently, this is often done using simple threshold policies to reserve large parts of each cluster, which leads to low efficiency (i.e., low average utilization of the cluster). we propose more sophisticated policies for controlling admission to a cluster and demonstrate that they significantly increase cluster utilization. we first introduce the cluster admission problem and formalize it as a constrained partially observable markov decision process (pomdp). as it is infeasible to solve the pomdp optimally, we then systematically design admission policies that estimate moments of each workload’s distribution of future resource usage. via extensive simulations grounded in a trace from microsoft azure, we show that our admission policies lead to a substantial improvement over the simple threshold policy. we then show that substantial further gains are possible if high-quality information is available about arriving workloads. based on this, we propose an information elicitation approach to incentivize users to provide this information and simulate its effects.",2021
"game plan: what ai can do for football, and what football can do for ai","the rapid progress in artificial intelligence (ai) and machine learning has opened unprecedented analytics possibilities in various team and individual sports, including baseball, basketball, and tennis. more recently, ai techniques have been applied to football, due to a huge increase in data collection by professional teams, increased computational power, and advances in machine learning, with the goal of better addressing new scientific challenges involved in the analysis of both individual players’ and coordinated teams’ behaviors. the research challenges associated with predictive and prescriptive football analytics require new developments and progress at the intersection of statistical learning, game theory, and computer vision. in this paper, we provide an overarching perspective highlighting how the combination of these fields, in particular, forms a unique microcosm for ai research, while offering mutual benefits for professional teams, spectators, and broadcasters in the years to come. we illustrate that this duality makes football analytics a game changer of tremendous value, in terms of not only changing the game of football itself, but also in terms of what this domain can mean for the field of ai. we review the state-of-the-art and exemplify the types of analysis enabled by combining the aforementioned fields, including illustrative examples of counterfactual analysis using predictive models, and the combination of game-theoretic analysis of penalty kicks with statistical learning of player attributes. we conclude by highlighting envisioned downstream impacts, including possibilities for extensions to other sports (real and virtual).",2021
efficient local search based on dynamic connectivity maintenance for minimum connected dominating set,"the minimum connected dominating set (mcds) problem is an important extension of the minimum dominating set problem, with wide applications, especially in wireless networks. most previous works focused on solving mcds problem in graphs with relatively small size, mainly due to the complexity of maintaining connectivity. this paper explores techniques for solving mcds problem in massive real-world graphs with wide practical importance. firstly, we propose a local greedy construction method with reasoning rule called 1hopreason. secondly and most importantly, a hybrid dynamic connectivity maintenance method (hdc+) is designed to switch alternately between a novel fast connectivity maintenance method based on spanning tree and its previous counterpart. thirdly, we adopt a two-level vertex selection heuristic with a newly proposed scoring function called chronosafety to make the algorithm more considerate when selecting vertices. we design a new local search algorithm called fastcds based on the three ideas. experiments show that fastcds significantly outperforms five state-of-the-art mcds algorithms on both massive graphs and classic benchmarks.",2021
learning over no-preferred and preferred sequence of items for robust recommendation,"in this paper, we propose a theoretically supported sequential strategy for training a large-scale recommender system (rs) over implicit feedback, mainly in the form of clicks. the proposed approach consists in minimizing pairwise ranking loss over blocks of consecutive items constituted by a sequence of non-clicked items followed by a clicked one for each user. we present two variants of this strategy where model parameters are updated using either the momentum method or a gradient-based approach. to prevent updating the parameters for an abnormally high number of clicks over some targeted items (mainly due to bots), we introduce an upper and a lower threshold on the number of updates for each user. these thresholds are estimated over the distribution of the number of blocks in the training set. they affect the decision of rs by shifting the distribution of items that are shown to the users. furthermore, we provide a convergence analysis of both algorithms and demonstrate their practical efficiency over six large-scale collections with respect to various ranking measures and computational time.",2021
welfare guarantees in schelling segregation,"schelling’s model is an influential model that reveals how individual perceptions and incentives can lead to residential segregation. inspired by a recent stream of work, we study welfare guarantees and complexity in this model with respect to several welfare measures. first, we show that while maximizing the social welfare is np-hard, computing an assignment of agents to the nodes of any topology graph with approximately half of the maximum welfare can be done in polynomial time. we then consider pareto optimality, introduce two new optimality notions based on it, and establish mostly tight bounds on the worst-case welfare loss for assignments satisfying these notions as well as the complexity of computing such assignments. in addition, we show that for tree topologies, it is possible to decide whether there exists an assignment that gives every agent a positive utility in polynomial time; moreover, when every node in the topology has degree at least 2, such an assignment always exists and can be found efficiently.",2021
viewpoint: ai as author bridging the gap between machine learning and literary theory,"anticipating the rise in artificial intelligence’s ability to produce original works of literature, this study suggests that literariness, or that which constitutes a text as literary, is understudied in relation to text generation. from a computational perspective, literature is particularly challenging because it typically employs figurative and ambiguous language. literary expertise would be beneficial to understanding how meaning and emotion are conveyed in this art form but is often overlooked. we propose placing experts from two dissimilar disciplines – machine learning and literary studies – in conversation to improve the quality of ai writing. concentrating on evaluation as a vital stage in the text generation process, the study demonstrates that benefit could be derived from literary theoretical perspectives. this knowledge would improve algorithm design and enable a deeper understanding of how ai learns and generates. this article appears in the special track on ai and society.",2021
"measuring the occupational impact of ai: tasks, cognitive abilities and ai benchmarks","in this paper we develop a framework for analysing the impact of artificial intelligence (ai) on occupations. this framework maps 59 generic tasks from worker surveys and an occupational database to 14 cognitive abilities (that we extract from the cognitive science literature) and these to a comprehensive list of 328 ai benchmarks used to evaluate research intensity across a broad range of different ai areas. the use of cognitive abilities as an intermediate layer, instead of mapping work tasks to ai benchmarks directly, allows for an identification of potential ai exposure for tasks for which ai applications have not been explicitly created. an application of our framework to occupational databases gives insights into the abilities through which ai is most likely to affect jobs and allows for a ranking of occupations with respect to ai exposure. moreover, we show that some jobs that were not known to be affected by previous waves of automation may now be subject to higher ai exposure. finally, we find that some of the abilities where ai research is currently very intense are linked to tasks with comparatively limited labour input in the labour markets of advanced economies (e.g., visual and auditory processing using deep learning, and sensorimotor interaction through (deep) reinforcement learning). this article appears in the special track on ai and society.",2021
rwne: a scalable random-walk based network embedding framework with personalized higher-order proximity preserved,"higher-order proximity preserved network embedding has attracted increasing attention. in particular, due to the superior scalability, random-walk-based network embedding has also been well developed, which could efficiently explore higher-order neighborhoods via multi-hop random walks. however, despite the success of current random-walk-based methods, most of them are usually not expressive enough to preserve the personalized higher-order proximity and lack a straightforward objective to theoretically articulate what and how network proximity is preserved. in this paper, to address the above issues, we present a general scalable random-walk-based network embedding framework, in which random walk is explicitly incorporated into a sound objective designed theoretically to preserve arbitrary higher-order proximity. further, we introduce the random walk with restart process into the framework to naturally and effectively achieve personalized-weighted preservation of proximities of different orders. we conduct extensive experiments on several real-world networks and demonstrate that our proposed method consistently and substantially outperforms the state-of-the-art network embedding methods.",2021
declarative algorithms and complexity results for assumption-based argumentation,"the study of computational models for argumentation is a vibrant area of artificial intelligence and, in particular, knowledge representation and reasoning research. arguments most often have an intrinsic structure made explicit through derivations from more basic structures. computational models for structured argumentation enable making the internal structure of arguments explicit. assumption-based argumentation (aba) is a central structured formalism for argumentation in ai. in this article, we make both algorithmic and complexity-theoretic advances in the study of aba. in terms of algorithms, we propose a new approach to reasoning in a commonly studied fragment of aba (namely the logic programming fragment) with and without preferences. while previous approaches to reasoning over aba frameworks apply either specialized algorithms or translate aba reasoning to reasoning over abstract argumentation frameworks, we develop a direct declarative approach to aba reasoning by encoding aba reasoning tasks in answer set programming. we show via an extensive empirical evaluation that our approach significantly improves on the empirical performance of current aba reasoning systems. in terms of computational complexity, while the complexity of reasoning over aba frameworks is well-understood, the complexity of reasoning in the aba+ formalism integrating preferences into aba is currently not fully established. towards bridging this gap, our results suggest that the integration of preferential information into aba via so-called reverse attacks results in increased problem complexity for several central argumentation semantics.",2021
playing codenames with language graphs and word embeddings,"although board games and video games have been studied for decades in artificial intelligence research, challenging word games remain relatively unexplored. word games are not as constrained as games like chess or poker. instead, word game strategy is defined by the players’ understanding of the way words relate to each other. the word game codenames provides a unique opportunity to investigate common sense understanding of relationships between words, an important open challenge. we propose an algorithm that can generate codenames clues from the language graph babelnet or from any of several embedding methods – word2vec, glove, fasttext or bert. we introduce a new scoring function that measures the quality of clues, and we propose a weighting term called detect that incorporates dictionary-based word representations and document frequency to improve clue selection. we develop babelnet-word selection framework (babelnet-wsf) to improve babelnet clue quality and overcome the computational barriers that previously prevented leveraging language graphs for codenames. extensive experiments with human evaluators demonstrate that our proposed innovations yield state-of-the-art performance, with up to 102.8% improvement in precision@2 in some cases. overall, this work advances the formal study of word games and approaches for common sense language understanding.",2021
a tight bound for stochastic submodular cover,"we show that the adaptive greedy algorithm of golovin and krause achieves an approximation bound of (ln(q/η)+1) for stochastic submodular cover: here q is the “goal value” and η is the minimum gap between q and any attainable utility value q'&lt;q. although this bound was claimed by golovin and krause in the original version of their paper, the proof was later shown to be incorrect by nan &amp; saligrama. the subsequent corrected proof of golovin and krause gives a quadratic bound of (ln(q/η)+1)2. a bound of 56(ln(q/η)+1) is implied by work of im et al. other bounds for the problem depend on quantities other than q and η. our bound restores the original bound claimed by golovin and krause, generalizing the well-known (ln m + 1) approximation bound on the greedy algorithm for the classical set cover problem, where m is the size of the ground set.",2021
multi-document summarization with determinantal point process attention,"the ability to convey relevant and diverse information is critical in multi-document summarization and yet remains elusive for neural seq-to-seq models whose outputs are often redundant and fail to correctly cover important details. in this work, we propose an attention mechanism which encourages greater focus on relevance and diversity. attention weights are computed based on (proportional) probabilities given by determinantal point processes (dpps) defined on the set of content units to be summarized. dpps have been successfully used in extractive summarisation, here we use them to select relevant and diverse content for neural abstractive summarisation. we integrate dpp-based attention with various seq-to-seq architectures ranging from cnns to lstms, and transformers. experimental evaluation shows that our attention mechanism consistently improves summarization and delivers performance comparable with the state-of-the-art on the multinews dataset",2021
representative committees of peers,"a population of voters must elect representatives among themselves to decide on a sequence of possibly unforeseen binary issues. voters care only about the final decision, not the elected representatives. the disutility of a voter is proportional to the fraction of issues, where his preferences disagree with the decision. while an issue-by-issue vote by all voters would maximize social welfare, we are interested in how well the preferences of the population can be approximated by a small committee. we show that a k-sortition (a random committee of k voters with the majority vote within the committee) leads to an outcome within the factor 1+o(1/√ k) of the optimal social cost for any number of voters n, any number of issuesm, and any preference profile.for a small number of issues m, the social cost can be made even closer to optimal by delegation procedures that weigh committee members according to their number of followers. however, for large m, we demonstrate that the k-sortition is the worst-case optimal rule within a broad family of committee-based rules that take into account metric information about the preference profile of the whole population.",2021
confronting abusive language online: a survey from the ethical and human rights perspective,"the pervasiveness of abusive content on the internet can lead to severe psychological and physical harm. significant effort in natural language processing (nlp) research has been devoted to addressing this problem through abusive content detection and related sub-areas, such as the detection of hate speech, toxicity, cyberbullying, etc. although current technologies achieve high classification performance in research studies, it has been observed that the real-life application of this technology can cause unintended harms, such as the silencing of under-represented groups. we review a large body of nlp research on automatic abuse detection with a new focus on ethical challenges, organized around eight established ethical principles: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and promotion of human values. in many cases, these principles relate not only to situational ethical codes, which may be context-dependent, but are in fact connected to universal human rights, such as the right to privacy, freedom from discrimination, and freedom of expression. we highlight the need to examine the broad social impacts of this technology, and to bring ethical and human rights considerations to every stage of the application life-cycle, from task formulation and dataset design, to model training and evaluation, to application deployment. guided by these principles, we identify several opportunities for rights-respecting, socio-technical solutions to detect and confront online abuse, including ‘nudging’, ‘quarantining’, value sensitive design, counter-narratives, style transfer, and ai-driven public education applications.evaluation, to application deployment. guided by these principles, we identify several opportunities for rights-respecting, socio-technical solutions to detect and confront online abuse, including 'nudging', 'quarantining', value sensitive design, counter-narratives, style transfer, and ai-driven public education applications.",2021
epidemioptim: a toolbox for the optimization of control policies in epidemiological models,"modeling the dynamics of epidemics helps to propose control strategies based on pharmaceuticaland non-pharmaceutical interventions (contact limitation, lockdown, vaccination,etc). hand-designing such strategies is not trivial because of the number of possibleinterventions and the difficulty to predict long-term effects. this task can be cast as an optimization problem where state-of-the-art machine learning methods such as deep reinforcement learning might bring significant value. however, the specificity of each domain|epidemic modeling or solving optimization problems|requires strong collaborationsbetween researchers from different fields of expertise. this is why we introduce epidemioptim, a python toolbox that facilitates collaborations between researchers inepidemiology and optimization. epidemioptim turns epidemiological models and cost functions into optimization problems via a standard interface commonly used by optimization practitioners (openai gym). reinforcement learning algorithms based on qlearning with deep neural networks (dqn) and evolutionary algorithms (nsga-ii) are already implemented. we illustrate the use of epidemioptim to find optimal policies fordynamical on-o lockdown control under the optimization of the death toll and economic recess using a susceptible-exposed-infectious-removed (seir) model for covid-19. using epidemioptim and its interactive visualization platform in jupyter notebooks, epidemiologists, optimization practitioners and others (e.g. economists) can easily compare epidemiological models, costs functions and optimization algorithms to address important choicesto be made by health decision-makers. trained models can be explored by experts and non-experts via a web interface. this article is part of the special track on ai and covid-19.",2021
intelligence in strategic games,"if an agent, or a coalition of agents, has a strategy, knows that she has a strategy, and knows what the strategy is, then she has a know-how strategy. several modal logics of coalition power for know-how strategies have been studied before. the contribution of the article is three-fold. first, it proposes a new class of know-how strategies that depend on the intelligence information about the opponents’ actions. second, it shows that the coalition power modality for the proposed new class of strategies cannot be expressed through the standard know-how modality. third, it gives a sound and complete logical system that describes the interplay between the coalition power modality with intelligence and the distributed knowledge modality in games with imperfect information.",2021
conceptual modeling of explainable recommender systems: an ontological formalization to guide their design and development,"with the increasing importance of e-commerce and the immense variety of products, users need help to decide which ones are the most interesting to them. this is one of the main goals of recommender systems. however, users’ trust may be compromised if they do not understand how or why the recommendation was achieved. here, explanations are essential to improve user confidence in recommender systems and to make the recommendation useful. providing explanation capabilities into recommender systems is not an easy task as their success depends on several aspects such as the explanation’s goal, the user’s expectation, the knowledge available, or the presentation method. therefore, this work proposes a conceptual model to alleviate this problem by defining the requirements of explanations for recommender systems. our goal is to provide a model that guides the development of effective explanations for recommender systems as they are correctly designed and suited to the user’s needs. although earlier explanation taxonomies sustain this work, our model includes new concepts not considered in previous works. moreover, we make a novel contribution regarding the formalization of this model as an ontology that can be integrated into the development of proper explanations for recommender systems.",2021
ethics and governance of artificial intelligence: evidence from a survey of machine learning researchers,"machine learning (ml) and artificial intelligence (ai) researchers play an important role in the ethics and governance of ai, including through their work, advocacy, and choice of employment. nevertheless, this influential group's attitudes are not well understood, undermining our ability to discern consensuses or disagreements between ai/ml researchers. to examine these researchers' views, we conducted a survey of those who published in two top ai/ml conferences (n = 524). we compare these results with those from a 2016 survey of ai/ml researchers (grace et al., 2018) and a 2018 survey of the us public (zhang &amp; dafoe, 2020). we find that ai/ml researchers place high levels of trust in international organizations and scientific organizations to shape the development and use of ai in the public interest; moderate trust in most western tech companies; and low trust in national militaries, chinese tech companies, and facebook. while the respondents were overwhelmingly opposed to ai/ml researchers working on lethal autonomous weapons, they are less opposed to researchers working on other military applications of ai, particularly logistics algorithms. a strong majority of respondents think that ai safety research should be prioritized and that ml institutions should conduct pre-publication review to assess potential harms. being closer to the technology itself, ai/ml researchers are well placed to highlight new risks and develop technical solutions, so this novel attempt to measure their attitudes has broad relevance. the findings should help to improve how researchers, private sector executives, and policymakers think about regulations, governance frameworks, guiding principles, and national and international governance strategies for ai. this article appears in the special track on ai &amp; society.",2021
improving the effectiveness and efficiency of stochastic neighbour embedding with isolation kernel,"this paper presents a new insight into improving the performance of stochastic neighbour embedding (t-sne) by using isolation kernel instead of gaussian kernel. isolation kernel outperforms gaussian kernel in two aspects. first, the use of isolation kernel in t-sne overcomes the drawback of misrepresenting some structures in the data, which often occurs when gaussian kernel is applied in t-sne. this is because gaussian kernel determines each local bandwidth based on one local point only, while isolation kernel is derived directly from the data based on space partitioning. second, the use of isolation kernel yields a more efficient similarity computation because data-dependent isolation kernel has only one parameter that needs to be tuned. in contrast, the use of data-independent gaussian kernel increases the computational cost by determining n bandwidths for a dataset of n points. as the root cause of these deficiencies in t-sne is gaussian kernel, we show that simply replacing gaussian kernel with isolation kernel in t-sne significantly improves the quality of the final visualisation output (without creating misrepresented structures) and removes one key obstacle that prevents t-sne from processing large datasets. moreover, isolation kernel enables t-sne to deal with large-scale datasets in less runtime without trading off accuracy, unlike existing methods in speeding up t-sne.",2021
goal recognition for deceptive human agents through planning and gaze,"eye gaze has the potential to provide insight into the minds of individuals, and this idea has been used in prior research to improve human goal recognition by combining human's actions and gaze. however, most existing research assumes that people are rational and honest. in adversarial scenarios, people may deliberately alter their actions and gaze, which presents a challenge to goal recognition systems. in this paper, we present new models for goal recognition under deception using a combination of gaze behaviour and observed movements of the agent. these models aim to detect when a person is deceiving by analysing their gaze patterns and use this information to adjust the goal recognition. we evaluated our models in two human-subject studies: (1) using data collected from 30 individuals playing a navigation game inspired by an existing deception study and (2) using data collected from 40 individuals playing a competitive game (ticket to ride). we found that one of our models (modulated deception gaze+ontic) offers promising results compared to the previous state-of-the-art model in both studies. our work complements existing adversarial goal recognition systems by equipping these systems with the ability to tackle ambiguous gaze behaviours.",2021
dimensional inconsistency measures and postulates in spatio-temporal databases,"the problem of managing spatio-temporal data arises in many applications, such as location-based services, environmental monitoring, geographic information systems, and many others. often spatio-temporal data arising from such applications turn out to be inconsistent, i.e., representing an impossible situation in the real world. though several inconsistency measures have been proposed to quantify in a principled way inconsistency in propositional knowledge bases, little effort has been done so far on inconsistency measures tailored for the spatio-temporal setting. in this paper, we define and investigate new measures that are particularly suitable for dealing with inconsistent spatio-temporal information, because they explicitly take into account the spatial and temporal dimensions, as well as the dimension concerning the identifiers of the monitored objects. specifically, we first define natural measures that look at individual dimensions (time, space, and objects), and then propose measures based on the notion of a repair. we then analyze their behavior w.r.t. common postulates defined for classical propositional knowledge bases, and find that the latter are not suitable for spatio-temporal databases, in that the proposed inconsistency measures do not often satisfy them. in light of this, we argue that also postulates should explicitly take into account the spatial, temporal, and object dimensions and thus define “dimension-aware” counterparts of common postulates, which are indeed often satisfied by the new inconsistency measures. finally, we study the complexity of the proposed inconsistency measures.",2021
merge-and-shrink: a compositional theory of transformations of factored transition systems,"the merge-and-shrink framework has been introduced as a general approach for defining abstractions of large state spaces arising in domain-independent planning and related areas. the distinguishing characteristic of the merge-and-shrink approach is that it operates directly on the factored representation of state spaces, repeatedly modifying this representation through transformations such as shrinking (abstracting a factor of the representation), merging (combining two factors), label reduction (abstracting the way in which different factors interact), and pruning (removing states or transitions of a factor). we provide a novel view of the merge-and-shrink framework as a “toolbox” or “algebra” of transformations on factored transition systems, with the construction of abstractions as only one possible application. for each transformation, we study desirable properties such as conservativeness (overapproximating the original transition system), inducedness (absence of spurious states and transitions), and refinability (reconstruction of paths in the original transition system from the transformed one). we provide the first complete characterizations of the conditions under which these desirable properties can be achieved. we also provide the first full formal account of factored mappings, the mechanism used within the merge-and-shrink framework to establish the relationship between states in the original and transformed factored transition system. unlike earlier attempts to develop a theory for merge-and-shrink, our approach is fully compositional: the properties of a sequence of transformations can be entirely understood by the properties of the individual transformations involved. this aspect is key to the use of merge-and-shrink as a general toolbox for transforming factored transition systems. new transformations can easily be added to our theory, with compositionality taking care of the seamless integration with the existing components. similarly, new properties of transformations can be integrated into the theory by showing their compositionality and studying under which conditions they are satisfied by the building blocks of merge-and-shrink.",2021
set-to-sequence methods in machine learning: a review,"machine learning on sets towards sequential output is an important and ubiquitous task, with applications ranging from language modelling and meta-learning to multi-agent strategy games and power grid optimization. combining elements of representation learning and structured prediction, its two primary challenges include obtaining a meaningful, permutation invariant set representation and subsequently utilizing this representation to output a complex target permutation. this paper provides a comprehensive introduction to the field as well as an overview of important machine learning methods tackling both of these key challenges, with a detailed qualitative comparison of selected model architectures.",2021
evaluating strategic structures in multi-agent inverse reinforcement learning,"a core question in multi-agent systems is understanding the motivations for an agent's actions based on their behavior. inverse reinforcement learning provides a framework for extracting utility functions from observed agent behavior, casting the problem as finding domain parameters which induce such a behavior from rational decision makers. we show how to efficiently and scalably extend inverse reinforcement learning to multi-agent settings, by reducing the multi-agent problem to n single-agent problems while still satisfying rationality conditions such as strong rationality. however, we observe that rewards learned naively tend to lack insightful structure, which causes them to produce undesirable behavior when optimized in games with different players from those encountered during training. we further investigate conditions under which rewards or utility functions can be precisely identified, on problem domains such as normal-form and markov games, as well as auctions, where we show we can learn reward functions that properly generalize to new settings.",2021
agent-based markov modeling for improved covid-19 mitigation policies,"the year 2020 saw the covid-19 virus lead to one of the worst global pandemics in history. as a result, governments around the world have been faced with the challenge of protecting public health while keeping the economy running to the greatest extent possible. epidemiological models provide insight into the spread of these types of diseases and predict the effects of possible intervention policies. however, to date, even the most data-driven intervention policies rely on heuristics. in this paper, we study how reinforcement learning (rl) and bayesian inference can be used to optimize mitigation policies that minimize economic impact without overwhelming hospital capacity. our main contributions are (1) a novel agent-based pandemic simulator which, unlike traditional models, is able to model fine-grained interactions among people at specific locations in a community; (2) an rlbased methodology for optimizing fine-grained mitigation policies within this simulator; and (3) a hidden markov model for predicting infected individuals based on partial observations regarding test results, presence of symptoms, and past physical contacts. this article is part of the special track on ai and covid-19.",2021
bribery and control in stable marriage,"we initiate the study of external manipulations in stable marriage by considering several manipulative actions as well as several manipulation goals. for instance, one goal is to make sure that a given pair of agents is matched in a stable solution, and this may be achieved by the manipulative action of reordering some agents' preference lists. we present a comprehensive study of the computational complexity of all problems arising in this way. we find several polynomial-time solvable cases as well as np-hard ones. for the np-hard cases, focusing on the natural parameter ""budget"" (that is, the number of manipulative actions one is allowed to perform), we also conduct a parameterized complexity analysis and encounter mostly parameterized hardness results.",2021
"election manipulation on social networks: seeding, edge removal, edge addition","we focus on the election manipulation problem through social influence, where a manipulator exploits a social network to make her most preferred candidate win an election. influence is due to information in favor of and/or against one or multiple candidates, sent by seeds and spreading through the network according to the independent cascade model. we provide a comprehensive theoretical study of the election control problem, investigating two forms of manipulations: seeding to buy influencers given a social network and removing or adding edges in the social network given the set of the seeds and the information sent. in particular, we study a wide range of cases distinguishing in the number of candidates or the kind of information spread over the network. our main result shows that the election manipulation problem is not affordable in the worst-case, even when one accepts to get an approximation of the optimal margin of victory, except for the case of seeding when the number of hard-to-manipulate voters is not too large, and the number of uncertain voters is not too small, where we say that a voter that does not vote for the manipulator's candidate is hard-to-manipulate if there is no way to make her vote for this candidate, and uncertain otherwise. we also provide some results showing the hardness of the problems in special cases. more precisely, in the case of seeding, we show that the manipulation is hard even if the graph is a line and that a large class of algorithms, including most of the approaches recently adopted for social-influence problems (e.g., greedy, degree centrality, pagerank, voterank), fails to compute a bounded approximation even on elementary networks, such as undirected graphs with every node having a degree at most two or directed trees. in the case of edge removal or addition, our hardness results also apply to election manipulation when the manipulator has an unlimited budget, being allowed to remove or add an arbitrary number of edges, and to the basic case of social influence maximization/minimization in the restricted case of finite budget. interestingly, our hardness results for seeding and edge removal/addition still hold in a re-optimization variant, where the manipulator already knows an optimal solution to the problem and computes a new solution once a local modification occurs, e.g., the removal/addition of a single edge.",2021
"probabilistic temporal networks with ordinary distributions: theory, robustness and expected utility","most existing works in probabilistic simple temporal networks (pstns) base their frameworks on well-defined, parametric probability distributions. under the operational contexts of both strong and dynamic control, this paper addresses robustness measure of pstns, i.e. the execution success probability, where the probability distributions of the contingent durations are ordinary, not necessarily parametric, nor symmetric (e.g. histograms, pert), as long as these can be discretized. in practice, one would obtain ordinary distributions by considering empirical observations (compiled as histograms), or even hand-drawn by field experts. in this new realm of pstns, we study and formally define concepts such as degree of weak/strong/dynamic controllability, robustness under a predefined dispatching protocol, and introduce the concept of pstn expected execution utility. we also discuss the limitation of existing controllability levels, and propose new levels within dynamic controllability, to better characterize dynamic controllable pstns based on based practical complexity considerations. we propose a novel fixed-parameter pseudo-polynomial time computation method to obtain both the success probability and expected utility measures. we apply our computation method to various pstn datasets, including realistic planetary exploration scenarios in the context of the mars 2020 rover. moreover, we propose additional original applications of the method.",2021
"socially responsible ai algorithms: issues, purposes, and challenges","in the current era, people and society have grown increasingly reliant on artificial intelligence (ai) technologies. ai has the potential to drive us towards a future in which all of humanity flourishes. it also comes with substantial risks for oppression and calamity. discussions about whether we should (re)trust ai have repeatedly emerged in recent years and in many quarters, including industry, academia, healthcare, services, and so on. technologists and ai researchers have a responsibility to develop trustworthy ai systems. they have responded with great effort to design more responsible ai algorithms. however, existing technical solutions are narrow in scope and have been primarily directed towards algorithms for scoring or classification tasks, with an emphasis on fairness and unwanted bias. to build long-lasting trust between ai and human beings, we argue that the key is to think beyond algorithmic fairness and connect major aspects of ai that potentially cause ai’s indifferent behavior. in this survey, we provide a systematic framework of socially responsible ai algorithms that aims to examine the subjects of ai indifference and the need for socially responsible ai algorithms, define the objectives, and introduce the means by which we may achieve these objectives. we further discuss how to leverage this framework to improve societal well-being through protection, information, and prevention/mitigation. this article appears in the special track on ai &amp; society.",2021
"trends in integration of vision and language research: a survey of tasks, datasets, and methods","interest in artificial intelligence (ai) and its applications has seen unprecedented growth in the last few years. this success can be partly attributed to the advancements made in the sub-fields of ai such as machine learning, computer vision, and natural language processing. much of the growth in these fields has been made possible with deep learning, a sub-area of machine learning that uses artificial neural networks. this has created significant interest in the integration of vision and language. in this survey, we focus on ten prominent tasks that integrate language and vision by discussing their problem formulation, methods, existing datasets, evaluation measures, and compare the results obtained with corresponding state-of-the-art methods. our efforts go beyond earlier surveys which are either task-specific or concentrate only on one type of visual content, i.e., image or video. furthermore, we also provide some potential future directions in this field of research with an anticipation that this survey stimulates innovative thoughts and ideas to address the existing challenges and build new applications.",2021
on the decomposition of abstract dialectical frameworks and the complexity of naive-based semantics,"abstract dialectical frameworks (adfs) are a recently introduced powerful generalization of dung’s popular abstract argumentation frameworks (afs). inspired by similar work for afs, we introduce a decomposition scheme for adfs, which proceeds along the adf’s strongly connected components. we find that, for several semantics, the decomposition-based version coincides with the original semantics, whereas for others, it gives rise to a new semantics. these new semantics allow us to deal with pertinent problems such as odd-length negative cycles in a more general setting, that for instance also encompasses logic programs. we perform an exhaustive analysis of the computational complexity of these new, so-called naive-based semantics. the results are quite interesting, for some of them involve little-known classes of the so-called boolean hierarchy (another hierarchy in between classes of the polynomial hierarchy). furthermore, in credulous and sceptical entailment, the complexity can be different depending on whether we check for truth or falsity of a specific statement.",2021
superintelligence cannot be contained: lessons from computability theory,"superintelligence is a hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human minds. in light of recent advances in machine intelligence, a number of scientists, philosophers and technologists have revived the discussion about the potentially catastrophic risks entailed by such an entity. in this article, we trace the origins and development of the neo-fear of superintelligence, and some of the major proposals for its containment. we argue that total containment is, in principle, impossible, due to fundamental limits inherent to computing itself. assuming that a superintelligence will contain a program that includes all the programs that can be executed by a universal turing machine on input potentially as complex as the state of the world, strict containment requires simulations of such a program, something theoretically (and practically) impossible. this article is part of the special track on ai and society.",2021
integrated offline and online decision making under uncertainty,"this paper considers multi-stage optimization problems under uncertainty that involve distinct offline and online phases. in particular it addresses the issue of integrating these phases to show how the two are often interrelated in real-world applications. our methods are applicable under two (fairly general) conditions: 1) the uncertainty is exogenous; 2) it is possible to define a greedy heuristic for the online phase that can be modeled as a parametric convex optimization problem. we start with a baseline composed by a two-stage offline approach paired with the online greedy heuristic. we then propose multiple methods to tighten the offline/online integration, leading to significant quality improvements, at the cost of an increased computation effort either in the offline or the online phase. overall, our methods provide multiple options to balance the solution quality/time trade-off, suiting a variety of practical application scenarios. to test our methods, we ground our approaches on two real cases studies with both offline and online decisions: an energy management problem with uncertain renewable generation and demand, and a vehicle routing problem with uncertain travel times. the application domains feature respectively continuous and discrete decisions. an extensive analysis of the experimental results shows that indeed offline/online integration may lead to substantial benefits.",2021
zone path construction (zac) based approaches for effective real-time ridesharing,"real-time ridesharing systems such as uberpool, lyft line and grabshare have become hugely popular as they reduce the costs for customers, improve per trip revenue for drivers and reduce traffic on the roads by grouping customers with similar itineraries. the key challenge in these systems is to group the “right” requests to travel together in the “right” available vehicles in real-time, so that the objective (e.g., requests served, revenue or delay) is optimized. this challenge has been addressed in existing work by: (i) generating as many relevant feasible combinations of requests (with respect to the available delay for customers) as possible in real-time; and then (ii) optimizing assignment of the feasible request combinations to vehicles. since the number of request combinations increases exponentially with the increase in vehicle capacity and number of requests, unfortunately, such approaches have to employ ad hoc heuristics to identify a subset of request combinations for assignment. our key contribution is in developing approaches that employ zone (abstraction of individual locations) paths instead of request combinations. zone paths allow for generation of significantly more “relevant” combinations (in comparison to ad hoc heuristics) in real-time than competing approaches due to two reasons: (i) each zone path can typically represent multiple request combinations; (ii) zone paths are generated using a combination of offline and online methods. specifically, we contribute both myopic (ridesharing assignment focussed on current requests only) and non-myopic (ridesharing assignment considers impact on expected future requests) approaches that employ zone paths. in our experimental results, we demonstrate that our myopic approach outperforms the current best myopic approach for ridesharing on both real-world and synthetic datasets (with respect to both objective and runtime). we also show that our non-myopic approach obtains 14.7% improvement over existing myopic approach. our non-myopic approach gets improvements of up to 12.48% over a recent non-myopic approach, neuradp. even when neuradp is allowed to optimize learning over test settings, results largely remain comparable except in a couple of cases, where neuradp performs better.",2021
"cost-optimal planning, delete relaxation, approximability, and heuristics","cost-optimal planning is a very well-studied topic within planning, and it has proven to be computationally hard both in theory and in practice. since cost-optimal planning is an optimisation problem, it is natural to analyse it through the lens of approximation. an important reason for studying cost-optimal planning is heuristic search; heuristic functions that guide the search in planning can often be viewed as algorithms solving or approximating certain optimisation problems. many heuristic functions (such as the ubiquitious h+ heuristic) are based on delete relaxation, which ignores negative effects of actions. planning for instances where the actions have no negative effects is often referred to as monotone planning. the aim of this article is to analyse the approximability of cost-optimal monotone planning, and thus the performance of relevant heuristic functions. our findings imply that it may be beneficial to study these kind of problems within the framework of parameterised complexity and we initiate work in this direction.",2021
learning temporal causal sequence relationships from real-time time-series,"we aim to mine temporal causal sequences that explain observed events (consequents) in time-series traces. causal explanations of key events in a time-series have applications in design debugging, anomaly detection, planning, root-cause analysis and many more. we make use of decision trees and interval arithmetic to mine sequences that explain defining events in the time-series. we propose modified decision tree construction metrics to handle the non-determinism introduced by the temporal dimension. the mined sequences are expressed in a readable temporal logic language that is easy to interpret. the application of the proposed methodology is illustrated through various examples.",2021
a survey on the explainability of supervised machine learning,"predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. insights about the decision making are mostly opaque for humans. particularly understanding the decision making in highly sensitive areas such as healthcare or finance, is of paramount importance. the decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. this survey paper provides essential definitions, an overview of the different principles and methodologies of explainable supervised machine learning (sml). we conduct a state-of-the-art survey that reviews past and recent explainable sml approaches and classifies them according to the introduced definitions. finally, we illustrate principles by means of an explanatory case study and discuss important future directions.",2021
efficient multi-objective reinforcement learning via multiple-gradient descent with iteratively discovered weight-vector sets,"solving multi-objective optimization problems is important in various applications where users are interested in obtaining optimal policies subject to multiple (yet often conflicting) objectives. a typical approach to obtain the optimal policies is to first construct a loss function based on the scalarization of individual objectives and then derive optimal policies that minimize the scalarized loss function. albeit simple and efficient, the typical approach provides no insights/mechanisms on the optimization of multiple objectives due to the lack of ability to quantify the inter-objective relationship. to address the issue, we propose to develop a new efficient gradient-based multi-objective reinforcement learning approach that seeks to iteratively uncover the quantitative inter-objective relationship via finding a minimum-norm point in the convex hull of the set of multiple policy gradients when the impact of one objective on others is unknown a priori. in particular, we first propose a new paols algorithm that integrates pruning and approximate optimistic linear support algorithm to efficiently discover the weight-vector sets of multiple gradients that quantify the inter-objective relationship. then we construct an actor and a multi-objective critic that can co-learn the policy and the multi-objective vector value function. finally, the weight discovery process and the policy and vector value function learning process can be iteratively executed to yield stable weight-vector sets and policies. to validate the effectiveness of the proposed approach, we present a quantitative evaluation of the approach based on three case studies.",2021
the computational complexity of understanding binary classifier decisions,"for a d-ary boolean function φ: {0, 1}d → {0, 1} and an assignment to its variables x = (x1, x2, . . . , xd) we consider the problem of finding those subsets of the variables that are sufficient to determine the function value with a given probability δ. this is motivated by the task of interpreting predictions of binary classifiers described as boolean circuits, which can be seen as special cases of neural networks. we show that the problem of deciding whether such subsets of relevant variables of limited size k ≤ d exist is complete for the complexity class nppp and thus, generally, unfeasible to solve. we then introduce a variant, in which it suffices to check whether a subset determines the function value with probability at least δ or at most δ − γ for 0 &lt; γ &lt; δ. this promise of a probability gap reduces the complexity to the class npbpp. finally, we show that finding the minimal set of relevant variables cannot be reasonably approximated, i.e. with an approximation factor d1−α for α &gt; 0, by a polynomial time algorithm unless p = np. this holds even with the promise of a probability gap.",2021
hybrid-order network consensus for distributed multi-agent systems,"as an important field of distributed artificial intelligence (dai), multi-agent systems (mass) have attracted the attention of extensive research scholars. consensus as the most important issue in mas, much progress has been made in studying the consensus control of mas, but there are some problems remained largely unaddressed which cause the mas to lose some useful network structure information. first, multi-agent consensus protocol usually proceeds over the low-order structure by only considering the direct edges between agents, but ignores the higher-order structure of the whole topology network. second, the existing work assumes all the edges in a topology network have the same weight without exploring the potential diversity of the connections. in this way, multi-agent systems fail to enforce consensus, resulting in fragmentation into multiple clusters. to address the above issues, this paper proposes a motif-aware weighted multi-agent system (mwms) method for consensus control. we focus more on triangle motif in the network, but it can be extended to other kinds of motifs as well. first, a novel weighted network is used which is the combination of the edge-based lower-order structure and the motif-based higher-order structure, i.e., hybrid-order structure. subsequently, by simultaneously considering the quantity and the quality of the connections in the network, a novel consensus framework for mas is designed to update agents. then, two baseline consensus algorithms are used in mwms. in our experiments, we use ten topologies of different shapes, densities and ranges to comprehensively analyze the performance of our proposed algorithms. the simulation results show that the hybrid higher-order network can effectively enhance the consensus of the multi-agent system in different network topologies.",2021
benchmark and survey of automated machine learning frameworks,"machine learning (ml) has become a vital part in many aspects of our daily life. however, building well performing machine learning applications requires highly specialized data scientists and domain experts. automated machine learning (automl) aims to reduce the demand for data scientists by enabling domain experts to build machine learning applications automatically without extensive knowledge of statistics and machine learning. this paper is a combination of a survey on current automl methods and a benchmark of popular automl frameworks on real data sets. driven by the selected frameworks for evaluation, we summarize and review important automl techniques and methods concerning every step in building an ml pipeline. the selected automl frameworks are evaluated on 137 data sets from established automl benchmark suites.",2021
on super strong eth,"multiple known algorithmic paradigms (backtracking, local search and the polynomial method) only yield a 2n(1-1/o(k)) time algorithm for k-sat in the worst case. for this reason, it has been hypothesized that the worst-case k-sat problem cannot be solved in 2n(1-f(k)/k) time for any unbounded function f. this hypothesis has been called the ""super-strong eth"", modelled after the eth and the strong eth. it has also been hypothesized that k-sat is hard to solve for randomly chosen instances near the ""critical threshold"", where the clause-to-variable ratio is such that randomly chosen instances are satisfiable with probability 1/2. we give a randomized algorithm which refutes the super-strong eth for the case of random k-sat and planted k-sat for any clause-to-variable ratio. for example, given any random k-sat instance f with n variables and m clauses, our algorithm decides satisfiability for f in 2n(1-c*log(k)/k) time with high probability (over the choice of the formula and the randomness of the algorithm). it turns out that a well-known algorithm from the literature on sat algorithms does the job: the ppz algorithm of paturi, pudlak, and zane (1999). the unique k-sat problem is the special case where there is at most one satisfying assignment. improving prior reductions, we show that the super-strong eths for unique k-sat and k-sat are equivalent. more precisely, we show the time complexities of unique k-sat and k-sat are very tightly correlated: if unique k-sat is in 2n(1-f(k)/k) time for an unbounded f, then k-sat is in 2n(1-f(k)/(2k)) time.",2021
general value function networks,"state construction is important for learning in partially observable environments. a general purpose strategy for state construction is to learn the state update using a recurrent neural network (rnn), which updates the internal state using the current internal state and the most recent observation. this internal state provides a summary of the observed sequence, to facilitate accurate predictions and decision-making. at the same time, specifying and training rnns is notoriously tricky, particularly as the common strategy to approximate gradients back in time, called truncated back-prop through time (bptt), can be sensitive to the truncation window. further, domain-expertise—which can usually help constrain the function class and so improve trainability—can be difficult to incorporate into complex recurrent units used within rnns. in this work, we explore how to use multi-step predictions to constrain the rnn and incorporate prior knowledge. in particular, we revisit the idea of using predictions to construct state and ask: does constraining (parts of) the state to consist of predictions about the future improve rnn trainability? we formulate a novel rnn architecture, called a general value function network (gvfn), where each internal state component corresponds to a prediction about the future represented as a value function. we first provide an objective for optimizing gvfns, and derive several algorithms to optimize this objective. we then show that gvfns are more robust to the truncation level, in many cases only requiring one-step gradient updates.",2021
an external knowledge enhanced graph-based neural network for sentence ordering,"as an important text coherence modeling task, sentence ordering aims to coherently organize a given set of unordered sentences. to achieve this goal, the most important step is to effectively capture and exploit global dependencies among these sentences. in this paper, we propose a novel and flexible external knowledge enhanced graph-based neural network for sentence ordering. specifically, we first represent the input sentences as a graph, where various kinds of relations (i.e., entity-entity, sentence-sentence and entity-sentence) are exploited to make the graph representation more expressive and less noisy. then, we introduce graph recurrent network to learn semantic representations of the sentences. to demonstrate the effectiveness of our model, we conduct experiments on several benchmark datasets. the experimental results and in-depth analysis show our model significantly outperforms the existing state-of-the-art models.",2021
on the distortion value of elections with abstention,"in spatial voting theory, distortion is a measure of how good the winner is. it has been proved that no deterministic voting mechanism can guarantee a distortion better than 3, even for simple metrics such as a line. in this study, we wish to answer the following question: how does the distortion value change if we allow less motivated agents to abstain from the election? we consider an election with two candidates and suggest an abstention model, which is a general form of the abstention model proposed by kirchgassner. our results characterize the distortion ¨ value and provide a rather complete picture of the model.",2021
generic constraint-based block modeling using constraint programming,"block modeling has been used extensively in many domains including social science, spatial temporal data analysis and even medical imaging. original formulations of the problem modeled it as a mixed integer programming problem, but were not scalable. subsequent work relaxed the discrete optimization requirement, and showed that adding constraints is not straightforward in existing approaches. in this work, we present a new approach based on constraint programming, allowing discrete optimization of block modeling in a manner that is not only scalable, but also allows the easy incorporation of constraints. we introduce a new constraint filtering algorithm that outperforms earlier approaches, in both constrained and unconstrained settings, for an exhaustive search and for a type of local search called large neighborhood search. we show its use in the analysis of real datasets. finally, we show an application of the cp framework for model selection using the minimum description length principle.",2021
regarding goal bounding and jump point search,"jump point search (jps) is a well known symmetry-breaking algorithm that can substantially improve performance for grid-based optimal pathfinding. when the input grid is static further speedups can be obtained by combining jps with goal bounding techniques such as geometric containers (instantiated as bounding boxes) and compressed path databases. two such methods, jps+bb and two-oracle path planning (topping), are currently among the fastest known approaches for computing shortest paths on grids. the principal drawback for these algorithms is the overhead costs: each one requires an all-pairs precomputation step, the running time and subsequent storage costs of which can be prohibitive. in this work we consider an alternative approach where we precompute and store goal bounding data only for grid cells which are also jump points. since the number of jump points is usually much smaller than the total number of grid cells, we can save up to orders of magnitude in preprocessing time and space. considerable precomputation savings do not necessarily mean performance degradation. for a second contribution we show how canonical orderings, partial expansion strategies and enhanced intermediate pruning can be leveraged to improve online query performance despite a reduction in preprocessed data. the combination of faster preprocessing and stronger online reasoning leads to three new and highly performant algorithms: jps+bb+ and two-oracle pathfinding search (tops) based on search, and topping+ based on path extraction. we give a theoretical analysis showing that each method is complete and optimal. we also report convincing gains in a comprehensive empirical evaluation that includes almost all current and cutting-edge algorithms for grid-based pathfinding.",2021
classifier chains: a review and perspectives,"the family of methods collectively known as classifier chains has become a popular approach to multi-label learning problems. this approach involves chaining together off-the-shelf binary classifiers in a directed structure, such that individual label predictions become features for other classifiers. such methods have proved flexible and effective and have obtained state-of-the-art empirical performance across many datasets and multi-label evaluation metrics. this performance led to further studies of the underlying mechanism and efficacy, and investigation into how it could be improved. in the recent decade, numerous studies have explored the theoretical underpinnings of classifier chains, and many improvements have been made to the training and inference procedures, such that this method remains among the best options for multi-label learning. given this past and ongoing interest, which covers a broad range of applications and research themes, the goal of this work is to provide a review of classifier chains, a survey of the techniques and extensions provided in the literature, as well as perspectives for this approach in the domain of multi-label classification in the future. we conclude positively, with a number of recommendations for researchers and practitioners, as well as outlining key issues for future research.",2021
two-facility location games with minimum distance requirement,"we study the mechanism design problem of a social planner for locating two facilities on a line interval [0, 1], where a set of n strategic agents report their locations and a mechanism determines the locations of the two facilities. we consider the requirement of a minimum distance 0 ≤ d ≤ 1 between the two facilities. given the two facilities are heterogeneous, we model the cost/utility of an agent as the sum of his distances to both facilities. in the heterogeneous two-facility location game to minimize the social cost, we show that the optimal solution can be computed in polynomial time and prove that carefully choosing one optimal solution as output is strategyproof. we also design a strategyproof mechanism minimizing the maximum cost. given the two facilities are homogeneous, we model the cost/utility of an agent as his distance to the closer facility. in the homogeneous two-facility location game for minimizing the social cost, we show that any deterministic strategyproof mechanism has unbounded approximation ratio. moreover, in the obnoxious heterogeneous two-facility location game for maximizing the social utility, we propose new deterministic group strategyproof mechanisms with provable approximation ratios and establish a lower bound (7 − d)/6 for any deterministic strategyproof mechanism. we also design a strategyproof mechanism maximizing the minimum utility. in the obnoxious homogeneous two-facility location game for maximizing the social utility, we propose deterministic group strategyproof mechanisms with provable approximation ratios and establish a lower bound 4/3. besides, in the two-facility location game with triple-preference, where each facility may be favorable, obnoxious, indifferent for any agent, we further motivate agents to report both their locations and preferences towards the two facilities truthfully, and design a deterministic group strategyproof mechanism with an approximation ratio 4.",2021
efficient large-scale multi-drone delivery using transit networks,"we consider the problem of routing a large fleet of drones to deliver packages simultaneously across broad urban areas. besides flying directly, drones can use public transit vehicles such as buses and trams as temporary modes of transportation to conserve energy. adding this capability to our formulation augments effective drone travel range and the space of possible deliveries but also increases problem input size due to the large transit networks. we present a comprehensive algorithmic framework that strives to minimize the maximum time to complete any delivery and addresses the multifaceted computational challenges of our problem through a two-layer approach. first, the upper layer assigns drones to package delivery sequences with an approximately optimal polynomial time allocation algorithm. then, the lower layer executes the allocation by periodically routing the fleet over the transit network, using efficient, bounded suboptimal multi-agent pathfinding techniques tailored to our setting. we demonstrate the efficiency of our approach on simulations with up to 200 drones, 5000 packages, and transit networks with up to 8000 stops in san francisco and the washington dc metropolitan area. our framework computes solutions for most settings within a few seconds on commodity hardware and enables drones to extend their effective range by a factor of nearly four using transit.",2021
a sufficient statistic for influence in structured multiagent environments,"making decisions in complex environments is a key challenge in artificial intelligence (ai). situations involving multiple decision makers are particularly complex, leading to computational intractability of principled solution methods. a body of work in ai has tried to mitigate this problem by trying to distill interaction to its essence: how does the policy of one agent influence another agent? if we can find more compact representations of such influence, this can help us deal with the complexity, for instance by searching the space of influences rather than the space of policies. however, so far these notions of influence have been restricted in their applicability to special cases of interaction. in this paper we formalize influence-based abstraction (iba), which facilitates the elimination of latent state factors without any loss in value, for a very general class of problems described as factored partially observable stochastic games (fposgs). on the one hand, this generalizes existing descriptions of influence, and thus can serve as the foundation for improvements in scalability and other insights in decision making in complex multiagent settings. on the other hand, since the presence of other agents can be seen as a generalization of single agent settings, our formulation of iba also provides a sufficient statistic for decision making under abstraction for a single agent. we also give a detailed discussion of the relations to such previous works, identifying new insights and interpretations of these approaches. in these ways, this paper deepens our understanding of abstraction in a wide range of sequential decision making settings, providing the basis for new approaches and algorithms for a large class of problems.",2021
taking principles seriously: a hybrid approach to value alignment in artificial intelligence,"an important step in the development of value alignment (va) systems in artificial intelligence (ai) is understanding how va can reflect valid ethical principles. we propose that designers of va systems incorporate ethics by utilizing a hybrid approach in which both ethical reasoning and empirical observation play a role. this, we argue, avoids committing “naturalistic fallacy,” which is an attempt to derive “ought” from “is,” and it provides a more adequate form of ethical reasoning when the fallacy is not committed. using quantified modal logic, we precisely formulate principles derived from deontological ethics and show how they imply particular “test propositions” for any given action plan in an ai rule base. the action plan is ethical only if the test proposition is empirically true, a judgment that is made on the basis of empirical va. this permits empirical va to integrate seamlessly with independently justified ethical principles. this article is part of the special track on ai and society.",2021
on the evolvability of monotone conjunctions with an evolutionary mutation mechanism,"a bernoulli(p)n distribution bn,p over {0, 1}n is a product distribution where each variable is satisfied with the same constant probability p. diochnos (2016) showed that valiant's swapping algorithm for monotone conjunctions converges efficiently under bn,p distributions over {0, 1}n for any 0 &lt; p &lt; 1. we continue the study of monotone conjunctions in valiant's framework of evolvability. in particular, we prove that given a bn,p distribution characterized by some p ∈ (0, 1/3] ∪ {1/2}, then an evolutionary mechanism that relies on the basic mutation mechanism of a (1+1) evolutionary algorithm converges efficiently, with high probability, to an ε-optimal hypothesis. furthermore, for 0 &lt; α ≤ 3/13, a slight modification of the algorithm, with a uniform setup this time, evolves with high probability an ε-optimal hypothesis, for every bn,p distribution such that p ∈ [α, 1/3 - 4α/9] ∪ {1/3} ∪ {1/2}.",2021
safe multi-agent pathfinding with time uncertainty,"in many real-world scenarios, the time it takes for a mobile agent, e.g., a robot, to move from one location to another may vary due to exogenous events and be difficult to predict accurately. planning in such scenarios is challenging, especially in the context of multi-agent pathfinding (mapf), where the goal is to find paths to multiple agents and temporal coordination is necessary to avoid collisions. in this work, we consider a mapf problem with this form of time uncertainty, where we are only given upper and lower bounds on the time it takes each agent to move. the objective is to find a safe solution, which is a solution that can be executed by all agents and is guaranteed to avoid collisions. we propose two complete and optimal algorithms for finding safe solutions based on well-known mapf algorithms, namely, a* with operator decomposition (a* + od) and conflict-based search (cbs). experimentally, we observe that on several standard mapf grids the cbs-based algorithm performs better. we also explore the option of online replanning in this context, i.e., modifying the agents' plans during execution, to reduce the overall execution cost. we consider two online settings: (a) when an agent can sense the current time and its current location, and (b) when the agents can also communicate seamlessly during execution. for each setting, we propose a replanning algorithm and analyze its behavior theoretically and empirically. our experimental evaluation confirms that indeed online replanning in both settings can significantly reduce solution cost.",2021
constrained multiagent markov decision processes: a taxonomy of problems and algorithms,"in domains such as electric vehicle charging, smart distribution grids and autonomous warehouses, multiple agents share the same resources. when planning the use of these resources, agents need to deal with the uncertainty in these domains. although several models and algorithms for such constrained multiagent planning problems under uncertainty have been proposed in the literature, it remains unclear when which algorithm can be applied. in this survey we conceptualize these domains and establish a generic problem class based on markov decision processes. we identify and compare the conditions under which algorithms from the planning literature for problems in this class can be applied: whether constraints are soft or hard, whether agents are continuously connected, whether the domain is fully observable, whether a constraint is momentarily (instantaneous) or on a budget, and whether the constraint is on a single resource or on multiple. further we discuss the advantages and disadvantages of these algorithms. we conclude by identifying open problems that are directly related to the conceptualized domains, as well as in adjacent research areas.",2021
the societal implications of deep reinforcement learning,"deep reinforcement learning (drl) is an avenue of research in artificial intelligence (ai) that has received increasing attention within the research community in recent years, and is beginning to show potential for real-world application. drl is one of the most promising routes towards developing more autonomous ai systems that interact with and take actions in complex real-world environments, and can more flexibly solve a range of problems for which we may not be able to precisely specify a correct ‘answer’. this could have substantial implications for people’s lives: for example by speeding up automation in various sectors, changing the nature and potential harms of online influence, or introducing new safety risks in physical infrastructure. in this paper, we review recent progress in drl, discuss how this may introduce novel and pressing issues for society, ethics, and governance, and highlight important avenues for future research to better understand drl’s societal implications. this article appears in the special track on ai and society.",2021
induction and exploitation of subgoal automata for reinforcement learning,"in this paper we present isa, an approach for learning and exploiting subgoals in episodic reinforcement learning (rl) tasks. isa interleaves reinforcement learning with the induction of a subgoal automaton, an automaton whose edges are labeled by the task’s subgoals expressed as propositional logic formulas over a set of high-level events. a subgoal automaton also consists of two special states: a state indicating the successful completion of the task, and a state indicating that the task has finished without succeeding. a state-of-the-art inductive logic programming system is used to learn a subgoal automaton that covers the traces of high-level events observed by the rl agent. when the currently exploited automaton does not correctly recognize a trace, the automaton learner induces a new automaton that covers that trace. the interleaving process guarantees the induction of automata with the minimum number of states, and applies a symmetry breaking mechanism to shrink the search space whilst remaining complete. we evaluate isa in several gridworld and continuous state space problems using different rl algorithms that leverage the automaton structures. we provide an in-depth empirical analysis of the automaton learning performance in terms of the traces, the symmetry breaking and specific restrictions imposed on the final learnable automaton. for each class of rl problem, we show that the learned automata can be successfully exploited to learn policies that reach the goal, achieving an average reward comparable to the case where automata are not learned but handcrafted and given beforehand.",2021
lilotane: a lifted sat-based approach to hierarchical planning,"one of the oldest and most popular approaches to automated planning is to encode the problem at hand into a propositional formula and use a satisfiability (sat) solver to find a solution. in all established sat-based approaches for hierarchical task network (htn) planning, grounding the problem is necessary and oftentimes introduces a combinatorial blowup in terms of the number of actions and reductions to encode. our contribution named lilotane (lifted logic for task networks) eliminates this issue for totally ordered htn planning by directly encoding the lifted representation of the problem at hand. we lazily instantiate the problem hierarchy layer by layer and use a novel sat encoding which allows us to defer decisions regarding method arguments to the stage of sat solving. we show the correctness of our encoding and compare it to the best performing prior sat encoding in a worst-case analysis. empirical evaluations confirm that lilotane outperforms established sat-based approaches, often by orders of magnitude, produces much smaller formulae on average, and compares favorably to other state-of-the-art htn planners regarding robustness and plan quality. in the international planning competition (ipc) 2020, a preliminary version of lilotane scored the second place. we expect these considerable improvements to sat-based htn planning to open up new perspectives for sat-based approaches in related problem classes.",2021
computational complexity of computing symmetries in finite-domain planning,"symmetry-based pruning is a powerful method for reducing the search effort in finitedomain planning. this method is based on exploiting an automorphism group connected to the ground description of the planning task { these automorphisms are known as structural symmetries. in particular, we are interested in the structsym problem where the generators of this group are to be computed. it has been observed in practice that the structsym problem is surprisingly easy to solve. we explain this phenomenon by showing that structsym is gi-complete, i.e., the graph isomorphism problem is polynomial-time equivalent to it and, consequently, solvable in quasi-polynomial time. this implies that it is solvable substantially faster than most computationally hard problems encountered in ai. we accompany this result by identifying natural restrictions of the planning task and its causal graph that ensure that structsym can be solved in polynomial time. given that the structsym problem is gi-complete and thus solvable quite efficiently, it is interesting to analyse if other symmetries (than those that are encompassed by the structsym problem) can be computed and/or analysed efficiently, too. to this end, we present a highly negative result: checking whether there exists an automorphism of the state transition graph that maps one state s into another state t is a pspace-hard problem and, consequently, at least as hard as the planning problem itself.",2021
liquid democracy: an algorithmic perspective,"we study liquid democracy, a collective decision making paradigm that allows voters to transitively delegate their votes, through an algorithmic lens. in our model, there are two alternatives, one correct and one incorrect, and we are interested in the probability that the majority opinion is correct. our main question is whether there exist delegation mechanisms that are guaranteed to outperform direct voting, in the sense of being always at least as likely, and sometimes more likely, to make a correct decision. even though we assume that voters can only delegate their votes to better-informed voters, we show that local delegation mechanisms, which only take the local neighborhood of each voter as input (and, arguably, capture the spirit of liquid democracy), cannot provide the foregoing guarantee. by contrast, we design a non-local delegation mechanism that does provably outperform direct voting under mild assumptions about voters.",2021
strategyproof mechanisms for additively separable and fractional hedonic games,"additively separable hedonic games and fractional hedonic games have received considerable attention in the literature. they are coalition formation games among selfish agents based on their mutual preferences. most of the work in the literature characterizes the existence and structure of stable outcomes (i.e., partitions into coalitions) assuming that preferences are given. however, there is little discussion of this assumption. in fact, agents receive different utilities if they belong to different coalitions, and thus it is natural for them to declare their preferences strategically in order to maximize their benefit. in this paper we consider strategyproof mechanisms for additively separable hedonic games and fractional hedonic games, that is, partitioning methods without payments such that utility maximizing agents have no incentive to lie about their true preferences. we focus on social welfare maximization and provide several lower and upper bounds on the performance achievable by strategyproof mechanisms for general and specific additive functions. in most of the cases we provide tight or asymptotically tight results. all our mechanisms are simple and can be run in polynomial time. moreover, all the lower bounds are unconditional, that is, they do not rely on any computational complexity assumptions.",2021
weighted first-order model counting in the two-variable fragment with counting quantifiers,"it is known due to the work of van den broeck, meert and darwiche that weighted first-order model counting (wfomc) in the two-variable fragment of first-order logic can be solved in time polynomial in the number of domain elements. in this paper we extend this result to the two-variable fragment with counting quantifiers.",2021
the ai liability puzzle and a fund-based work-around,"confidence in the regulatory environment is crucial to enable responsible ai innovation and foster the social acceptance of these powerful new technologies. one notable source of uncertainty is, however, that the existing legal liability system is unable to assign responsibility where a potentially harmful conduct and/or the harm itself are unforeseeable, yet some instantiations of ai and/or the harms they may trigger are not foreseeable in the legal sense. the unpredictability of how courts would handle such cases makes the risks involved in the investment and use of ai difficult to calculate with confidence, creating an environment that is not conducive to innovation and may deprive society of some benefits ai could provide. to tackle this problem, we propose to draw insights from financial regulatory best practices and establish a system of ai guarantee schemes. we envisage the system to form part of the broader market-structuring regulatory frameworks, with the primary function to provide a readily available, clear, and transparent funding mechanism to compensate claims that are either extremely hard or impossible to realize via conventional litigation. we propose it to be at least partially industry-funded. funding arrangements should depend on whether it would pursue other potential policy goals aimed more broadly at controlling the trajectory of ai innovation to increase economic and social welfare worldwide. because of the global relevance of the issue, rather than focusing on any particular legal system, we trace relevant developments across multiple jurisdictions and engage in a high-level, comparative conceptual debate around the suitability of the foreseeability concept to limit legal liability. the paper also refrains from confronting the intricacies of the case law of specific jurisdictions for now and—recognizing the importance of this task—leaves this to further research in support of the legal system’s incremental adaptation to the novel challenges of present and future ai technologies. this article appears in the special track on ai and society.",2021
instance-level update in dl-lite ontologies through first-order rewriting,"in this paper we study instance-level update in dl-litea , a well-known description logic that influenced the owl 2 ql standard. instance-level update regards insertions and deletions in the abox of an ontology. in particular we focus on formula-based approaches to instance-level update. we show that dl-litea , which is well-known for enjoying first-order rewritability of query answering, enjoys a first-order rewritability property also for instance-level update. that is, every update can be reformulated into a set of insertion and deletion instructions computable through a non-recursive datalog program with negation. such a program is readily translatable into a first-order query over the abox considered as a database, and hence into sql. by exploiting this result, we implement an update component for dl-litea-based systems and perform some experiments showing that the approach works in practice.",2021
confident learning: estimating uncertainty in dataset labels,"learning exists in the context of data, yet notions of confidence typically focus on model predictions, not label quality. confident learning (cl) is an alternative approach which focuses instead on label quality by characterizing and identifying label errors in datasets, based on the principles of pruning noisy data, counting with probabilistic thresholds to estimate noise, and ranking examples to train with confidence. whereas numerous studies have developed these principles independently, here, we combine them, building on the assumption of a class-conditional noise process to directly estimate the joint distribution between noisy (given) labels and uncorrupted (unknown) labels. this results in a generalized cl which is provably consistent and experimentally performant. we present sufficient conditions where cl exactly finds label errors, and show cl performance exceeding seven recent competitive approaches for learning with noisy labels on the cifar dataset. uniquely, the cl framework is not coupled to a specific data modality or model (e.g., we use cl to find several label errors in the presumed error-free mnist dataset and improve sentiment classification on text data in amazon reviews). we also employ cl on imagenet to quantify ontological class overlap (e.g., estimating 645 missile images are mislabeled as their parent class projectile), and moderately increase model accuracy (e.g., for resnet) by cleaning data prior to training. these results are replicable using the open-source cleanlab release.",2021
"aggregation over metric spaces: proposing and voting in elections, budgeting, and legislation","we present a unifying framework encompassing a plethora of social choice settings. viewing each social choice setting as voting in a suitable metric space, we offer a general model of social choice over metric spaces, in which—similarly to the spatial model of elections—each voter specifies an ideal element of the metric space. the ideal element acts as a vote, where each voter prefers elements that are closer to her ideal element. but it also acts as a proposal, thus making all participants equal not only as voters but also as proposers. we consider condorcet aggregation and a continuum of solution concepts, ranging from minimizing the sum of distances to minimizing the maximum distance. we study applications of our abstract model to various social choice settings, including single-winner elections, committee elections, participatory budgeting, and participatory legislation. for each setting, we compare each solution concept to known voting rules and study various properties of the resulting voting rules. our framework provides expressive aggregation for a broad range of social choice settings while remaining simple for voters; and may enable a unified and integrated implementation for all these settings, as well as unified extensions such as sybil-resiliency, proxy voting, and deliberative decision making.",2021
efficient retrieval of matrix factorization-based top-k recommendations: a survey of recent approaches,"top-k recommendation seeks to deliver a personalized list of k items to each individual user. an established methodology in the literature based on matrix factorization (mf), which usually represents users and items as vectors in low-dimensional space, is an effective approach to recommender systems, thanks to its superior performance in terms of recommendation quality and scalability. a typical matrix factorization recommender system has two main phases: preference elicitation and recommendation retrieval. the former analyzes user-generated data to learn user preferences and item characteristics in the form of latent feature vectors, whereas the latter ranks the candidate items based on the learnt vectors and returns the top-k items from the ranked list. for preference elicitation, there have been numerous works to build accurate mf-based recommendation algorithms that can learn from large datasets. however, for the recommendation retrieval phase, naively scanning a large number of items to identify the few most relevant ones may inhibit truly real-time applications. in this work, we survey recent advances and state-of-the-art approaches in the literature that enable fast and accurate retrieval for mf-based personalized recommendations. also, we include analytical discussions of approaches along different dimensions to provide the readers with a more comprehensive understanding of the surveyed works.",2021
"loss functions, axioms, and peer review","it is common to see a handful of reviewers reject a highly novel paper, because they view, say, extensive experiments as far more important than novelty, whereas the community as a whole would have embraced the paper. more generally, the disparate mapping of criteria scores to final recommendations by different reviewers is a major source of inconsistency in peer review. in this paper we present a framework inspired by empirical risk minimization (erm) for learning the community's aggregate mapping. the key challenge that arises is the specification of a loss function for erm. we consider the class of l(p,q) loss functions, which is a matrix-extension of the standard class of lp losses on vectors; here the choice of the loss function amounts to choosing the hyperparameters p and q. to deal with the absence of ground truth in our problem, we instead draw on computational social choice to identify desirable values of the hyperparameters p and q. specifically, we characterize p=q=1 as the only choice of these hyperparameters that satisfies three natural axiomatic properties. finally, we implement and apply our approach to reviews from ijcai 2017.",2021
madras : multi agent driving simulator,"autonomous driving has emerged as one of the most active areas of research as it has the promise of making transportation safer and more efficient than ever before. most real-world autonomous driving pipelines perform perception, motion planning and action in a loop. in this work we present madras, an open-source multi-agent driving simulator for use in the design and evaluation of motion planning algorithms for autonomous driving. given a start and a goal state, the task of motion planning is to solve for a sequence of position, orientation and speed values in order to navigate between the states while adhering to safety constraints. these constraints often involve the behaviors of other agents in the environment. madras provides a platform for constructing a wide variety of highway and track driving scenarios where multiple driving agents can be trained for motion planning tasks using reinforcement learning and other machine learning algorithms. madras is built on torcs, an open-source car-racing simulator. torcs offers a variety of cars with different dynamic properties and driving tracks with different geometries and surface. madras inherits these functionalities from torcs and introduces support for multi-agent training, inter-vehicular communication, noisy observations, stochastic actions, and custom traffic cars whose behaviors can be programmed to simulate challenging traffic conditions encountered in the real world. madras can be used to create driving tasks whose complexities can be tuned along eight axes in well-defined steps. this makes it particularly suited for curriculum and continual learning. madras is lightweight and it provides a convenient openai gym interface for independent control of each car. apart from the primitive steering-acceleration-brake control mode of torcs, madras offers a hierarchical track-position – speed control mode that can potentially be used to achieve better generalization. madras uses a udp based client server model where the simulation engine is the server and each client is a driving agent. madras uses multiprocessing to run each agent as a parallel process for efficiency and integrates well with popular reinforcement learning libraries like rllib. we show experiments on single and multi-agent reinforcement learning with and without curriculum",2021
labeled bipolar argumentation frameworks,"an essential part of argumentation-based reasoning is to identify arguments in favor and against a statement or query, select the acceptable ones, and then determine whether or not the original statement should be accepted. we present here an abstract framework that considers two independent forms of argument interaction—support and conflict—and is able to represent distinctive information associated with these arguments. this information can enable additional actions such as: (i) a more in-depth analysis of the relations between the arguments; (ii) a representation of the user’s posture to help in focusing the argumentative process, optimizing the values of attributes associated with certain arguments; and (iii) an enhancement of the semantics taking advantage of the availability of richer information about argument acceptability. thus, the classical semantic definitions are enhanced by analyzing a set of postulates they satisfy. finally, a polynomial-time algorithm to perform the labeling process is introduced, in which the argument interactions are considered.",2021
point at the triple: generation of text summaries from knowledge base triples,"we investigate the problem of generating natural language summaries from knowledge base triples. our approach is based on a pointer-generator network, which, in addition to generating regular words from a fixed target vocabulary, is able to verbalise triples in several ways. we undertake an automatic and a human evaluation on single and open-domain summaries generation tasks. both show that our approach significantly outperforms other data-driven baselines.",2020
constraint and satisfiability reasoning for graph coloring,"graph coloring is an important problem in combinatorial optimization and a major component of numerous allocation and scheduling problems. in this paper we introduce a hybrid cp/sat approach to graph coloring based on the addition-contraction recurrence of zykov. decisions correspond to either adding an edge between two non-adjacent vertices or contracting these two vertices, hence enforcing inequality or equality, respectively. this scheme yields a symmetry-free tree and makes learnt clauses stronger by not committing to a particular color. we introduce a new lower bound for this problem based on mycielskian graphs; a method to produce a clausal explanation of this bound for use in a cdcl algorithm; a branching heuristic emulating brelaz’ heuristic on the zykov tree; and dedicated pruning techniques relying on marginal costs with respect to the bound and on reasoning about transitivity when unit propagating learnt clauses. the combination of these techniques in both a branch-and-bound and in a bottom-up search outperforms other sat-based approaches and dsatur on standard benchmarks both for finding upper bounds and for proving lower bounds.",2020
on sparse discretization for graphical games,"graphical games are one of the earliest examples of the impact that the general field of graphical models have had in other areas, and in this particular case, in classical mathematical models in game theory. graphical multi-hypermatrix games, a concept formally introduced in this research note, generalize graphical games while allowing the possibility of further space savings in model representation to that of standard graphical games. the main focus of this research note is discretization schemes for computing approximate nash equilibria, with emphasis on graphical games, but also briefly touching on normal-form and polymatrix games. the main technical contribution is a theorem that establishes sufficient conditions for a discretization of the players’ space of mixed strategies to contain an approximate nash equilibrium. the result is actually stronger because every exact nash equilibrium has a nearby approximate nash equilibrium on the grid induced by the discretization. the sufficient conditions are weaker than those of previous results. in particular, a uniform discretization of size linear in the inverse of the approximation error and in the natural game-representation parameters suffices. the theorem holds for a generalization of graphical games, introduced here. the result has already been useful in the design and analysis of tractable algorithms for graphical games with parametric payoff functions and certain game-graph structures. for standard graphical games, under natural conditions, the discretization is logarithmic in the game-representation size, a substantial improvement over the linear dependency previously required. combining the improved discretization result with old results on constraint networks in ai simplifies the derivation and analysis of algorithms for computing approximate nash equilibria in graphical games.",2020
incompatibilities between iterated and relevance-sensitive belief revision,"the agm paradigm for belief change, as originally introduced by alchourron, gardenfors and makinson, lacks any guidelines for the process of iterated revision. one of the most influential work addressing this problem is darwiche and pearl's approach (dp approach, for short), which, despite its well-documented shortcomings, remains to this date the most dominant. in this article, we make further observations on the dp approach. in particular, we prove that the dp postulates are, in a strong sense, inconsistent with parikh's relevance-sensitive axiom (p), extending previous initial conflicts. immediate consequences of this result are that an entire class of intuitive revision operators, which includes dalal's operator, violates the dp postulates, as well as that the independence postulate and spohn's conditionalization are inconsistent with axiom (p). the whole study, essentially, indicates that two fundamental aspects of the revision process, namely, iteration and relevance, are in deep conflict, and opens the discussion for a potential reconciliation towards a comprehensive formal framework for knowledge dynamics.",2020
contiguous cake cutting: hardness results and approximation algorithms,"we study the fair allocation of a cake, which serves as a metaphor for a divisible resource, under the requirement that each agent should receive a contiguous piece of the cake. while it is known that no finite envy-free algorithm exists in this setting, we exhibit efficient algorithms that produce allocations with low envy among the agents. we then establish np-hardness results for various decision problems on the existence of envy-free allocations, such as when we fix the ordering of the agents or constrain the positions of certain cuts. in addition, we consider a discretized setting where indivisible items lie on a line and show a number of hardness results extending and strengthening those from prior work. finally, we investigate connections between approximate and exact envy-freeness, as well as between continuous and discrete cake cutting.",2020
annotator rationales for labeling tasks in crowdsourcing,"when collecting item ratings from human judges, it can be difficult to measure and enforce data quality due to task subjectivity and lack of transparency into how judges make each rating decision. to address this, we investigate asking judges to provide a specific form of rationale supporting each rating decision. we evaluate this approach on an information retrieval task in which human judges rate the relevance of web pages for different search topics. cost-benefit analysis over 10,000 judgments collected on amazon’s mechanical turk suggests a win-win. firstly, rationales yield a multitude of benefits: more reliable judgments, greater transparency for evaluating both human raters and their judgments, reduced need for expert gold, the opportunity for dual-supervision from ratings and rationales, and added value from the rationales themselves. secondly, once experienced in the task, crowd workers provide rationales with almost no increase in task completion time. consequently, we can realize the above benefits with minimal additional cost.",2020
the parameterized complexity of motion planning for snake-like robots,"we study the parameterized complexity of a variant of the classic video game snake that models real-world problems of motion planning. given a snake-like robot with an initial position and a final position in an environment (modeled by a graph), our objective is to determine whether the robot can reach the final position from the initial position without intersecting itself. naturally, this problem models a wide-variety of scenarios, ranging from the transportation of linked wagons towed by a locomotor at an airport or a supermarket to the movement of a group of agents that travel in an “ant-like” fashion and the construction of trains in amusement parks. unfortunately, already on grid graphs, this problem is pspace-complete. nevertheless, we prove that even on general graphs, the problem is solvable in fpt time with respect to the size of the snake. in particular, this shows that the problem is fixed-parameter tractable (fpt). towards this, we show how to employ color-coding to sparsify the configuration graph of the problem to reduce its size significantly. we believe that our approach will find other applications in motion planning. additionally, we show that the problem is unlikely to admit a polynomial kernel even on grid graphs, but it admits a treewidth-reduction procedure. to the best of our knowledge, the study of the parameterized complexity of motion planning problems (where the intermediate configurations of the motion are of importance) has so far been largely overlooked. thus, our work is pioneering in this regard.",2020
improved high dimensional discrete bayesian network inference using triplet region construction,"performing efficient inference on high dimensional discrete bayesian networks (bns) is challenging. when using exact inference methods the space complexity can grow exponentially with the tree-width, thus making computation intractable. this paper presents a general purpose approximate inference algorithm, based on a new region belief approximation method, called triplet region construction (trc). trc reduces the cluster space complexity for factorized models from worst-case exponential to polynomial by performing graph factorization and producing clusters of limited size. unlike previous generations of region-based algorithms, trc is guaranteed to converge and effectively addresses the region choice problem that bedevils other region-based algorithms used for bn inference. our experiments demonstrate that it also achieves significantly more accurate results than competing algorithms.",2020
"planning high-level paths in hostile, dynamic, and uncertain environments","this paper introduces and studies a graph-based variant of the path planning problem arising in hostile environments. we consider a setting where an agent (e.g. a robot) must reach a given destination while avoiding being intercepted by probabilistic entities which exist in the graph with a given probability and move according to a probabilistic motion pattern known a priori. given a goal vertex and a deadline to reach it, the agent must compute the path to the goal that maximizes its chances of survival. we study the computational complexity of the problem, and present two algorithms for computing high quality solutions in the general case: an exact algorithm based on mixed-integer nonlinear programming, working well in instances of moderate size, and a pseudo-polynomial time heuristic algorithm allowing to solve large scale problems in reasonable time. we also consider the two limit cases where the agent can survive with probability 0 or 1, and provide specialized algorithms to detect these kinds of situations more efficiently.",2020
neural machine translation: a review,"the field of machine translation (mt), the automatic translation of written text from one natural language into another, has experienced a major paradigm shift in recent years. statistical mt, which mainly relies on various count-based models and which used to dominate mt research for decades, has largely been superseded by neural machine translation (nmt), which tackles translation with a single neural network. in this work we will trace back the origins of modern nmt architectures to word and sentence embeddings and earlier examples of the encoder-decoder network family. we will conclude with a short survey of more recent trends in the field.",2020
amp chain graphs: minimal separators and structure learning algorithms,"this paper deals with chain graphs (cgs) under the andersson–madigan–perlman (amp) interpretation. we address the problem of finding a minimal separator in an amp cg, namely, finding a set z of nodes that separates a given non-adjacent pair of nodes such that no proper subset of z separates that pair. we analyze several versions of this problem and offer polynomial time algorithms for each. these include finding a minimal separator from a restricted set of nodes, finding a minimal separator for two given disjoint sets, and testing whether a given separator is minimal. to address the problem of learning the structure of amp cgs from data, we show that the pc-like algorithm is order dependent, in the sense that the output can depend on the order in which the variables are given. we propose several modifications of the pc-like algorithm that remove part or all of this order-dependence. we also extend the decomposition-based approach for learning bayesian networks (bns) to learn amp cgs, which include bns as a special case, under the faithfulness assumption. we prove the correctness of our extension using the minimal separator results. using standard benchmarks and synthetically generated models and data in our experiments demonstrate the competitive performance of our decomposition-based method, called lcd-amp, in comparison with the (modified versions of) pc-like algorithm. the lcd-amp algorithm usually outperforms the pc-like algorithm, and our modifications of the pc-like algorithm learn structures that are more similar to the underlying ground truth graphs than the original pc-like algorithm, especially in high-dimensional settings. in particular, we empirically show that the results of both algorithms are more accurate and stabler when the sample size is reasonably large and the underlying graph is sparse",2020
the petlon algorithm to plan efficiently for task-level-optimal navigation,"intelligent mobile robots have recently become able to operate autonomously in large-scale indoor environments for extended periods of time. in this process, mobile robots need the capabilities of both task and motion planning. task planning in such environments involves sequencing the robot’s high-level goals and subgoals, and typically requires reasoning about the locations of people, rooms, and objects in the environment, and their interactions to achieve a goal. one of the prerequisites for optimal task planning that is often overlooked is having an accurate estimate of the actual distance (or time) a robot needs to navigate from one location to another. state-of-the-art motion planning algorithms, though often computationally complex, are designed exactly for this purpose of finding routes through constrained spaces. in this article, we focus on integrating task and motion planning (tmp) to achieve task-level-optimal planning for robot navigation while maintaining manageable computational efficiency. to this end, we introduce tmp algorithm petlon (planning efficiently for task-level-optimal navigation), including two configurations with different trade-offs over computational expenses between task and motion planning, for everyday service tasks using a mobile robot. experiments have been conducted both in simulation and on a mobile robot using object delivery tasks in an indoor office environment. the key observation from the results is that petlon is more efficient than a baseline approach that pre-computes motion costs of all possible navigation actions, while still producing plans that are optimal at the task level. we provide results with two different task planning paradigms in the implementation of petlon, and offer tmp practitioners guidelines for the selection of task planners from an engineering perspective.",2020
properties of switch-list representations of boolean functions,"in this paper, we focus on a less usual way to represent boolean functions, namely on representations by switch-lists, which are closely related to interval representations. given a truth table representation of a boolean function f the switch-list representation of f is a list of boolean vectors from the truth table which have a different function value than the preceding boolean vector in the truth table. the main aim of this paper is to include this type of representation in the knowledge compilation map by darwiche and marquis and to argue that switch-lists may in certain situations constitute a reasonable choice for a target language in knowledge compilation. first, we compare switch-list representations with a number of standard representations (such as cnf, dnf, and obdd) with respect to their relative succinctness. as a by-product of this analysis, we also give a short proof of a longstanding open question proposed by darwiche and marquis, namely the incomparability of mods (models) and pi (prime implicates) representations. next, using the succinctness result between switch-lists and obdds, we develop a polynomial time compilation algorithm from switch-lists to obdds. finally, we analyze which standard transformations and queries (those considered by darwiche and marquis) can be performed in polynomial time with respect to the size of the input if the input knowledge is represented by a switch-list. we show that this collection is very broad and the combination of polynomial time transformations and queries is quite unique. some of the queries can be answered directly using the switch-list input, others require a compilation of the input to obdd representations which are then used to answer the queries.",2020
computing bayes-nash equilibria in combinatorial auctions with verification,"we present a new algorithm for computing pure-strategy ε-bayes-nash equilibria (ε-bnes) in combinatorial auctions. the main innovation of our algorithm is to separate the algorithm’s search phase (for finding the ε-bne) from the verification phase (for computing the ε). using this approach, we obtain an algorithm that is both very fast and provides theoretical guarantees on the ε it finds. our main contribution is a verification method which, surprisingly, allows us to upper bound the ε across the whole continuous value space without making assumptions about the mechanism. using our algorithm, we can now compute ε-bnes in multi-minded domains that are significantly more complex than what was previously possible to solve. we release our code under an open-source license to enable researchers to perform algorithmic analyses of auctions, to enable bidders to analyze different strategies, and many other applications.",2020
the bottleneck simulator: a model-based deep reinforcement learning approach,"deep reinforcement learning has recently shown many impressive successes. however, one major obstacle towards applying such methods to real-world problems is their lack of data-efficiency. to this end, we propose the bottleneck simulator: a model-based reinforcement learning method which combines a learned, factorized transition model of the environment with rollout simulations to learn an effective policy from few examples. the learned transition model employs an abstract, discrete (bottleneck) state, which increases sample efficiency by reducing the number of model parameters and by exploiting structural properties of the environment. we provide a mathematical analysis of the bottleneck simulator in terms of fixed points of the learned policy, which reveals how performance is affected by four distinct sources of error: an error related to the abstract space structure, an error related to the transition model estimation variance, an error related to the transition model estimation bias, and an error related to the transition model class bias. finally, we evaluate the bottleneck simulator on two natural language processing tasks: a text adventure game and a real-world, complex dialogue response selection task. on both tasks, the bottleneck simulator yields excellent performance beating competing approaches.",2020
maximin share allocations on cycles,"the problem of fair division of indivisible goods is a fundamental problem of resource allocation in multi-agent systems, also studied extensively in social choice. recently, the problem was generalized to the case when goods form a graph and the goal is to allocate goods to agents so that each agent’s bundle forms a connected subgraph. for the maximin share fairness criterion, researchers proved that if goods form a tree, an allocation offering each agent a bundle of at least her maximin share value always exists. moreover, it can be found in polynomial time. in this paper we consider the problem of maximin share allocations of goods on a cycle. despite the simplicity of the graph, the problem turns out to be significantly harder than its tree version. we present cases when maximin share allocations of goods on cycles exist and provide in this case results on allocations guaranteeing each agent a certain fraction of her maximin share. we also study algorithms for computing maximin share allocations of goods on cycles.",2020
"belief change and 3-valued logics: characterization of 19,683 belief change operators","in this work we introduce a 3-valued logic with modalities, with the aim of having a clear and precise representation of epistemic states, thus the formulas of this logic will be our epistemic states. indeed, these formulas are identified with ranking functions of 3 values, a generalization of total preorders of three levels. in this framework we analyze some types of changes of these epistemic structures and give syntactical characterizations of them in the introduced logic. in particular, we introduce and study carefully a new operator called cautious improvement operator. we also characterize all operators that are definable in this framework.",2020
the complexity landscape of outcome determination in judgment aggregation,"we provide a comprehensive analysis of the computational complexity of the outcome determination problem for the most important aggregation rules proposed in the literature on logic-based judgment aggregation. judgment aggregation is a powerful and flexible framework for studying problems of collective decision making that has attracted interest in a range of disciplines, including legal theory, philosophy, economics, political science, and artificial intelligence. the problem of computing the outcome for a given list of individual judgments to be aggregated into a single collective judgment is the most fundamental algorithmic challenge arising in this context. our analysis applies to several different variants of the basic framework of judgment aggregation that have been discussed in the literature, as well as to a new framework that encompasses all existing such frameworks in terms of expressive power and representational succinctness.",2020
structure from randomness in halfspace learning with the zero-one loss,"we prove risk bounds for halfspace learning when the data dimensionality is allowed to be larger than the sample size, using a notion of compressibility by random projection. in particular, we give upper bounds for the empirical risk minimizer learned efficiently from randomly projected data, as well as uniform upper bounds in the full high-dimensional space. our main findings are the following: i) in both settings, the obtained bounds are able to discover and take advantage of benign geometric structure, which turns out to depend on the cosine similarities between the classifier and points of the input space, and provide a new interpretation of margin distribution type arguments. ii) furthermore our bounds allow us to draw new connections between several existing successful classification algorithms, and we also demonstrate that our theory is predictive of empirically observed performance in numerical simulations and experiments. iii) taken together, these results suggest that the study of compressive learning can improve our understanding of which benign structural traits – if they are possessed by the data generator – make it easier to learn an effective classifier from a sample.",2020
using machine learning for decreasing state uncertainty in planning,"we present a novel approach for decreasing state uncertainty in planning prior to solving the planning problem. this is done by making predictions about the state based on currently known information, using machine learning techniques. for domains where uncertainty is high, we define an active learning process for identifying which information, once sensed, will best improve the accuracy of predictions. we demonstrate that an agent is able to solve problems with uncertainties in the state with less planning effort compared to standard planning techniques. moreover, agents can solve problems for which they could not find valid plans without using predictions. experimental results also demonstrate that using our active learning process for identifying information to be sensed leads to gathering information that improves the prediction process.",2020
mapping the landscape of artificial intelligence applications against covid-19,"covid-19, the disease caused by the sars-cov-2 virus, has been declared a pandemic by the world health organization, which has reported over 18 million confirmed cases as of august 5, 2020. in this review, we present an overview of recent studies using machine learning and, more broadly, artificial intelligence, to tackle many aspects of the covid19 crisis. we have identified applications that address challenges posed by covid-19 at different scales, including: molecular, by identifying new or existing drugs for treatment; clinical, by supporting diagnosis and evaluating prognosis based on medical imaging and non-invasive measures; and societal, by tracking both the epidemic and the accompanying infodemic using multiple data sources. we also review datasets, tools, and resources needed to facilitate artificial intelligence research, and discuss strategic considerations related to the operational implementation of multidisciplinary partnerships and open science. we highlight the need for international cooperation to maximize the potential of ai in this and future pandemics.",2020
contrasting the spread of misinformation in online social networks,"online social networks are nowadays one of the most effective and widespread tools used to share information. in addition to being employed by individuals for communicating with friends and acquaintances, and by brands for marketing and customer service purposes, they constitute a primary source of daily news for a significant number of users. unfortunately, besides legit news, social networks also allow to effectively spread inaccurate or even entirely fabricated ones. also due to sensationalist claims, misinformation can spread from the original sources to a large number of users in a very short time, with negative consequences that, in extreme cases, can even put at risk public safety or health. in this work we discuss and propose methods to limit the spread of misinformation over online social networks. the issue is split in two separate sub-problems. we first aim to identify the most probable sources of the misinformation among the subset of users that have been reached by it. in the second step, assuming to know the misinformation sources, we want to locate a minimum number of monitors (that is, entities able to identify and block false information) in the network in order to prevent that the misinformation campaign reaches some “critical” nodes while maintaining low the number of nodes exposed to the infection. for each of the two issues, we provide both heuristics and mixed integer programming formulations. to verify the quality and efficiency of our suggested solutions, we conduct experiments on several real-world networks. the results of this extensive experimental phase validate our heuristics as effective tools to contrast the spread of misinformation in online social networks. regarding the source identification step, our approach showed success rates above 80% in most of the considered settings, and above 60% in almost all of them. with respect to the second issue, our heuristic proved to be able to obtain solutions that exceeded (in terms of number of required monitors) the ones obtained through our milp-based approach of more than 20% in only few test scenarios. our heuristics for both problems also proved to outperform significantly some previously proposed algorithms.",2020
to regulate or not: a social dynamics analysis of an idealised ai race,"rapid technological advancements in artificial intelligence (ai), as well as the growing deployment of intelligent technologies in new application domains, have generated serious anxiety and a fear of missing out among different stake-holders, fostering a racing narrative. whether real or not, the belief in such a race for domain supremacy through ai, can make it real simply from its consequences, as put forward by the thomas theorem. these consequences may be negative, as racing for technological supremacy creates a complex ecology of choices that could push stake-holders to underestimate or even ignore ethical and safety procedures. as a consequence, different actors are urging to consider both the normative and social impact of these technological advancements, contemplating the use of the precautionary principle in ai innovation and research. yet, given the breadth and depth of ai and its advances, it is difficult to assess which technology needs regulation and when. as there is no easy access to data describing this alleged ai race, theoretical models are necessary to understand its potential dynamics, allowing for the identification of when procedures need to be put in place to favour outcomes beneficial for all. we show that, next to the risks of setbacks and being reprimanded for unsafe behaviour, the time-scale in which domain supremacy can be achieved plays a crucial role. when this can be achieved in a short term, those who completely ignore the safety precautions are bound to win the race but at a cost to society, apparently requiring regulatory actions. our analysis reveals that imposing regulations for all risk and timing conditions may not have the anticipated effect as only for specific conditions a dilemma arises between what is individually preferred and globally beneficial. similar observations can be made for the long-term development case. yet different from the short-term situation, conditions can be identified that require the promotion of risk-taking as opposed to compliance with safety regulations in order to improve social welfare. these results remain robust both when two or several actors are involved in the race and when collective rather than individual setbacks are produced by risk-taking behaviour. when defining codes of conduct and regulatory policies for applications of ai, a clear understanding of the time-scale of the race is thus required, as this may induce important non-trivial effects. this article is part of the special track on ai and society.",2020
qualitative numeric planning: reductions and complexity,"qualitative numerical planning is classical planning extended with non-negative real variables that can be increased or decreased ""qualitatively"", i.e., by positive indeterminate amounts. while deterministic planning with numerical variables is undecidable in general, qualitative numerical planning is decidable and provides a convenient abstract model for generalized planning. the solutions to qualitative numerical problems (qnps) were shown to correspond to the strong cyclic solutions of an associated fully observable non-deterministic (fond) problem that terminate. this leads to a generate-and-test algorithm for solving qnps where solutions to a fond problem are generated one by one and tested for termination. the computational shortcomings of this approach for solving qnps, however, are that it is not simple to amend fond planners to generate all solutions, and that the number of solutions to check can be doubly exponential in the number of variables. in this work we address these limitations while providing additional insights on qnps. more precisely, we introduce two polynomial-time reductions, one from qnps to fond problems and the other from fond problems to qnps both of which do not involve termination tests. a result of these reductions is that qnps are shown to have the same expressive power and the same complexity as fond problems.",2020
modular structures and atomic decomposition in ontologies,"with the growth of ontologies used in diverse application areas, the need for module extraction and modularisation techniques has risen. the notion of the modular structure of an ontology, which comprises a suitable set of base modules together with their logical dependencies, has the potential to help users and developers in comprehending, sharing, and maintaining an ontology. we have developed a new modular structure, called atomic decomposition (ad), which is based on modules that provide strong logical properties, such as locality-based modules. in this article, we present the theoretical foundations of ad, review its logical and computational properties, discuss its suitability as a modular structure, and report on an experimental evaluation of ad. in addition, we discuss the concept of a modular structure in ontology engineering and provide a survey of existing decomposition approaches.",2020
credibility-limited base revision: new classes and their characterizations,"in this paper we study a kind of operator —known as credibility-limited base revisions— which addresses two of the main issues that have been pointed out to the agm model of belief change. indeed, on the one hand, these operators are defined on belief bases (rather than belief sets) and, on the other hand, they are constructed with the underlying idea that not all new information is accepted. we propose twenty different classes of credibility-limited base revision operators and obtain axiomatic characterizations for each of them. additionally we thoroughly investigate the interrelations (in the sense of inclusion) among all those classes. more precisely, we analyse whether each one of those classes is or is not (strictly) contained in each of the remaining ones.",2020
representing fitness landscapes by valued constraints to understand the complexity of local search,"local search is widely used to solve combinatorial optimisation problems and to model biological evolution, but the performance of local search algorithms on different kinds of fitness landscapes is poorly understood. here we consider how fitness landscapes can be represented using valued constraints, and investigate what the structure of such representations reveals about the complexity of local search. first, we show that for fitness landscapes representable by binary boolean valued constraints there is a minimal necessary constraint graph that can be easily computed. second, we consider landscapes as equivalent if they allow the same (improving) local search moves; we show that a minimal constraint graph still exists, but is np-hard to compute. we then develop several techniques to bound the length of any sequence of local search moves. we show that such a bound can be obtained from the numerical values of the constraints in the representation, and show how this bound may be tightened by considering equivalent representations. in the binary boolean case, we prove that a degree 2 or treestructured constraint graph gives a quadratic bound on the number of improving moves made by any local search; hence, any landscape that can be represented by such a model will be tractable for any form of local search. finally, we build two families of examples to show that the conditions in our tractability results are essential. with domain size three, even just a path of binary constraints can model a landscape with an exponentially long sequence of improving moves. with a treewidth-two constraint graph, even with a maximum degree of three, binary boolean constraints can model a landscape with an exponentially long sequence of improving moves.",2020
epistemic argumentation framework: theory and computation,"the paper introduces the notion of an epistemic argumentation framework(eaf) as a means to integrate the beliefs of a reasoner with argumentation. intuitively, an eaf encodes the beliefs of an agent who reasons about arguments. formally, an eaf is a pair of an argumentation framework and an epistemic constraint. the semantics of the eaf is defined by the notion of an ω-epistemic labelling set,where ω is complete, stable, grounded, or preferred, which is a set of ω-labellings that collectively satisfies the epistemic constraint of the eaf. the paper shows how eaf can represent different views of reasoners on the same argumentation framework. it also includes representing preferences in eaf and multi-agent argumentation. finally, the paper discusses complexity issues and computation using epistemic logic programming.",2020
a differential privacy mechanism that accounts for network effects for crowdsourcing systems,"in crowdsourcing systems, it is important for the crowdsource campaign initiator to incentivize users to share their data to produce results of the desired computational accuracy. this problem becomes especially challenging when users are concerned about the privacy of their data. to overcome this challenge, existing work often aims to provide users with differential privacy guarantees to incentivize privacy-sensitive users to share their data. however, this work neglects the network effect that a user enjoys greater privacy protection when he aligns his participation behaviour with that of other users. to explore this network effect, we formulate the interaction among users regarding their participation decisions as a population game, because a user’s welfare from the interaction depends not only on his own participation decision but also the distribution of others’ decisions. we show that the nash equilibrium of this game consists of a threshold strategy, where all users whose privacy sensitivity is below a certain threshold will participate and the remaining users will not. we characterize the existence and uniqueness of this equilibrium, which depends on the privacy guarantee, the reward provided by the initiator and the population size. based on this equilibria analysis, we design the pine (privacy incentivization with network effects) mechanism and prove that it maximizes the initiator’s payoff while providing participating users with a guaranteed degree of privacy protection. numerical simulations, on both real and synthetic data, show that (i) pine improves the initiator’s expected payoff by up to 75%, compared to state of the art mechanisms that do not consider this effect; (ii) the performance gain by exploiting the network effect is particularly good when the majority of users are flexible over their privacy attitudes and when there are a large number of low quality task performers.",2020
adaptive stress testing: finding likely failure events with reinforcement learning,"finding the most likely path to a set of failure states is important to the analysis of safety-critical systems that operate over a sequence of time steps, such as aircraft collision avoidance systems and autonomous cars. in many applications such as autonomous driving, failures cannot be completely eliminated due to the complex stochastic environment in which the system operates. as a result, safety validation is not only concerned about whether a failure can occur, but also discovering which failures are most likely to occur. this article presents adaptive stress testing (ast), a framework for finding the most likely path to a failure event in simulation. we consider a general black box setting for partially observable and continuous-valued systems operating in an environment with stochastic disturbances. we formulate the problem as a markov decision process and use reinforcement learning to optimize it. the approach is simulation-based and does not require internal knowledge of the system, making it suitable for black-box testing of large systems. we present different formulations depending on whether the state is fully observable or partially observable. in the latter case, we present a modified monte carlo tree search algorithm that only requires access to the pseudorandom number generator of the simulator to overcome partial observability. we also present an extension of the framework, called differential adaptive stress testing (dast), that can find failures that occur in one system but not in another. this type of differential analysis is useful in applications such as regression testing, where we are concerned with finding areas of relative weakness compared to a baseline. we demonstrate the effectiveness of the approach on an aircraft collision avoidance application, where a prototype aircraft collision avoidance system is stress tested to find the most likely scenarios of near mid-air collision.",2020
lifted bayesian filtering in multiset rewriting systems,"we present a model for bayesian filtering (bf) in discrete dynamic systems where multiple entities (inter)-act, i.e. where the system dynamics is naturally described by a multiset rewriting system (mrs). typically, bf in such situations is computationally expensive due to the high number of discrete states that need to be maintained explicitly. we devise a lifted state representation, based on a suitable decomposition of multiset states, such that some factors of the distribution are exchangeable and thus afford an efficient representation. intuitively, this representation groups together similar entities whose properties follow an exchangeable joint distribution. subsequently, we introduce a bf algorithm that works directly on lifted states, without resorting to the original, much larger ground representation. this algorithm directly lends itself to approximate versions by limiting the number of explicitly represented lifted states in the posterior. we show empirically that the lifted representation can lead to a factorial reduction in the representational complexity of the distribution, and in the approximate cases can lead to a lower variance of the estimate and a lower estimation error compared to the original, ground representation.",2020
"reviewing autoencoders for missing data imputation: technical trends, applications and outcomes","missing data is a problem often found in real-world datasets and it can degrade the performance of most machine learning models. several deep learning techniques have been used to address this issue, and one of them is the autoencoder and its denoising and variational variants. these models are able to learn a representation of the data with missing values and generate plausible new ones to replace them. this study surveys the use of autoencoders for the imputation of tabular data and considers 26 works published between 2014 and 2020. the analysis is mainly focused on discussing patterns and recommendations for the architecture, hyperparameters and training settings of the network, while providing a detailed discussion of the results obtained by autoencoders when compared to other state-of-the-art methods, and of the data contexts where they have been applied. the conclusions include a set of recommendations for the technical settings of the network, and show that denoising autoencoders outperform their competitors, particularly the often used statistical methods.",2020
adapting behavior via intrinsic reward: a survey and empirical study,"learning about many things can provide numerous benefits to a reinforcement learning system. for example, learning many auxiliary value functions, in addition to optimizing the environmental reward, appears to improve both exploration and representation learning. the question we tackle in this paper is how to sculpt the stream of experience—how to adapt the learning system’s behavior—to optimize the learning of a collection of value functions. a simple answer is to compute an intrinsic reward based on the statistics of each auxiliary learner, and use reinforcement learning to maximize that intrinsic reward. unfortunately, implementing this simple idea has proven difficult, and thus has been the focus of decades of study. it remains unclear which of the many possible measures of learning would work well in a parallel learning setting where environmental reward is extremely sparse or absent. in this paper, we investigate and compare different intrinsic reward mechanisms in a new bandit-like parallel-learning testbed. we discuss the interaction between reward and prediction learners and highlight the importance of introspective prediction learners: those that increase their rate of learning when progress is possible, and decrease when it is not. we provide a comprehensive empirical comparison of 14 different rewards, including well-known ideas from reinforcement learning and active learning. our results highlight a simple but seemingly powerful principle: intrinsic rewards based on the amount of learning can generate useful behavior, if each individual learner is introspective.",2020
on the complexity of learning a class ratio from unlabeled data,"in the problem of learning a class ratio from unlabeled data, which we call cr learning, the training data is unlabeled, and only the ratios, or proportions, of examples receiving each label are given. the goal is to learn a hypothesis that predicts the proportions of labels on the distribution underlying the sample. this model of learning is applicable to a wide variety of settings, including predicting the number of votes for candidates in political elections from polls. in this paper, we formally define this class and resolve foundational questions regarding the computational complexity of cr learning and characterize its relationship to pac learning. among our results, we show, perhaps surprisingly, that for finite vc classes what can be efficiently cr learned is a strict subset of what can be learned efficiently in pac, under standard complexity assumptions. we also show that there exist classes of functions whose cr learnability is independent of zfc, the standard set theoretic axioms. this implies that cr learning cannot be easily characterized (like pac by vc dimension).",2020
an evaluation of communication protocol languages for engineering multiagent systems,"communication protocols are central to engineering decentralized multiagent systems. modern protocol languages are typically formal and address aspects of decentralization, such as asynchrony. however, modern languages differ in important ways in their basic abstractions and operational assumptions. this diversity makes a comparative evaluation of protocol languages a challenging task. we contribute a rich evaluation of diverse and modern protocol languages. among the selected languages, scribble is based on session types; trace-c and trace-f on trace expressions; hapn on hierarchical state machines, and bspl on information causality. our contribution is four-fold. one, we contribute important criteria for evaluating protocol languages. two, for each criterion, we compare the languages on the basis of whether they are able to specify elementary protocols that go to the heart of the criterion. three, for each language, we map our findings to a canonical architecture style for multiagent systems, highlighting where the languages depart from the architecture. four, we identify design principles for protocol languages as guidance for future research.",2020
bounds on the size of pc and urc formulas,"in this paper, we investigate cnf encodings, for which unit propagation is strong enough to derive a contradiction if the encoding is not consistent with a partial assignment of the variables (unit refutation complete or urc encoding) or additionally to derive all implied literals if the encoding is consistent with the partial assignment (propagation complete or pc encoding). we prove an exponential separation between the sizes of pc and urc encodings without auxiliary variables and strengthen the known results on their relationship to the pc and urc encodings that can use auxiliary variables. besides of this, we prove that the sizes of any two irredundant pc formulas representing the same function differ at most by a factor polynomial in the number of the variables and present an example of a function demonstrating that a similar statement is not true for urc formulas. one of the separations above implies that a q-horn formula may require an exponential number of additional clauses to become a urc formula. on the other hand, for every q-horn formula, we present a polynomial size urc encoding of the same function using auxiliary variables. this encoding is not q-horn in general.",2020
deep reinforcement learning: a state-of-the-art walkthrough,"deep reinforcement learning is a topic that has gained a lot of attention recently, due to the unprecedented achievements and remarkable performance of such algorithms in various benchmark tests and environmental setups. the power of such methods comes from the combination of an already established and strong field of deep learning, with the unique nature of reinforcement learning methods. it is, however, deemed necessary to provide a compact, accurate and comparable view of these methods and their results for the means of gaining valuable technical and practical insights. in this work we gather the essential methods related to deep reinforcement learning, extracting common property structures for three complementary core categories: a) model-free, b) model-based and c) modular algorithms. for each category, we present, analyze and compare state-of-the-art deep reinforcement learning algorithms that achieve high performance in various environments and tackle challenging problems in complex and demanding tasks. in order to give a compact and practical overview of their differences, we present comprehensive comparison figures and tables, produced by reported performances of the algorithms under two popular simulation platforms: the atari learning environment and the mujoco physics simulation platform. we discuss the key differences of the various kinds of algorithms, indicate their potential and limitations, as well as provide insights to researchers regarding future directions of the field.",2020
diagnosis of deep discrete-event systems,"an abduction-based diagnosis technique for a class of discrete-event systems (dess), called deep dess (ddess), is presented. a ddes has a tree structure, where each node is a network of communicating automata, called an active unit (au). the interaction of components within an au gives rise to emergent events. an emergent event occurs when specific components collectively perform a sequence of transitions matching a given regular language. any event emerging in an au triggers the transition of a component in its parent au. we say that the ddes has a deep behavior, in the sense that the behavior of an au is governed not only by the events exchanged by the components within the au but also by the events emerging from child aus. deep behavior characterizes not only living beings, including humans, but also artifacts, such as robots that operate in contexts at varying abstraction levels. surprisingly, experimental results indicate that the hierarchical complexity of the system translates into a decreased computational complexity of the diagnosis task. hence, the diagnosis technique is shown to be (formally) correct as well as (empirically) efficient.",2020
asnets: deep learning for generalised planning,"in this paper, we discuss the learning of generalised policies for probabilistic and classical planning problems using action schema networks (asnets). the asnet is a neural network architecture that exploits the relational structure of (p)pddl planning problems to learn a common set of weights that can be applied to any problem in a domain. by mimicking the actions chosen by a traditional, non-learning planner on a handful of small problems in a domain, asnets are able to learn a generalised reactive policy that can quickly solve much larger instances from the domain. this work extends the asnet architecture to make it more expressive, while still remaining invariant to a range of symmetries that exist in ppddl problems. we also present a thorough experimental evaluation of asnets, including a comparison with heuristic search planners on seven probabilistic and deterministic domains, an extended evaluation on over 18,000 blocksworld instances, and an ablation study. finally, we show that sparsity-inducing regularisation can produce asnets that are compact enough for humans to understand, yielding insights into how the structure of asnets allows them to generalise across a domain.",2020
vocabulary alignment in openly specified interactions,"the problem of achieving common understanding between agents that use different vocabularies has been mainly addressed by techniques that assume the existence of shared external elements, such as a meta-language or a physical environment. in this article, we consider agents that use different vocabularies and only share knowledge of how to perform a task, given by the specification of an interaction protocol. we present a framework that lets agents learn a vocabulary alignment from the experience of interacting. unlike previous work in this direction, we use open protocols that constrain possible actions instead of defining procedures, making our approach more general. we present two techniques that can be used either to learn an alignment from scratch or to repair an existent one, and we evaluate their performance experimentally.",2020
variational bayes in private settings (vips),"many applications of bayesian data analysis involve sensitive information such as personal documents or medical records, motivating methods which ensure that privacy is protected. we introduce a general privacy-preserving framework for variational bayes (vb), a widely used optimization-based bayesian inference method. our framework respects differential privacy, the gold-standard privacy criterion, and encompasses a large class of probabilistic models, called the conjugate exponential (ce) family. we observe that we can straightforwardly privatise vb’s approximate posterior distributions for models in the ce family, by perturbing the expected sufficient statistics of the complete-data likelihood. for a broadly-used class of non-ce models, those with binomial likelihoods, we show how to bring such models into the ce family, such that inferences in the modified model resemble the private variational bayes algorithm as closely as possible, using the polya-gamma data augmentation scheme. the iterative nature of variational bayes presents a further challenge since iterations increase the amount of noise needed. we overcome this by combining: (1) an improved composition method for differential privacy, called the moments accountant, which provides a tight bound on the privacy cost of multiple vb iterations and thus significantly decreases the amount of additive noise; and (2) the privacy amplification effect of subsampling mini-batches from large-scale data in stochastic learning. we empirically demonstrate the effectiveness of our method in ce and non-ce models including latent dirichlet allocation, bayesian logistic regression, and sigmoid belief networks, evaluated on real-world datasets.",2020
towards knowledgeable supervised lifelong learning systems,"learning a sequence of tasks is a long-standing challenge in machine learning. this setting applies to learning systems that observe examples of a range of tasks at different points in time. a learning system should become more knowledgeable as more related tasks are learned. although the problem of learning sequentially was acknowledged for the first time decades ago, the research in this area has been rather limited. research in transfer learning, multitask learning, metalearning and deep learning has studied some challenges of these kinds of systems. recent research in lifelong machine learning and continual learning has revived interest in this problem. we propose proficiente, a full framework for long-term learning systems. proficiente relies on knowledge transferred between hypotheses learned with support vector machines. the first component of the framework is focused on transferring forward selectively from a set of existing hypotheses or functions representing knowledge acquired during previous tasks to a new target task. a second component of proficiente is focused on transferring backward, a novel ability of long-term learning systems that aim to exploit knowledge derived from recent tasks to encourage refinement of existing knowledge. we propose a method that transfers selectively from a task learned recently to existing hypotheses representing previous tasks. the method encourages retention of existing knowledge whilst refining. we analyse the theoretical properties of the proposed framework. proficiente is accompanied by an agnostic metric that can be used to determine if a long-term learning system is becoming more knowledgeable. we evaluate proficiente in both synthetic and real-world datasets, and demonstrate scenarios where knowledgeable supervised learning systems can be achieved by means of transfer.",2020
improving nash social welfare approximations,"we consider the problem of fairly allocating a set of indivisible goods among n agents. various fairness notions have been proposed within the rapidly growing field of fair division, but the nash social welfare (nsw) serves as a focal point. in part, this follows from the ‘unreasonable’ fairness guarantees provided, in the sense that a max nsw allocation meets multiple other fairness metrics simultaneously, all while satisfying a standard economic concept of efficiency, pareto optimality. however, existing approximation algorithms fail to satisfy all of the remarkable fairness guarantees offered by a max nsw allocation, instead targeting only the specific nsw objective. we address this issue by presenting a 2 max nsw, prop-1, 1/(2n) mms, and pareto optimal allocation in strongly polynomial time. our techniques are based on a market interpretation of a fractional max nsw allocation. we present novel definitions of fairness concepts in terms of market prices, and design a new scheme to round a market equilibrium into an integral allocation in a way that provides most of the fairness properties of an integral max nsw allocation.",2020
"bridging the gap between probabilistic model checking and probabilistic planning: survey, compilations, and empirical comparison","markov decision processes are of major interest in the planning community as well as in the model checking community. but in spite of the similarity in the considered formal models, the development of new techniques and methods happened largely independently in both communities. this work is intended as a beginning to unite the two research branches. we consider goal-reachability analysis as a common basis between both communities. the core of this paper is the translation from jani, an overarching input language for quantitative model checkers, into the probabilistic planning domain definition language (ppddl), and vice versa from ppddl into jani. these translations allow the creation of an overarching benchmark collection, including existing case studies from the model checking community, as well as benchmarks from the international probabilistic planning competitions (ippc). we use this benchmark set as a basis for an extensive empirical comparison of various approaches from the model checking community, variants of value iteration, and mdp heuristic search algorithms developed by the ai planning community. on a per benchmark domain basis, techniques from one community can achieve state-ofthe-art performance in benchmarks of the other community. across all benchmark domains of one community, the performance comparison is however in favor of the solvers and algorithms of that particular community. reasons are the design of the benchmarks, as well as tool-related limitations. our translation methods and benchmark collection foster crossfertilization between both communities, pointing out specific opportunities for widening the scope of solvers to different kinds of models, as well as for exchanging and adopting algorithms across communities.",2020
sliding-window thompson sampling for non-stationary settings,"multi-armed bandit (mab) techniques have been successfully applied to many classes of sequential decision problems in the past decades. however, non-stationary settings -- very common in real-world applications -- received little attention so far, and theoretical guarantees on the regret are known only for some frequentist algorithms. in this paper, we propose an algorithm, namely sliding-window thompson sampling (sw-ts), for nonstationary stochastic mab settings. our algorithm is based on thompson sampling and exploits a sliding-window approach to tackle, in a unified fashion, two different forms of non-stationarity studied separately so far: abruptly changing and smoothly changing. in the former, the reward distributions are constant during sequences of rounds, and their change may be arbitrary and happen at unknown rounds, while, in the latter, the reward distributions smoothly evolve over rounds according to unknown dynamics. under mild assumptions, we provide regret upper bounds on the dynamic pseudo-regret of sw-ts for the abruptly changing environment, for the smoothly changing one, and for the setting in which both the non-stationarity forms are present. furthermore, we empirically show that sw-ts dramatically outperforms state-of-the-art algorithms even when the forms of non-stationarity are taken separately, as previously studied in the literature.",2020
conservative extensions in horn description logics with inverse roles,"we investigate the decidability and computational complexity of conservative extensions and the related notions of inseparability and entailment in horn description logics (dls) with inverse roles. we consider both query conservative extensions, defined by requiring that the answers to all conjunctive queries are left unchanged, and deductive conservative extensions, which require that the entailed concept inclusions, role inclusions, and functionality assertions do not change. upper bounds for query conservative extensions are particularly challenging because characterizations in terms of unbounded homomorphisms between universal models, which are the foundation of the standard approach to establishing decidability, fail in the presence of inverse roles. we resort to a characterization that carefully mixes unbounded and bounded homomorphisms and enables a decision procedure that combines tree automata and a mosaic technique. our main results are that query conservative extensions are 2exptime-complete in all dls between eli and horn-alchif and between horn-alc and horn-alchif, and that deductive conservative extensions are 2exptime-complete in all dls between eli and elhif_bot. the same results hold for inseparability and entailment.",2020
predicting strategic behavior from free text,"the connection between messaging and action is fundamental both to web applications, such as web search and sentiment analysis, and to economics. however, while prominent online applications exploit messaging in natural (human) language in order to predict non-strategic action selection, the economics literature focuses on the connection between structured stylized messaging to strategic decisions in games and multi-agent encounters. this paper aims to connect these two strands of research, which we consider highly timely and important due to the vast online textual communication on the web. particularly, we introduce the following question: can free text expressed in natural language serve for the prediction of action selection in an economic context, modeled as a game in order to initiate the research on this question, we introduce the study of an individual’s action prediction in a one-shot game based on free text he/she provides, while being unaware of the game to be played. we approach the problem by attributing commonsensical personality attributes via crowd-sourcing to free texts written by individuals, and employing transductive learning to predict actions taken by these individuals in one-shot games based on these attributes. our approach allows us to train a single classifier that can make predictions with respect to actions taken in multiple games. in experiments with three well-studied games, our algorithm compares favorably with strong alternative approaches. in ablation analysis, we demonstrate the importance of our modeling choices—the representation of the text with the commonsensical personality attributes and our classifier—to the predictive power of our model.",2020
automated conjecturing ii: chomp and reasoned game play,"we demonstrate the use of a program that generates conjectures about positions of the combinatorial game chomp—explanations of why certain moves are bad. these could be used in the design of a chomp-playing program that gives reasons for its moves. we prove one of these chomp conjectures—demonstrating that our conjecturing program can produce genuine chomp knowledge. the conjectures are generated by a general purpose conjecturing program that was previously and successfully used to generate mathematical conjectures. our program is initialized with chomp invariants and example game boards—the conjectures take the form of invariant-relation statements interpreted to be true for all board positions of a certain kind. the conjectures describe a theory of chomp positions. the program uses limited, natural input and suggests how theories generated on-the-fly might be used in a variety of situations where decisions—based on reasons—are required.",2020
preferences single-peaked on a circle,"we introduce the domain of preferences that are single-peaked on a circle, which is a generalization of the well-studied single-peaked domain. this preference restriction is useful, e.g., for scheduling decisions, certain facility location problems, and for one-dimensional decisions in the presence of extremist preferences. we give a fast recognition algorithm of this domain, provide a characterisation by finitely many forbidden subprofiles, and show that many popular single- and multi-winner voting rules are polynomial-time computable on this domain. in particular, we prove that proportional approval voting can be computed in polynomial time for profiles that are single-peaked on a circle. in contrast, kemeny's rule remains hard to evaluate, and several impossibility results from social choice theory can be proved using only profiles in this domain.",2020
ontology reasoning with deep neural networks,"the ability to conduct logical reasoning is a fundamental aspect of intelligent human behavior, and thus an important problem along the way to human-level artificial intelligence. traditionally, logic-based symbolic methods from the field of knowledge representation and reasoning have been used to equip agents with capabilities that resemble human logical reasoning qualities. more recently, however, there has been an increasing interest in using machine learning rather than logic-based symbolic formalisms to tackle these tasks. in this paper, we employ state-of-the-art methods for training deep neural networks to devise a novel model that is able to learn how to effectively perform logical reasoning in the form of basic ontology reasoning. this is an important and at the same time very natural logical reasoning task, which is why the presented approach is applicable to a plethora of important real-world problems. we present the outcomes of several experiments, which show that our model is able to learn to perform highly accurate ontology reasoning on very large, diverse, and challenging benchmarks. furthermore, it turned out that the suggested approach suffers much less from different obstacles that prohibit logic-based symbolic reasoning, and, at the same time, is surprisingly plausible from a biological point of view.",2020
simulating offender mobility: modeling activity nodes from large-scale human activity data,"in recent years, simulation techniques have been applied to investigate the spatiotemporal dynamics of crime. researchers have instantiated mobile offenders in agent-based simulations for theory testing, experimenting with crime prevention strategies, and exploring crime prediction techniques, despite facing challenges due to the complex dynamics of crime and the lack of detailed information about offender mobility. this paper presents a simulation model to explore offender mobility, focusing on the interplay between the agent's awareness space and activity nodes. the simulation generates patterns of individual mobility aiming to cumulatively match crime patterns. to instantiate a realistic urban environment, we use open data to simulate the urban structure, location-based social networks data to represent activity nodes as a proxy for human activity, and taxi trip data as a proxy for human movement between regions of the city. we analyze and systematically compare 35 different mobility strategies and demonstrate the benefits of using large-scale human activity data to simulate offender mobility. the strategies combining taxi trip data or historic crime data with popular activity nodes perform best compared to other strategies, especially for robbery. our approach provides a basis for building agent-based crime simulations that infer offender mobility in urban areas from real-world data.",2020
scalable planning with deep neural network learned transition models,"in many complex planning problems with factored, continuous state and action spaces such as reservoir control, heating ventilation and air conditioning (hvac), and navigation domains, it is difficult to obtain a model of the complex nonlinear dynamics that govern state evolution. however, the ubiquity of modern sensors allows us to collect large quantities of data from each of these complex systems and build accurate, nonlinear deep neural network models of their state transitions. but there remains one major problem for the task of control – how can we plan with deep network learned transition models without resorting to monte carlo tree search and other black-box transition model techniques that ignore model structure and do not easily extend to continuous domains? in this paper, we introduce two types of planning methods that can leverage deep neural network learned transition models: hybrid deep milp planner (hd-milp-plan) and tensorflow planner (tf-plan). in hd-milp-plan, we make the critical observation that the rectified linear unit (relu) transfer function for deep networks not only allows faster convergence of model learning, but also permits a direct compilation of the deep network transition model to a mixed-integer linear program (milp) encoding. further, we identify deep network specific optimizations for hd-milp-plan that improve performance over a base encoding and show that we can plan optimally with respect to the learned deep networks. in tf-plan, we take advantage of the efficiency of auto-differentiation tools and gpu-based computation where we encode a subclass of purely continuous planning problems as recurrent neural networks and directly optimize the actions through backpropagation. we compare both planners and show that tf-plan is able to approximate the optimal plans found by hd-milp-plan in less computation time. hence this article offers two novel planners for continuous state and action domains with learned deep neural net transition models: one optimal method (hd-milp-plan) and a scalable alternative for large-scale problems (tf-plan).",2020
a general approach to multimodal document quality assessment,"the perceived quality of a document is affected by various factors, including grammat- icality, readability, stylistics, and expertise depth, making the task of document quality assessment a complex one. in this paper, we explore this task in the context of assessing the quality of wikipedia articles and academic papers. observing that the visual rendering of a document can capture implicit quality indicators that are not present in the document text — such as images, font choices, and visual layout — we propose a joint model that combines the text content with a visual rendering of the document for document qual- ity assessment. our joint model achieves state-of-the-art results over five datasets in two domains (wikipedia and academic papers), which demonstrates the complementarity of textual and visual features, and the general applicability of our model. to examine what kinds of features our model has learned, we further train our model in a multi-task learning setting, where document quality assessment is the primary task and feature learning is an auxiliary task. experimental results show that visual embeddings are better at learning structural features while textual embeddings are better at learning readability scores, which further verifies the complementarity of visual and textual features.",2020
the effects of experience on deception in human-agent negotiation,"negotiation is the complex social process by which multiple parties come to mutual agreement over a series of issues. as such, it has proven to be a key challenge problem for designing adequately social ais that can effectively navigate this space. artificial ai agents that are capable of negotiating must be capable of realizing policies and strategies that govern offer acceptances, offer generation, preference elicitation, and more. but the next generation of agents must also adapt to reflect their users’ experiences. the best human negotiators tend to have honed their craft through hours of practice and experience. but, not all negotiators agree on which strategic tactics to use, and endorsement of deceptive tactics in particular is a controversial topic for many negotiators. we examine the ways in which deceptive tactics are used and endorsed in non-repeated human negotiation and show that prior experience plays a key role in governing what tactics are seen as acceptable or useful in negotiation. previous work has indicated that people that negotiate through artificial agent representatives may be more inclined to fairness than those people that negotiate directly. we present a series of three user studies that challenge this initial assumption and expand on this picture by examining the role of past experience. this work constructs a new scale for measuring endorsement of manipulative negotiation tactics and introduces its use to artificial intelligence research. it continues by presenting the results of a series of three studies that examine how negotiating experience can change what negotiation tactics and strategies human endorse. study #1 looks at human endorsement of deceptive techniques based on prior negotiating experience as well as representative effects. study #2 further characterizes the negativity of prior experience in relation to endorsement of deceptive techniques. finally, in study #3, we show that the lessons learned from the empirical observations in study #1 and #2 can in fact be induced—by designing agents that provide a specific type of negative experience, human endorsement of deception can be predictably manipulated.",2020
image captioning using facial expression and attention,"benefiting from advances in machine vision and natural language processing techniques, current image captioning systems are able to generate detailed visual descriptions. for the most part, these descriptions represent an objective characterisation of the image, although some models do incorporate subjective aspects related to the observer’s view of the image, such as sentiment; current models, however, usually do not consider the emotional content of images during the caption generation process. this paper addresses this issue by proposing novel image captioning models which use facial expression features to generate image captions. the models generate image captions using long short-term memory networks applying facial features in addition to other visual features at different time steps. we compare a comprehensive collection of image captioning models with and without facial features using all standard evaluation metrics. the evaluation metrics indicate that applying facial features with an attention mechanism achieves the best performance, showing more expressive and more correlated image captions, on an image caption dataset extracted from the standard flickr 30k dataset, consisting of around 11k images containing faces. an analysis of the generated captions finds that, perhaps unexpectedly, the improvement in caption quality appears to come not from the addition of adjectives linked to emotional aspects of the images, but from more variety in the actions described in the captions.",2020
subgoaling techniques for satisficing and optimal numeric planning,"this paper studies novel subgoaling relaxations for automated planning with propositional and numeric state variables. subgoaling relaxations address one source of complexity of the planning problem: the requirement to satisfy conditions simultaneously. the core idea is to relax this requirement by recursively decomposing conditions into atomic subgoals that are considered in isolation. such relaxations are typically used for pruning, or as the basis for computing admissible or inadmissible heuristic estimates to guide optimal or satisificing heuristic search planners. in the last decade or so, the subgoaling principle has underpinned the design of an abundance of relaxation-based heuristics whose formulations have greatly extended the reach of classical planning. this paper extends subgoaling relaxations to support numeric state variables and numeric conditions. we provide both theoretical and practical results, with the aim of reaching a good trade-off between accuracy and computation costs within a heuristic state-space search planner. our experimental results validate the theoretical assumptions, and indicate that subgoaling substantially improves on the state of the art in optimal and satisficing numeric planning via forward state-space search.",2020
how to do things with words: a bayesian approach,"communication changes the beliefs of the listener and of the speaker. the value of a communicative act stems from the valuable belief states which result from this act. to model this we build on the interactive pomdp (ipomdp) framework, which extends pomdps to allow agents to model others in multi-agent settings, and we include communication that can take place between the agents to formulate communicative ipomdps (cipomdps). we treat communication as a type of action and therefore, decisions regarding communicative acts are based on decision-theoretic planning using the bellman optimality principle and value iteration, just as they are for all other rational actions. as in any form of planning, the results of actions need to be precisely specified. we use the bayes’ theorem to derive how agents update their beliefs in cipomdps; updates are due to agents’ actions, observations, messages they send to other agents, and messages they receive from others. the bayesian decision-theoretic approach frees us from the commonly made assumption of cooperative discourse – we consider agents which are free to be dishonest while communicating and are guided only by their selfish rationality. we use a simple tiger game to illustrate the belief update, and to show that the ability to rationally communicate allows agents to improve efficiency of their interactions.",2020
gradient-based learning methods extended to smooth manifolds applied to automated clustering,"grassmann manifold based sparse spectral clustering is a classification technique that consists in learning a latent representation of data, formed by a subspace basis, which is sparse. in order to learn a latent representation, spectral clustering is formulated in terms of a loss minimization problem over a smooth manifold known as grassmannian. such minimization problem cannot be tackled by one of traditional gradient-based learning algorithms, which are only suitable to perform optimization in absence of constraints among parameters. it is, therefore, necessary to develop specific optimization/learning algorithms that are able to look for a local minimum of a loss function under smooth constraints in an efficient way. such need calls for manifold optimization methods. in this paper, we extend classical gradient-based learning algorithms on at parameter spaces (from classical gradient descent to adaptive momentum) to curved spaces (smooth manifolds) by means of tools from manifold calculus. we compare clustering performances of these methods and known methods from the scientific literature. the obtained results confirm that the proposed learning algorithms prove lighter in computational complexity than existing ones without detriment in clustering efficacy.",2020
towards partial order reductions for strategic ability,"we propose a general semantics for strategic abilities of agents in asynchronous systems, with and without perfect information. based on the semantics, we show some general complexity results for verification of strategic abilities in asynchronous interaction. more importantly, we develop a methodology for partial order reduction in verification of agents with imperfect information. we show that the reduction preserves an important subset of strategic properties, with as well as without the fairness assumption. we also demonstrate the effectiveness of the reduction on a number of benchmarks. interestingly, the reduction does not work for strategic abilities under perfect information.",2020
"best-first enumeration based on bounding conflicts, and its application to large-scale hybrid estimation","there is an increasing desire for autonomous systems to have high levels of robustness and safety, attained through continuously planning and self-repairing online. underlying this is the need to accurately estimate the system state and diagnose subtle failures. estimation methods based on hybrid discrete and continuous state models have emerged as a method of precisely computing these estimates. however, existing methods have difficulty scaling to systems with more than a handful of components. discrete, consistency based state estimation capabilities can scale to this level by combining best-first enumeration and conflict-directed search. while best-first methods have been developed for hybrid estimation, conflict-directed methods have thus far been elusive as conflicts learn inconsistencies from constraint violation, but probabilistic hybrid estimation is relatively unconstrained. in this paper we present an approach to hybrid estimation that unifies best-first enumeration and conflict-directed search through the concept of ""bounding"" conflicts, an extension of conflicts that represent tighter bounds on the cost of regions of the search space. this paper presents a general best-first enumeration algorithm based on bounding conflicts (a*bc) and a hybrid estimation method using this enumeration algorithm. experiments show that an a*bc powered state estimator produces estimates up to an order of magnitude faster than the current state of the art, particularly on large systems.",2020
the impact of treewidth on grounding and solving of answer set programs,"in this paper, we aim to study how the performance of modern answer set programming (asp) solvers is influenced by the treewidth of the input program and to investigate the consequences of this relationship. we first perform an experimental evaluation that shows that the solving performance is heavily influenced by treewidth, given ground input programs that are otherwise uniform, both in size and construction. this observation leads to an important question for asp, namely, how to design encodings such that the treewidth of the resulting ground program remains small. to this end, we study two classes of disjunctive programs, namely guarded and connection-guarded programs. in order to investigate these classes, we formalize the grounding process using mso transductions. our main results show that both classes guarantee that the treewidth of the program after grounding only depends on the treewidth (and the maximum degree, in case of connection-guarded programs) of the input instance. in terms of parameterized complexity, our findings yield corresponding fpt results for answer-set existence for bounded treewidth (and also degree, for connection-guarded programs) of the input instance. we further show that bounding treewidth alone leads to np-hardness in the data complexity for connection-guarded programs,which indicates that the two classes are fundamentally different. finally, we show that for both classes, the data complexity remainsas hard as in the general case of asp.",2020
the 2^k neighborhoods for grid path planning,"grid path planning is an important problem in ai. its understanding has been key for the development of autonomous navigation systems. an interesting and rather surprising fact about the vast literature on this problem is that only a few neighborhoods have been used when evaluating these algorithms. indeed, only the 4- and 8-neighborhoods are usually considered, and rarely the 16-neighborhood. this paper describes three contributions that enable the construction of effective grid path planners for extended 2k-neighborhoods; that is, neighborhoods that admit 2k neighbors per state, where k is a parameter. first, we provide a simple recursive definition of the 2k-neighborhood in terms of the 2k-1-neighborhood. second, we derive distance functions, for any k ≥ 2, which allow us to propose admissible heuristics that are perfect for obstacle-free grids, which generalize the well-known manhattan and octile distances. third, we define the notion of canonical path for the 2k-neighborhood; this allows us to incorporate our neighborhoods into two versions of a*, namely canonical a* and jump point search (jps), whose performance, we show, scales well when increasing k. our empirical evaluation shows that, when increasing k, the cost of the solution found improves substantially. used with the 2k-neighborhood, canonical a* and jps, in many configurations, are also superior to the any-angle path planner theta* both in terms of solution quality and runtime. our planner is competitive with one implementation of the any-angle path planner, anya in some configurations. our main practical conclusion is that standard, well-understood grid path planning technology may provide an effective approach to any-angle grid path planning.",2020
regret bounds for reinforcement learning via markov chain concentration,"we give a simple optimistic algorithm for which it is easy to derive regret bounds of o(sqrt{t-mix sat}) steps in uniformly ergodic markov decision processes with s states, a actions, and mixing time parameter t-mix. these bounds are the first regret bounds in the general, non-episodic setting with an optimal dependence on all given parameters. they could only be improved by using an alternative mixing time parameter.",2020
saturated cost partitioning for optimal classical planning,"cost partitioning is a method for admissibly combining a set of admissible heuristic estimators by distributing operator costs among the heuristics. computing an optimal cost partitioning, i.e., the operator cost distribution that maximizes the heuristic value, is often prohibitively expensive to compute. saturated cost partitioning is an alternative that is much faster to compute and has been shown to yield high-quality heuristics. however, its greedy nature makes it highly susceptible to the order in which the heuristics are considered. we propose a greedy algorithm to generate orders and show how to use hill-climbing search to optimize a given order. combining both techniques leads to significantly better heuristic estimates than using the best random order that is generated in the same time. since there is often no single order that gives good guidance on the whole state space, we use the maximum of multiple orders as a heuristic that is significantly better informed than any single-order heuristic, especially when we actively search for a set of diverse orders.",2020
the force awakens: artificial intelligence for consumer law,"recent years have been tainted by market practices that continuously expose us, as consumers, to new risks and threats. we have become accustomed, and sometimes even resigned, to businesses monitoring our activities, examining our data, and even meddling with our choices. artificial intelligence (ai) is often depicted as a weapon in the hands of businesses and blamed for allowing this to happen. in this paper, we envision a paradigm shift, where ai technologies are brought to the side of consumers and their organizations, with the aim of building an efficient and effective counter-power. ai-powered tools can support a massive-scale automated analysis of textual and audiovisual data, as well as code, for the benefit of consumers and their organizations. this in turn can lead to a better oversight of business activities, help consumers exercise their rights, and enable the civil society to mitigate information overload. we discuss the societal, political, and technological challenges that stand before that vision. this article is part of the special track on ai and society.",2020
blind spot detection for safe sim-to-real transfer,"agents trained in simulation may make errors when performing actions in the real world due to mismatches between training and execution environments. these mistakes can be dangerous and difficult for the agent to discover because the agent is unable to predict them a priori. in this work, we propose the use of oracle feedback to learn a predictive model of these blind spots in order to reduce costly errors in real-world applications. we focus on blind spots in reinforcement learning (rl) that occur due to incomplete state representation: when the agent lacks necessary features to represent the true state of the world, and thus cannot distinguish between numerous states. we formalize the problem of discovering blind spots in rl as a noisy supervised learning problem with class imbalance. our system learns models for predicting blind spots within unseen regions of the state space by combining techniques for label aggregation, calibration, and supervised learning. these models take into consideration noise emerging from different forms of oracle feedback, including demonstrations and corrections. we evaluate our approach across two domains and demonstrate that it achieves higher predictive performance than baseline methods, and also that the learned model can be used to selectively query an oracle at execution time to prevent errors. we also empirically analyze the biases of various feedback types and how these biases influence the discovery of blind spots. further, we include analyses of our approach that incorporate relaxed initial optimality assumptions. (interestingly, relaxing the assumptions of an optimal oracle and an optimal simulator policy helped our models to perform better.) we also propose extensions to our method that are intended to improve performance when using corrections and demonstrations data.",2020
planning for hybrid systems via satisfiability modulo theories,"planning for hybrid systems is important for dealing with real-world applications, and pddl+ supports this representation of domains with mixed discrete and continuous dynamics. in this paper we present a new approach for planning for hybrid systems, based on encoding the planning problem as a satisfiability modulo theories (smt) formula. this is the first smt encoding that can handle the whole set of pddl+ features (including processes and events), and is implemented in the planner smtplan. smtplan not only covers the full semantics of pddl+, but can also deal with non-linear polynomial continuous change without discretization. this allows it to generate plans with non-linear dynamics that are correct-by-construction. the encoding is based on the notion of happenings, and can be applied on domains with nonlinear continuous change. we describe the encoding in detail and provide in-depth examples. we apply this encoding in an iterative deepening planning algorithm. experimental results show that the approach dramatically outperforms existing work in finding plans for pddl+ problems. we also present experiments which explore the performance of the proposed approach on temporal planning problems, showing that the scalability of the approach is limited by the size of the discrete search space. we further extend the encoding to include planning with control parameters. the extended encoding allows the definition of actions to include infinite domain parameters, called control parameters. we present experiments on a set of problems with control parameters to demonstrate the positive effect they provide to the approach of planning via smt.",2020
tensorlog: a probabilistic database implemented using deep-learning infrastructure,"we present an implementation of a probabilistic first-order logic called tensorlog, in which classes of logical queries are compiled into differentiable functions in a neural-network infrastructure such as tensorflow or theano. this leads to a close integration of probabilistic logical reasoning with deep-learning infrastructure: in particular, it enables high-performance deep learning frameworks to be used for tuning the parameters of a probabilistic logic. the integration with these frameworks enables use of gpu-based parallel processors for inference and learning, making tensorlog the first highly parallellizable probabilistic logic. experimental results show that tensorlog scales to problems involving hundreds of thousands of knowledge-base triples and tens of thousands of examples.",2020
jointly improving parsing and perception for natural language commands through human-robot dialog,"in this work, we present methods for using human-robot dialog to improve language understanding for a mobile robot agent. the agent parses natural language to underlying semantic meanings and uses robotic sensors to create multi-modal models of perceptual concepts like red and heavy. the agent can be used for showing navigation routes, delivering objects to people, and relocating objects from one location to another. we use dialog clari_cation questions both to understand commands and to generate additional parsing training data. the agent employs opportunistic active learning to select questions about how words relate to objects, improving its understanding of perceptual concepts. we evaluated this agent on amazon mechanical turk. after training on data induced from conversations, the agent reduced the number of dialog questions it asked while receiving higher usability ratings. additionally, we demonstrated the agent on a robotic platform, where it learned new perceptual concepts on the y while completing a real-world task.",2020
adversarial attacks on crowdsourcing quality control,"crowdsourcing is a popular methodology to collect manual labels at scale. such labels are often used to train ai models and, thus, quality control is a key aspect in the process. one of the most popular quality assurance mechanisms in paid micro-task crowdsourcing is based on gold questions: the use of a small set of tasks of which the requester knows the correct answer and, thus, is able to directly assess crowd work quality. in this paper, we show that such mechanism is prone to an attack carried out by a group of colluding crowd workers that is easy to implement and deploy: the inherent size limit of the gold set can be exploited by building an inferential system to detect which parts of the job are more likely to be gold questions. the described attack is robust to various forms of randomisation and programmatic generation of gold questions. we present the architecture of the proposed system, composed of a browser plug-in and an external server used to share information, and briefly introduce its potential evolution to a decentralised implementation. we implement and experimentally validate the gold detection system, using real-world data from a popular crowdsourcing platform. our experimental results show that crowdworkers using the proposed system spend more time on signalled gold questions but do not neglect the others thus achieving an increased overall work quality. finally, we discuss the economic and sociological implications of this kind of attack.",2020
graph width measures for cnf-encodings with auxiliary variables,"we consider bounded width cnf-formulas where the width is measured by popular graph width measures on graphs associated to cnf-formulas. such restricted graph classes, in particular those of bounded treewidth, have been extensively studied for their uses in the design of algorithms for various computational problems on cnf-formulas. here we consider the expressivity of these formulas in the model of clausal encodings with auxiliary variables. we first show that bounding the width for many of the measures from the literature leads to a dramatic loss of expressivity, restricting the formulas to such of low communication complexity. we then show that the width of optimal encodings with respect to different measures is strongly linked: there are two classes of width measures, one containing primal treewidth and the other incidence cliquewidth, such that in each class the width of optimal encodings only differs by constant factors. moreover, between the two classes the width differs at most by a factor logarithmic in the number of variables. both these results are in stark contrast to the setting without auxiliary variables where all width measures we consider here differ by more than constant factors and in many cases even by linear factors.",2020
catching cheats: detecting strategic manipulation in distributed optimisation of electric vehicle aggregators,"given the rapid rise of electric vehicles (evs) worldwide, and the ambitious targets set for the near future, the management of large ev fleets must be seen as a priority. specifically, we study a scenario where ev charging is managed through self-interested ev aggregators who compete in the day-ahead market in order to purchase the electricity needed to meet their clients' requirements. with the aim of reducing electricity costs and lowering the impact on electricity markets, a centralised bidding coordination framework has been proposed in the literature employing a coordinator. in order to improve privacy and limit the need for the coordinator, we propose a reformulation of the coordination framework as a decentralised algorithm, employing the alternating direction method of multipliers (admm). however, given the self-interested nature of the aggregators, they can deviate from the algorithm in order to reduce their energy costs. hence, we study the strategic manipulation of the admm algorithm and, in doing so, describe and analyse different possible attack vectors and propose a mathematical framework to quantify and detect manipulation. importantly, this detection framework is not limited to the considered ev scenario and can be applied to general admm algorithms. finally, we test the proposed decentralised coordination and manipulation detection algorithms in realistic scenarios using real market and driver data from spain. our empirical results show that the decentralised algorithm's convergence to the optimal solution can be effectively disrupted by manipulative attacks achieving convergence to a different non-optimal solution which benefits the attacker. with respect to the detection algorithm, results indicate that it achieves very high accuracies and significantly outperforms a naive benchmark.",2020
fair allocation with diminishing differences,"ranking alternatives is a natural way for humans to explain their preferences. it is used in many settings, such as school choice, course allocations and residency matches. without having any information on the underlying cardinal utilities, arguing about the fairness of allocations requires extending the ordinal item ranking to ordinal bundle ranking. the most commonly used such extension is stochastic dominance (sd), where a bundle x is preferred over a bundle y if its score is better according to all additive score functions. sd is a very conservative extension, by which few allocations are necessarily fair while many allocations are possibly fair. we propose to make a natural assumption on the underlying cardinal utilities of the players, namely that the difference between two items at the top is larger than the difference between two items at the bottom. this assumption implies a preference extension which we call diminishing differences (dd), where x is preferred over y if its score is better according to all additive score functions satisfying the dd assumption. we give a full characterization of allocations that are necessarily-proportional or possibly-proportional according to this assumption. based on this characterization, we present a polynomial-time algorithm for finding a necessarily-dd-proportional allocation whenever it exists. using simulations, we compare the various fairness criteria in terms of their probability of existence, and their probability of being fair by the underlying cardinal valuations. we find that necessary-dd-proportionality fares well in both measures. we also consider envy-freeness and pareto optimality under diminishing-differences, as well as chore allocation under the analogous condition --- increasing-differences.",2020
a global constraint for the exact cover problem: application to conceptual clustering,"we introduce the exactcover global constraint dedicated to the exact cover problem, the goal of which is to select subsets such that each element of a given set belongs to exactly one selected subset. this np-complete problem occurs in many applications, and we more particularly focus on a conceptual clustering application. we introduce three propagation algorithms for exactcover, called basic, dl, and dl+: basic ensures the same level of consistency as arc consistency on a classical decomposition of exactcover into binary constraints, without using any specific data structure; dl ensures the same level of consistency as basic but uses dancing links to efficiently maintain the relation between elements and subsets; and dl+ is a stronger propagator which exploits an extra property to filter more values than dl. we also consider the case where the number of selected subsets is constrained to be equal to a given integer variable k, and we show that this may be achieved either by combining exactcover with existing constraints, or by designing a specific propagator that integrates algorithms designed for the nvalues constraint. these different propagators are experimentally evaluated on conceptual clustering problems, and they are compared with state-of-the-art declarative approaches. in particular, we show that our global constraint is competitive with recent ilp and cp models for mono-criterion problems, and it has better scale-up properties for multi-criteria problems.",2020
robust multi-agent path finding and executing,"multi-agent path-finding (mapf) is the problem of finding a plan for moving a set of agents from their initial locations to their goals without collisions. following this plan, however, may not be possible due to unexpected events that delay some of the agents. in this work, we propose a holistic solution for mapf that is robust to such unexpected delays. first, we introduce the notion of a k-robust mapf plan, which is a plan that can be executed even if a limited number (k) of delays occur. we propose sufficient and required conditions for finding a k-robust plan, and show how to convert several mapf solvers to find such plans. then, we propose several robust execution policies. an execution policy is a policy for agents executing a mapf plan. an execution policy is robust if following it guarantees that the agents reach their goals even if they encounter unexpected delays. several classes of such robust execution policies are proposed and evaluated experimentally. finally, we present robust execution policies for cases where communication between the agents may also be delayed. we performed an extensive experimental evaluation in which we compared different algorithms for finding robust mapf plans, compared different ro- bust execution policies, and studied the interplay between having a robust plan and the performance when using a robust execution policy.",2020
agreement on target-bidirectional recurrent neural networks for sequence-to-sequence learning,"recurrent neural networks are extremely appealing for sequence-to-sequence learning tasks. despite their great success, they typically suffer from a shortcoming: they are prone to generate unbalanced targets with good prefixes but bad suffixes, and thus performance suffers when dealing with long sequences. we propose a simple yet effective approach to overcome this shortcoming. our approach relies on the agreement between a pair of target-directional rnns, which generates more balanced targets. in addition, we develop two efficient approximate search methods for agreement that are empirically shown to be almost optimal in terms of either sequence level or non-sequence level metrics. extensive experiments were performed on three standard sequence-to-sequence transduction tasks: machine transliteration, grapheme-to-phoneme transformation and machine translation. the results show that the proposed approach achieves consistent and substantial improvements, compared to many state-of-the-art systems.",2020
solving delete free planning with relaxed decision diagram based heuristics,"we investigate the use of relaxed decision diagrams (dds) for computing admissible heuristics for the cost-optimal delete-free planning (dfp) problem. our main contributions are the introduction of two novel dd encodings for a dfp task: a multivalued decision diagram that includes the sequencing aspect of the problem and a binary decision diagram representation of its sequential relaxation. we present construction algorithms for each dd that leverage these different perspectives of the dfp task and provide theoretical and empirical analyses of the associated heuristics. we further show that relaxed dds can be used beyond heuristic computation to extract delete-free plans, find action landmarks, and identify redundant actions. our empirical analysis shows that while dd-based heuristics trail the state of the art, even small relaxed dds are competitive with the linear programming heuristic for the dfp task, thus, revealing novel ways of designing admissible heuristics.",2020
a set of recommendations for assessing humanmachine parity in language translation,"the quality of machine translation has increased remarkably over the past years, to the degree that it was found to be indistinguishable from professional human translation in anumber of empirical investigations. we reassess hassan et al.'s 2018 investigation into chinese to english news translation, showing that the finding of human–machine paritywas owed to weaknesses in the evaluation design—which is currently considered best practice in the field. we show that the professional human translations containedsignificantly fewer errors, and that perceived quality in human evaluation depends on the choice of raters, the availability of linguistic context, and the creation of referencetranslations. our results call for revisiting current best practices to assess strong machine translation systems in general and human–machine parity in particular, for which weoffer a set of recommendations based on our empirical findings.",2020
using task descriptions in lifelong machine learning for improved performance and zero-shot transfer,"knowledge transfer between tasks can improve the performance of learned models, but requires an accurate estimate of inter-task relationships to identify the relevant knowledge to transfer. these inter-task relationships are typically estimated based on training data for each task, which is inefficient in lifelong learning settings where the goal is to learn each consecutive task rapidly from as little data as possible. to reduce this burden, we develop a lifelong learning method based on coupled dictionary learning that utilizes high-level task descriptions to model inter-task relationships. we show that using task descriptors improves the performance of the learned task policies, providing both theoretical justification for the benefit and empirical demonstration of the improvement across a variety of learning problems. given only the descriptor for a new task, the lifelong learner is also able to accurately predict a model for the new task through zero-shot learning using the coupled dictionary, eliminating the need to gather training data before addressing the task.",2020
hedonic games with ordinal preferences and thresholds,"we propose a new representation setting for hedonic games, where each agent partitions the set of other agents into friends, enemies, and neutral agents, with friends and enemies being ranked. under the assumption that preferences are monotonic (respectively, antimonotonic) with respect to the addition of friends (respectively, enemies), we propose a bipolar extension of the responsive extension principle, and use this principle to derive the (partial) preferences of agents over coalitions. then, for a number of solution concepts, we characterize partitions that necessarily or possibly satisfy them, and we study the related problems in terms of their complexity.",2020
compositionality decomposed: how do neural networks generalise?,"despite a multitude of empirical studies, little consensus exists on whether neural networks are able to generalise compositionally, a controversy that, in part, stems from a lack of agreement about what it means for a neural model to be compositional. as a response to this controversy, we present a set of tests that provide a bridge between, on the one hand, the vast amount of linguistic and philosophical theory about compositionality of language and, on the other, the successful neural models of language. we collect different interpretations of compositionality and translate them into five theoretically grounded tests for models that are formulated on a task-independent level. in particular, we provide tests to investigate (i) if models systematically recombine known parts and rules (ii) if models can extend their predictions beyond the length they have seen in the training data (iii) if models’ composition operations are local or global (iv) if models’ predictions are robust to synonym substitutions and (v) if models favour rules or exceptions during training. to demonstrate the usefulness of this evaluation paradigm, we instantiate these five tests on a highly compositional data set which we dub pcfg set and apply the resulting tests to three popular sequence-to-sequence models: a recurrent, a convolution-based and a transformer model. we provide an in-depth analysis of the results, which uncover the strengths and weaknesses of these three architectures and point to potential areas of improvement.",2020
incomplete preferences in single-peaked electorates,"incomplete preferences are likely to arise in real-world preference aggregation scenarios. this paper deals with determining whether an incomplete preference profile is single-peaked. this is valuable information since many intractable voting problems become tractable given singlepeaked preferences. we prove that the problem of recognizing single-peakedness is np-complete for incomplete profiles consisting of partial orders. despite this intractability result, we find several polynomial-time algorithms for reasonably restricted settings. in particular, we give polynomial-time recognition algorithms for weak orders, which can be viewed as preferences with indifference.",2020
htn planning as heuristic progression search,"the majority of search-based htn planning systems can be divided into those searching a space of partial plans (a plan space) and those performing progression search, i.e., that build the solution in a forward manner. so far, all htn planners that guide the search by using heuristic functions are based on plan space search. those systems represent the set of search nodes more effectively by maintaining a partial ordering between tasks, but they have only limited information about the current state during search. in this article, we propose the use of progression search as basis for heuristic htn planning systems. such systems can calculate their heuristics incorporating the current state, because it is tracked during search. our contribution is the following: we introduce two novel progression algorithms that avoid unnecessary branching when the problem at hand is partially ordered and show that both are sound and complete. we show that defining systematicity is problematic for search in htn planning, propose a definition, and show that it is fulfilled by one of our algorithms. then, we introduce a method to apply arbitrary classical planning heuristics to guide the search in htn planning. it relaxes the htn planning model to a classical model that is only used for calculating heuristics. it is updated during search and used to create heuristic values that are used to guide the htn search. we show that it can be used to create htn heuristics with interesting theoretical properties like safety, goal-awareness, and admissibility. our empirical evaluation shows that the resulting system outperforms the state of the art in search-based htn planning.",2020
learning the language of software errors,"we propose to use algorithms for learning deterministic finite automata (dfa), such as angluin’s l* algorithm, for learning a dfa that describes the possible scenarios under which a given program error occurs. the alphabet of this automaton is given by the user (for instance, a subset of the function call sites or branches), and hence the automaton describes a user-defined abstraction of those scenarios. more generally, the same technique can be used for visualising the behavior of a program or parts thereof. it can also be used for visually comparing different versions of a program (by presenting an automaton for the behavior in the symmetric difference between them), and for assisting in merging several development branches. we present experiments that demonstrate the power of an abstract visual representation of errors and of program segments, accessible via the project’s web page. in addition, our experiments in this paper demonstrate that such automata can be learned efficiently over real-world programs. we also present lazy learning, which is a method for reducing the number of membership queries while using l*, and demonstrate its effectiveness on standard benchmarks.",2020
solving the torpedo scheduling problem,"the article presents a solution approach for the torpedo scheduling problem, an operational planning problem found in steel production. the problem consists of the integrated scheduling and routing of torpedo cars, i. e. steel transporting vehicles, from a blast furnace to steel converters. in the continuous metallurgic transformation of iron into steel, the discrete transportation step of molten iron must be planned with considerable care in order to ensure a continuous material flow. the problem is solved by a simulated annealing algorithm, coupled with an approach of reducing the set of feasible material assignments. the latter is based on logical reductions and lower bound calculations on the number of torpedo cars. experimental investigations are performed on a larger number of problem instances, which stem from the 2016 implementation challenge of the association of constraint programming (acp). our approach was ranked first (joint first place) in the 2016 acp challenge and found optimal solutions for all used instances in this challenge.",2019
xeggora: exploiting immune-to-evidence symmetries with full aggregation in statistical relational models,"we present improvements in maximum a-posteriori inference for markov logic, a widely used srl formalism. inferring the most probable world for markov logic is np-hard in general. several approaches, including cutting plane aggregation (cpa), perform inference through translation to integer linear programs. aggregation exploits context-specific symmetries independently of evidence and reduces the size of the program. we illustrate much more symmetries occurring in long ground clauses that are ignored by cpa and can be exploited by higher-order aggregations. we propose full-constraint-aggregation, a superior algorithm to cpa which exploits the ignored symmetries via a lifted translation method and some constraint relaxations. rdbms and heuristic techniques are involved to improve the overall performance. we introduce xeggora as an evolutionary extension of rockit, the query engine that uses cpa. xeggora evaluation on real-world benchmarks shows progress in efficiency compared to rockit especially for models with long formulas.",2019
recognizing top-monotonic preference profiles in polynomial time,"we provide the first polynomial-time algorithm for recognizing if aprofile of (possibly weak) preference orders is top-monotonic.top-monotonicity is a generalization of the notions ofsingle-peakedness and single-crossingness, defined by barbera and moreno. top-monotonic profiles always have weak condorcet winnersand satisfy a variant of the median voter theorem. our algorithm proceeds by reducing the recognition problem to thesat-2cnf problem.",2019
pattern-based approach to the workflow satisfiability problem with user-independent constraints,"the fixed parameter tractable (fpt) approach is a powerful tool in tackling computationally hard problems. in this paper, we link fpt results to classic artificial intelligence (ai) techniques to show how they complement each other. specifically, we consider the workflow satisfiability problem (wsp) which asks whether there exists an assignment of authorised users to the steps in a workflow specification, subject to certain constraints on the assignment. it was shown by cohen et al. (jair 2014) that wsp restricted to the class of user-independent constraints (ui), covering many practical cases, admits fpt algorithms, i.e. can be solved in time exponential only in the number of steps k and polynomial in the number of users n. since usually k &lt;&lt; n in wsp, such fpt algorithms are of great practical interest. we present a new interpretation of the fpt nature of the wsp with ui constraints giving a decomposition of the problem into two levels. exploiting this two-level split, we develop a new fpt algorithm that is by many orders of magnitude faster than the previous state-of-the-art wsp algorithm and also has only polynomial-space complexity. we also introduce new pseudo-boolean (pb) and constraint satisfaction (csp) formulations of the wsp with ui constraints which efficiently exploit this new decomposition of the problem and raise the novel issue of how to use general-purpose solvers to tackle fpt problems in a fashion that meets fpt efficiency expectations. in our computational study, we investigate, for the first time, the phase transition (pt) properties of the wsp, under a model for generation of random instances. we show how pt studies can be extended, in a novel fashion, to support empirical evaluation of scaling of fpt algorithms.",2019
from support propagation to belief propagation in constraint programming,"the distinctive driving force of constraint programming to solve combinatorial problems has been a privileged access to problem structure through the high-level models it uses. from that exposed structure in the form of so-called global constraints, powerful inference algorithms have shared information between constraints by propagating it through shared variables’ domains, traditionally by removing unsupported values. this paper investigates a richer propagation medium made possible by recent work on counting solutions inside constraints. beliefs about individual variable-value assignments are exchanged between contraints and iteratively adjusted. it generalizes standard support propagation and aims to converge to the true marginal distributions of the solutions over individual variables. its advantage over standard belief propagation is that the higher-level models featuring large-arity (global) constraints do not tend to create as many cycles, which are known to be problematic for convergence. the necessary architectural changes to a constraint programming solver are described and an empirical study of the proposal is conducted on its implementation. we find that it provides close approximations to the true marginals and that it significantly improves search guidance.",2019
multi-fidelity gaussian process bandit optimisation,"in many scientific and engineering applications, we are tasked with the maximisation of an expensive to evaluate black box function f. traditional settings for this problem assume just the availability of this single function. however, in many cases, cheap approximations to f may be obtainable. for example, the expensive real world behaviour of a robot can be approximated by a cheap computer simulation. we can use these approximations to eliminate low function value regions cheaply and use the expensive evaluations of f in a small but promising region and speedily identify the optimum. we formalise this task as a multi-fidelity bandit problem where the target function and its approximations are sampled from a gaussian process. we develop mf-gp-ucb, a novel method based on upper confidence bound techniques. in our theoretical analysis we demonstrate that it exhibits precisely the above behaviour and achieves better bounds on the regret than strategies which ignore multi-fidelity information. empirically, mf-gp-ucb outperforms such naive strategies and other multi-fidelity methods on several synthetic and real experiments.",2019
"timed atl: forget memory, just count","in this paper we investigate the timed alternating-time temporal logic (tatl), a discrete-time extension of atl. in particular, we propose, systematize, and further study semantic variants of tatl, based on different notions of a strategy. the notions are derived from different assumptions about the agents’ memory and observational capabilities, and range from timed perfect recall to untimed memoryless plans. we also introduce a new semantics based on counting the number of visits to locations during the play. we show that all the semantics, except for the untimed memoryless one, are equivalent when punctuality constraints are not allowed in the formulae. in fact, abilities in all those notions of a strategy collapse to the “counting” semantics with only two actions allowed per location. on the other hand, this simple pattern does not extend to the full tatl. as a consequence, we establish a hierarchy of tatl semantics, based on the expressivity of the underlying strategies, and we show when some of the semantics coincide. in particular, we prove that more compact representations are possible for a reasonable subset of tatl specifications, which should improve the efficiency of model checking and strategy synthesis.",2019
context vectors are reflections of word vectors in half the dimensions,"this paper takes a step towards theoretical analysis of the relationship between word embeddings and context embeddings in models such as word2vec. we start from basic probabilistic assumptions on the nature of word vectors, context vectors, and text generation. these assumptions are supported either empirically or theoretically by the existing literature. next, we show that under these assumptions the widely-used word-word pmi matrix is approximately a random symmetric gaussian ensemble. this, in turn, implies that context vectors are reflections of word vectors in approximately half the dimensions. as a direct application of our result, we suggest a theoretically grounded way of tying weights in the sgns model.",2019
what is this article about? extreme summarization with topic-aware convolutional neural networks,"we introduce ""extreme summarization,"" a newsingle-document summarization task which aims at creating a short,one-sentence news summary answering the question ""what is thearticle about?"". we argue that extreme summarization, by nature, isnot amenable to extractive strategies and requires an abstractivemodeling approach. in the hope of driving research on this taskfurther: (a) we collect a real-world, large scale dataset byharvesting online articles from the british broadcasting corporation(bbc); and (b) propose a novel abstractive model which isconditioned on the article's topics and based entirely onconvolutional neural networks. we demonstrate experimentally thatthis architecture captures long-range dependencies in a document andrecognizes pertinent content, outperforming an oracle extractivesystem and state-of-the-art abstractive approaches when evaluated automatically and by humans on the extreme summarizationdataset.",2019
formulas free from inconsistency: an atom-centric characterization in priest's minimally inconsistent lp,"as one of fundamental properties to characterize inconsistency measures for knowledge bases, the property of free formula independence well captures the intuition that free formulas are independent of the amount of inconsistency in a knowledge base for cases where inconsistency is characterized in terms of minimal inconsistent subsets. but it has been argued that not all the free formulas are independent of inconsistency in some other contexts of inconsistency characterization. in this paper, we propose a characterization of formulas independent of inconsistency in the framework of priest's minimally inconsistent lp. based on an atom-based counterpart of the notion of free formula, we propose a notion of bi-free formula to describe formulas that are free from inconsistency in both syntax and paraconsistent models in this logic. then we propose the property of bi-free formula independence, which is more suitable for characterizing the role of formulas free from inconsistency in measuring inconsistency from both syntactic and semantic perspectives.",2019
type-aware convolutional neural networks for slot filling,"the slot filling task aims at extracting answers for queries about entities from text, such as ""who founded apple"". in this paper, we focus on the relation classification component of a slot filling system. we propose type-aware convolutional neural networks to benefit from the mutual dependencies between entity and relation classification. in particular, we explore different ways of integrating the named entity types of the relation arguments into a neural network for relation classification, including a joint training and a structured prediction approach. to the best of our knowledge, this is the first study on type-aware neural networks for slot filling. the type-aware models lead to the best results of our slot filling pipeline. joint training performs comparable to structured prediction. to understand the impact of the different components of the slot filling pipeline, we perform a recall analysis, a manual error analysis and several ablation studies. such analyses are of particular importance to other slot filling researchers since the official slot filling evaluations only assess pipeline outputs. the analyses show that especially coreference resolution and our convolutional neural networks have a large positive impact on the final performance of the slot filling pipeline. the presented models, the source code of our system as well as our coreference resource is publicly available.",2019
a survey on temporal reasoning for temporal information extraction from text,"time is deeply woven into how people perceive, and communicate about the world. almost unconsciously, we provide our language utterances with temporal cues, like verb tenses, and we can hardly produce sentences without such cues. extracting temporal cues from text, and constructing a global temporal view about the order of described events is a major challenge of automatic natural language understanding. temporal reasoning, the process of combining different temporal cues into a coherent temporal view, plays a central role in temporal information extraction. this article presents a comprehensive survey of the research from the past decades on temporal reasoning for automatic temporal information extraction from text, providing a case study on how combining symbolic reasoning with machine learning-based information extraction systems can improve performance. it gives a clear overview of the used methodologies for temporal reasoning, and explains how temporal reasoning can be, and has been successfully integrated into temporal information extraction systems. based on the distillation of existing work, this survey also suggests currently unexplored research areas. we argue that the level of temporal reasoning that current systems use is still incomplete for the full task of temporal information extraction, and that a deeper understanding of how the various types of temporal information can be integrated into temporal reasoning is required to drive future research in this area.",2019
dstl: solution to limitation of small corpus in speech emotion recognition,"traditional machine learning methods share a common hypothesis: training and testing datasets must be in a common feature space with the same distribution. however, in reality, the labeled target data may be rare, so that target space does not share the same feature space or distribution as an available training set (source domain). to address the mismatch of domains, we propose a dual-subspace transfer learning (dstl) framework that considers both the common and specific information of the two domains. in dstl, a latent common subspace is first learned to preserve the data properties and reduce the discrepancy of domains. then, we propose a mapping strategy to transfer the sourcespecific information to the target subspace. the integration of the domain-common and specific information constructs the proposed dstl framework. in comparison to the stateart-of works, the main contribution of our work is that the dstl framework not only considers the commonalities, but also exploits the specific information. experiments on three emotional speech corpora verify the effectiveness of our approach. the results show that the methods which include both domain-common and specific information perform better than the baseline methods which only exploit the domain commonalities.",2019
revisiting counting solutions for the global cardinality constraint,"counting solutions for a combinatorial problem has been identified as an important concern within the artificial intelligence field. it is indeed very helpful when exploring the structure of the solution space. in this context, this paper revisits the computation process to count solutions for the global cardinality constraint in the context of counting-based search. it first highlights an error and then presents a way to correct the upper bound on the number of solutions for this constraint.",2019
community structure in industrial sat instances,"modern sat solvers have experienced a remarkable progress on solving industrial instances. it is believed that most of these successful techniques exploit the underlying structure of industrial instances. recently, there have been some attempts to analyze the structure of industrial sat instances in terms of complex networks, with the aim of explaining the success of sat solving techniques, and possibly improving them. in this paper, we study the community structure, or modularity, of industrial sat instances. in a graph with clear community structure, or high modularity, we can find a partition of its nodes into communities such that most edges connect variables of the same community. representing sat instances as graphs, we show that most application benchmarks are characterized by a high modularity. on the contrary, random sat instances are closer to the classical erdos-renyi random graph model, where no structure can be observed. we also analyze how this structure evolves by the effects of the execution of a cdcl sat solver, and observe that new clauses learned by the solver during the search contribute to destroy the original structure of the formula. motivated by this observation, we finally present an application that exploits the community structure to detect relevant learned clauses, and we show that detecting these clauses results in an improvement on the performance of the sat solver. empirically, we observe that this improves the performance of several sat solvers on industrial sat formulas, especially on satisfiable instances.",2019
multi-agent inverse reinforcement learning for certain general-sum stochastic games,"this paper addresses the problem of multi-agent inverse reinforcement learning (mirl) in a two-player general-sum stochastic game framework. five variants of mirl are considered: ucs-mirl, adve-mirl, cooe-mirl, uce-mirl, and une-mirl, each distinguished by its solution concept. problem ucs-mirl is a cooperative game in which the agents employ cooperative strategies that aim to maximize the total game value. in problem uce-mirl, agents are assumed to follow strategies that constitute a correlated equilibrium while maximizing total game value. problem une-mirl is similar to uce-mirl in total game value maximization, but it is assumed that the agents are playing a nash equilibrium. problems adve-mirl and cooe-mirl assume agents are playing an adversarial equilibrium and a coordination equilibrium, respectively. we propose novel approaches to address these five problems under the assumption that the game observer either knows or is able to accurately estimate the policies and solution concepts for players. for ucs-mirl, we first develop a characteristic set of solutions ensuring that the observed bi-policy is a ucs and then apply a bayesian inverse learning method. for uce-mirl, we develop a linear programming problem subject to constraints that define necessary and sufficient conditions for the observed policies to be correlated equilibria. the objective is to choose a solution that not only minimizes the total game value difference between the observed bi-policy and a local ucs, but also maximizes the scale of the solution. we apply a similar treatment to the problem of une-mirl. the remaining two problems can be solved efficiently by taking advantage of solution uniqueness and setting up a convex optimization problem. results are validated on various benchmark grid-world games.",2019
synthesizing argumentation frameworks from examples,"argumentation is today a topical area of artificial intelligence (ai) research. abstract argumentation, with argumentation frameworks (afs) as the underlying knowledge representation formalism, is a central viewpoint to argumentation in ai. indeed, from the perspective of ai and computer science, understanding computational and representational aspects of afs is key in the study of argumentation. realizability of afs has been recently proposed as a central notion for analyzing the expressive power of afs under different semantics. in this work, we propose and study the af synthesis problem as a natural extension of realizability, addressing some of the shortcomings arising from the relatively stringent definition of realizability. in particular, realizability gives means of establishing exact conditions on when a given collection of subsets of arguments has an af with exactly the given collection as its set of extensions under a specific argumentation semantics. however, in various settings within the study of dynamics of argumentation---including revision and aggregation of afs---non-realizability can naturally occur. to accommodate such settings, our notion of af synthesis seeks to construct, or synthesize, afs that are semantically closest to the knowledge at hand even when no afs exactly representing the knowledge exist. going beyond defining the af synthesis problem, we study both theoretical and practical aspects of the problem. in particular, we (i) prove np-completeness of af synthesis under several semantics, (ii) study basic properties of the problem in relation to realizability, (iii) develop algorithmic solutions to np-hard af synthesis using the constraint optimization paradigms of maximum satisfiability and answer set programming, (iv) empirically evaluate our algorithms on different forms of af synthesis instances, as well as (v) discuss variants and generalizations of af synthesis.",2019
acceptable planning: influencing individual behavior to reduce transportation energy expenditure of a city,"our research aims at developing intelligent systems to reduce the transportation-related energy expenditure of a large city by influencing individual behavior. we introduce copter - an intelligent travel assistant that evaluates multi-modal travel alternatives to find a plan that is acceptable to a person given their context and preferences. we propose a formulation for acceptable planning that brings together ideas from ai, machine learning, and economics. this formulation has been incorporated in copter that produces acceptable plans in real-time. we adopt a novel empirical evaluation framework that combines human decision data with a high fidelity multi-modal transportation simulation to demonstrate a 4% energy reduction and 20% delay reduction in a realistic deployment scenario in los angeles, california, usa. this article is part of the special track on ai and society.",2019
variable elimination in binary csps,"we investigate rules which allow variable elimination in binary csp (constraint satisfaction problem) instances while conserving satisfiability. we study variable-elimination rules based on the language of forbidden patterns enriched with counting and quantification over variables and values. we propose new rules and compare them, both theoretically and experimentally. we give optimised algorithms to apply these rules and show that each define a novel tractable class. using our variable-elimination rules in preprocessing allowed us to solve more benchmark problems than without.",2019
on non-cooperativeness in social distance games,"we consider social distance games (sdgs), that is cluster formation games in which the utility of each agent only depends on the composition of the cluster she belongs to, proportionally to her harmonic centrality, i.e., to the average inverse distance from the other agents in the cluster. under a non-cooperative perspective, we adopt nash stable outcomes, in which no agent can improve her utility by unilaterally changing her coalition, as the target solution concept. although a nash equilibrium for a sdg can always be computed in polynomial time, we obtain a negative result concerning the game convergence and we prove that computing a nash equilibrium that maximizes the social welfare is np-hard by a polynomial time reduction from the np-complete restricted exact cover by 3-sets problem. we then focus on the performance of nash equilibria and provide matching upper bound and lower bounds on the price of anarchy of θ(n), where n is the number of nodes of the underlying graph. moreover, we show that there exists a class of sdgs having a lower bound on the price of stability of 6/5 − ε, for any ε &gt; 0. finally, we characterize the price of stability 5 of sdgs for graphs with girth 4 and girth at least 5, the girth being the length of the shortest cycle in the graph.",2019
on the time and space complexity of genetic programming for evolving boolean conjunctions,"genetic programming (gp) is a general purpose bio-inspired meta-heuristic for the evolution of computer programs. in contrast to the several successful applications, there is little understanding of the working principles behind gp. in this paper we present a performance analysis that sheds light on the behaviour of simple gp systems for evolving conjunctions of n variables (andn). the analysis of a random local search gp system with minimal terminal and function sets reveals the relationship between the number of iterations and the progress the gp makes toward finding the target function. afterwards we consider a more realistic gp system equipped with a global mutation operator and prove that it can efficiently solve andn by producing programs of linear size that fit a training set to optimality and with high probability generalise well. additionally, we consider more general problems which extend the terminal set with undesired variables or negated variables. in the presence of undesired variables, we prove that, if non-strict selection is used, then the algorithm fits the complete training set efficiently while the strict selection algorithm may fail with high probability unless the substitution operator is switched off. if negations are allowed, we show that while the algorithms fail to fit the complete training set, the constructed solutions generalise well. finally, from a problem hardness perspective, we reveal the existence of small training sets that allow the evolution of the exact conjunctions even with access to negations or undesired variables.",2019
embedding projection for targeted cross-lingual sentiment: model comparisons and a real-world study,"sentiment analysis benefits from large, hand-annotated resources in order to train and test machine learning models, which are often data hungry. while some languages, e.g., english, have a vast arrayof these resources, most under-resourced languages do not, especially for fine-grained sentiment tasks, such as aspect-level or targeted sentiment analysis. to improve this situation, we propose a cross-lingual approach to sentiment analysis that is applicable to under-resourced languages and takes into account target-level information. this model incorporates sentiment information into bilingual distributional representations, byjointly optimizing them for semantics and sentiment, showing state-of-the-art performance at sentence-level when combined with machine translation. the adaptation to targeted sentiment analysis on multiple domains shows that our model outperforms other projection-based bilingual embedding methods on binary targetedsentiment tasks. our analysis on ten languages demonstrates that the amount of unlabeled monolingual data has surprisingly little effect on the sentiment results. as expected, the choice of a annotated source language for projection to a target leads to better results for source-target language pairs which are similar. therefore, our results suggest that more efforts should be spent on the creation of resources for less similar languages tothose which are resource-rich already. finally, a domain mismatch leads to a decreased performance. this suggests resources in any language should ideally cover varieties of domains.",2019
interpretable charge prediction for criminal cases with dynamic rationale attention,"charge prediction which aims to determine appropriate charges for criminal cases based on textual fact descriptions, is an important technology in the ﬁeld of ai&amp;law. previous works focus on improving prediction accuracy, ignoring the interpretability, which limits the methods’ applicability. in this work, we propose a deep neural framework to extract short but charge-decisive text snippets – rationales – from input fact description, as the interpretation of charge prediction. to solve the scarcity problem ofrationale annotatedcorpus, rationalesare extractedinareinforcement stylewiththe only supervision in the form of charge labels. we further propose a dynamic rationale attention mechanism to better utilize the information in extracted rationales and predict the charges. experimental results show that besides providing charge prediction interpretation, our approach can also capture subtle details to help charge prediction.",2019
full characterization of parikh's relevance-sensitive axiom for belief revision,"in this article, the epistemic-entrenchment and partial-meet characterizations of parikh's relevance-sensitive axiom for belief revision, known as axiom (p), are provided. in short, axiom (p) states that, if a belief set $k$ can be divided into two disjoint compartments, and the new information $\varphi$ relates only to the first compartment, then the revision of $k$ by $\varphi$ should not affect the second compartment. accordingly, we identify the subclass of epistemic-entrenchment and that of selection-function preorders, inducing agm revision functions that satisfy axiom (p). hence, together with the faithful-preorders characterization of (p) that has already been provided, parikh's axiom is fully characterized in terms of all popular constructive models of belief revision. since the notions of relevance and local change are inherent in almost all intellectual activity, the completion of the constructive view of (p) has a significant impact on many theoretical, as well as applied, domains of artificial intelligence.",2019
enhancing statement evaluation in argumentation via multi-labelling systems,"in computational models of argumentation, the justification of statements has drawn less attention than the construction and justification of arguments. significant losses of sensitivity or expressibility on statement statuses can be incurred by otherwise appealing formalisms. in order to reappraise statement statuses and, more generally, to support a uniform modelling of different phases of the argumentation process we introduce multi-labelling systems, a generic formalism devoted to represent reasoning processes consisting of a sequence of labelling stages. in the argumentation context, two families of multi-labelling systens, called argument-focused and statement-focused approach are identified and compared. then they are shown to be able to encompass several prominent literature proposals as special cases, thereby enabling a systematic comparison evidencing their merits and limits. further, we show that the proposed model supports tunability of statement justification by specifying a few alternative statement justification labellings, and we illustrate how they can be seamlessly integrated into different formalisms.",2019
"deep dialog act recognition using multiple token, segment, and context information representations","automatic dialog act recognition is a task that has been widely explored over the years. in recent works, most approaches to the task explored different deep neural network architectures to combine the representations of the words in a segment and generate a segment representation that provides cues for intention. in this study, we explore means to generate more informative segment representations, not only by exploring different network architectures, but also by considering different token representations, not only at the word level, but also at the character and functional levels. at the word level, in addition to the commonly used uncontextualized embeddings, we explore the use of contextualized representations, which are able to provide information concerning word sense and segment structure. character-level tokenization is important to capture intention-related morphological aspects that cannot be captured at the word level. finally, the functional level provides an abstraction from words, which shifts the focus to the structure of the segment. additionally, we explore approaches to enrich the segment representation with context information from the history of the dialog, both in terms of the classifications of the surrounding segments and the turn-taking history. this kind of information has already been proved important for the disambiguation of dialog acts in previous studies. nevertheless, we are able to capture additional information by considering a summary of the dialog history and a wider turn-taking context. by combining the best approaches at each step, we achieve performance results that surpass the previous state-of-the-art on generic dialog act recognition on both the switchboard dialog act corpus (swda) and the icsi meeting recorder dialog act corpus (mrda), which are two of the most widely explored corpora for the task. furthermore, by considering both past and future context, similarly to what happens in an annotation scenario, our approach achieves a performance similar to that of a human annotator on swda and surpasses it on mrda.",2019
general game playing with imperfect information,"general game playing is a field which allows the researcher to investigate techniques that might eventually be used in an agent capable of artificial general intelligence. game playing presents a controlled environment in which to evaluate ai techniques, and so we have seen an increase in interest in this field of research. games of imperfect information offer the researcher an additional challenge in terms of complexity over games with perfect information. in this article, we look at imperfect-information games: their expression, their complexity, and the additional demands of their players. we consider the problems of working with imperfect information and introduce a technique called hyperplay, for efficiently sampling very large information sets, and present a formalism together with pseudo code so that others may implement it. we examine the design choices for the technique, show its soundness and completeness then provide some experimental results and demonstrate the use of the technique in a variety of imperfect-information games, revealing its strengths, weaknesses, and its efficiency against randomly generating samples. improving the technique, we present hyperplay-ii, capable of correctly valuing information-gathering moves. again, we provide some experimental results and demonstrate the use of the new technique revealing its strengths, weaknesses and its limitations.",2019
classifying inconsistency measures using graphs,"the aim of measuring inconsistency is to obtain an evaluation of the imperfections in a set of formulas, and this evaluation may then be used to help decide on some course of action (such as rejecting some of the formulas, resolving the inconsistency, seeking better sources of information, etc). a number of proposals have been made to define measures of inconsistency. each has its rationale. but to date, it is not clear how to delineate the space of options for measures, nor is it clear how we can classify measures systematically. to address these problems, we introduce a general framework for comparing syntactic measures of inconsistency. it is based on the notion of an inconsistency graph for each knowledgebase (a bipartite graph with a set of vertices representing formulas in the knowledgebase, a set of vertices representing minimal inconsistent subsets of the knowledgebase, and edges representing that a formula belongs to a minimal inconsistent subset). we then show that various measures can be computed using the inconsistency graph. then we introduce abstractions of the inconsistency graph and use them to construct a hierarchy of syntactic inconsistency measures. furthermore, we extend the inconsistency graph concept with a labeling that extends the hierarchy to include some other types of inconsistency measures.",2019
a semantic characterization asp base revision,"the paper deals with base revision for answer set programming (asp). base revision in classical logic is done by the removal of formulas. exploiting the non-monotonicity of asp allows one to propose other revision strategies, namely addition strategy or removal and/or addition strategy. these strategies allow one to define families of rule-based revision operators. the paper presents a semantic characterization of these families of revision operators in terms of answer sets. this semantic characterization allows for equivalently considering the evolution of syntactic logic programs and the evolution of their semantic content. it then studies the logical properties of the proposed operators and gives complexity results.",2019
strategic abstention based on preference extensions: positive results and computer-generated impossibilities,"voting rules allow multiple agents to aggregate their preferences in order to reach joint decisions.a common flaw of some voting rules, known as the no-show paradox, is that agents may obtain a more preferred outcome by abstaining from an election.we study strategic abstention for set-valued voting rules based on kelly's and fishburn's preference extensions.our contribution is twofold.first, we show that, whenever there are at least five alternatives and seven agents, everypareto-optimal majoritarian voting rule suffers from the no-show paradox with respect to fishburn's extension.this is achieved by reducing the statement to a finite - yet very large - problem, which is encoded as a formula in propositional logic and then shown to be unsatisfiable by a sat solver.we also provide a human-readable proof which we extracted from a minimal unsatisfiable core of the formula.secondly, we prove that every voting rule that satisfies two natural conditions cannot be manipulated by strategic abstention with respect to kelly's extension and give examples of well-known pareto-optimal majoritarian voting rules that meet these requirements.",2019
approximating weighted and priced bribery in scoring rules,"the classic bribery problem is to find a minimal subset of voters who need to change their vote to make some preferred candidate win. its important generalizations consider voters who are weighted and also have different prices. we provide an approximate solution for these problems for a broad family of scoring rules (which includes borda, t-approval, and dowdall).our algorithm is based on a randomized reduction from these bribery generalizations to weighted coalitional manipulation (wcm). to solve this wcm instance, we apply the birkhoff-von neumann (bvn) decomposition to a fractional manipulation matrix. this allows us to limit the size of the possible ballot search space reducing it from exponential to polynomial, while still obtaining good approximation guarantees. finding a solution in the truncated search space yields a new algorithm for wcm, which is of independent interest.",2019
if nothing is accepted -- repairing argumentation frameworks,"conflicting information in an agent's knowledge base may lead to a semantical defect, that is, a situation where it is impossible to draw any plausible conclusion. finding out the reasons for the observed inconsistency (so-called diagnoses) and/or restoring consistency in a certain minimal way (so-called repairs) are frequently occurring issues in knowledge representation and reasoning. in this article we provide a series of first results for these problems in the context of abstract argumentation theory regarding the two most important reasoning modes, namely credulous as well as sceptical acceptance. our analysis includes the following problems regarding minimal repairs/diagnoses: existence, verification, computation of one and enumeration of all solutions. the latter problem is tackled with a version of the so-called hitting set duality first introduced by raymond reiter in 1987. it turns out that grounded semantics plays an outstanding role not only in terms of complexity, but also as a useful tool to reduce the search space for diagnoses regarding other semantics.",2019
preference orders on families of sets - when can impossibility results be avoided?,"lifting a preference order on elements of some universe to a preference order on subsets of this universe is often guided by postulated properties the lifted order should have. well-known impossibility results pose severe limits on when such liftings exist if all non-empty subsets of the universe are to be ordered. the extent to which these negative results carry over to other families of sets is not known. in this paper, we consider families of sets that induce connected subgraphs in graphs. for such families, common in applications, we study whether lifted orders satisfying the well-studied axioms of dominance and (strict) independence exist for every or, in another setting, for some underlying order on elements (strong and weak orderability). we characterize families that are strongly and weakly orderable under dominance and strict independence, and obtain a tight bound on the class of families that are strongly orderable under dominance and independence.",2019
on overfitting and asymptotic bias in batch reinforcement learning with partial observability,"this paper provides an analysis of the tradeoff between asymptotic bias (suboptimality with unlimited data) and overfitting (additional suboptimality due to limited data) in the context of reinforcement learning with partial observability.our theoretical analysis formally characterizes that while potentially increasing the asymptotic bias, a smaller state representation decreases the risk of overfitting.this analysis relies on expressing the quality of a state representation by bounding $l_1$ error terms of the associated belief states.theoretical results are empirically illustrated when the state representation is a truncated history of observations, both on synthetic pomdps and on a large-scale pomdp in the context of smartgrids, with real-world data.finally, similarly to known results in the fully observable setting, we also briefly discuss and empirically illustrate how using function approximators and adapting the discount factor may enhance the tradeoff between asymptotic bias and overfitting in the partially observable context.",2019
a coupled operational semantics for goals and commitments,"commitments capture how an agent relates to anotheragent, whereas goals describe states of the world that an agent ismotivated to bring about. commitments are elements of the socialstate of a set of agents whereas goals are elements of the privatestates of individual agents. it makes intuitive sense that goals andcommitments are understood as being complementary to each other.more importantly, an agent's goals and commitments ought to becoherent, in the sense that an agent's goals would lead it to adoptor modify relevant commitments and an agent's commitments would leadit to adopt or modify relevant goals. however, despite theintuitive naturalness of the above connections, they have not beenadequately studied in a formal framework. this article provides a combined operational semantics for goals andcommitments by relating their respective life cycles as a basis forhow these concepts (1) cohere for an individual agent and (2) engendercooperation among agents. our semantics yields important desirableproperties of convergence of the configurations of cooperatingagents, thereby delineating some theoretically well-founded yetpractical modes of cooperation in a multiagent system.",2019
reba: a refinement-based architecture for knowledge representation and reasoning in robotics,"this article describes reba, a knowledge representation and reasoning architecture for robots that is based on tightly-coupled transition diagrams of the domain at two different levels of granularity. an action language is extended to support non-boolean fluents and non-deterministic causal laws, and used to describe the domain's transition diagrams, with the fine-resolution transition diagram being defined as a refinement of the coarse-resolution transition diagram. the coarse-resolution system description, and a history that includes prioritized defaults, are translated into an answer set prolog (asp) program. for any given goal, inference in the asp program provides a plan of abstract actions. to implement each such abstract action, the robot automatically zooms to the part of the fine-resolution transition diagram relevant to this action. the zoomed fine-resolution system description, and a probabilistic representation of the uncertainty in sensing and actuation, are used to construct a partially observable markov decision process (pomdp). the policy obtained by solving the pomdp is invoked repeatedly to implement the abstract action as a sequence of concrete actions. the fine-resolution outcomes of executing these concrete actions are used to infer coarse-resolution outcomes that are added to the coarse-resolution history and used for subsequent coarse-resolution reasoning. the architecture thus combines the complementary strengths of declarative programming and probabilistic graphical models to represent and reason with non-monotonic logic-based and probabilistic descriptions of uncertainty and incomplete domain knowledge. in addition, we describe a general methodology for the design of software components of a robot based on these knowledge representation and reasoning tools, and provide a path for proving the correctness of these components. the architecture is evaluated in simulation and on a mobile robot finding and moving target objects to desired locations in indoor domains, to show that the architecture supports reliable and efficient reasoning with violation of defaults, noisy observations and unreliable actions, in complex domains.",2019
dependency learning for qbf,"quantified boolean formulas (qbfs) can be used to succinctly encode problems from domains such as formal verification, planning, and synthesis. one of the main approaches to qbf solving is quantified conflict driven clause learning (qcdcl). by default, qcdcl assigns variables in the order of their appearance in the quantifier prefix so as to account for dependencies among variables. dependency schemes can be used to relax this restriction and exploit independence among variables in certain cases, but only at the cost of nontrivial interferences with the proof system underlying qcdcl. we introduce dependency learning, a new technique for exploiting variable independence within qcdcl that allows solvers to learn variable dependencies on the fly. the resulting version of qcdcl enjoys improved propagation and increased flexibility in choosing variables for branching while retaining ordinary (long-distance) q-resolution as its underlying proof system. we show that dependency learning can achieve exponential speedups over ordinary qcdcl. experiments on standard benchmark sets demonstrate the effectiveness of this technique.",2019
goal recognition design in deterministic environments,"goal recognition design (grd) facilitates understanding the goals of acting agents through the analysis and redesign of goal recognition models, thus offering a solution for assessing and minimizing the maximal progress of any agent in the model before goal recognition is guaranteed. in a nutshell, given a model of a domain and a set of possible goals, a solution to a grd problem determines (1) the extent to which actions performed by an agent within the model reveal the agent’s objective; and (2) how best to modify the model so that the objective of an agent can be detected as early as possible. this approach is relevant to any domain in which rapid goal recognition is essential and the model design can be controlled. applications include intrusion detection, assisted cognition, computer games, and human-robot collaboration. a grd problem has two components: the analyzed goal recognition setting, and a design model specifying the possible ways the environment in which agents act can be modified so as to facilitate recognition. this work formulates a general framework for grd in deterministic and partially observable environments, and offers a toolbox of solutions for evaluating and optimizing model quality for various settings. for the purpose of evaluation we suggest the worst case distinctiveness (wcd) measure, which represents the maximal cost of a path an agent may follow before its goal can be inferred by a goal recognition system. we offer novel compilations to classical planning for calculating wcd in settings where agents are bounded-suboptimal. we then suggest methods for minimizing wcd by searching for an optimal redesign strategy within the space of possible modifications, and using pruning to increase efficiency. we support our approach with an empirical evaluation that measures wcd in a variety of grd settings and tests the efficiency of our compilation-based methods for computing it. we also examine the effectiveness of reducing wcd via redesign and the performance gain brought about by our proposed pruning strategy.",2019
probabilistic planning with reduced models,"reduced models are simplified versions of a given domain, designed to accelerate the planning process. interest in reduced models has grown since the surprising success of determinization in the first international probabilistic planning competition, leading to the development of several enhanced determinization techniques. to address the drawbacks of previous determinization methods, we introduce a family of reduced models in which probabilistic outcomes are classified as one of two types: primary and exceptional. in each model that belongs to this family of reductions, primary outcomes can occur an unbounded number of times per trajectory, while exceptions can occur at most a finite number of times, specified by a parameter. distinct reduced models are characterized by two parameters: the maximum number of primary outcomes per action, and the maximum number of occurrences of exceptions per trajectory. this family of reductions generalizes the well-known most-likely-outcome determinization approach, which includes one primary outcome per action and zero exceptional outcomes per plan. we present a framework to determine the benefits of planning with reduced models, and develop a continual planning approach that handles situations where the number of exceptions exceeds the specified bound during plan execution. using this framework, we compare the performance of various reduced models and consider the challenge of generating good ones automatically. we show that each one of the dimensions---allowing more than one primary outcome or planning for some limited number of exceptions---could improve performance relative to standard determinization. the results place previous work on determinization in a broader context and lay the foundation for a systematic exploration of the space of model reductions.",2019
point-based value iteration for finite-horizon pomdps,"partially observable markov decision processes (pomdps) are a popular formalism for sequential decision making in partially observable environments. since solving pomdps to optimality is a difficult task, point-based value iteration methods are widely used. these methods compute an approximate pomdp solution, and in some cases they even provide guarantees on the solution quality, but these algorithms have been designed for problems with an infinite planning horizon. in this paper we discuss why state-of-the-art point-based algorithms cannot be easily applied to finite-horizon problems that do not include discounting. subsequently, we present a general point-based value iteration algorithm for finite-horizon problems which provides solutions with guarantees on solution quality. furthermore, we introduce two heuristics to reduce the number of belief points considered during execution, which lowers the computational requirements. in experiments we demonstrate that the algorithm is an effective method for solving finite-horizon pomdps.",2019
strong stubborn set pruning for star-topology decoupled state space search,"analyzing reachability in large discrete transition systems is an important sub-problem in several areas of ai, and of cs in general. state space search is a basic method for conducting such an analysis. a wealth of techniques have been proposed to reduce the search space without affecting the existence of (optimal) solution paths. in particular, strong stubborn set (sss) pruning is a prominent such method, analyzing action dependencies to prune commutative parts of the search space. we herein show how to apply this idea to star-topology decoupled state space search, a recent search reformulation method invented in the context of classical ai planning. star-topology decoupled state space search, short decoupled search, addresses planning tasks where a single center component interacts with several leaf components. the search exploits a form of conditional independence arising in this setting: given a fixed path p of transitions by the center, the possible leaf moves compliant with p are independent across the leaves. decoupled search thus searches over center paths only, maintaining the compliant paths for each leaf separately. this avoids the enumeration of combined states across leaves. just like standard search, decoupled search is adversely affected by commutative parts of its search space. the adaptation of strong stubborn set pruning is challenging due to the more complex structure of the search space, and the resulting ways in which action dependencies may affect the search. we spell out how to address this challenge, designing optimality-preserving decoupled strong stubborn set (dsss) pruning methods. we introduce a design for star topologies in full generality, as well as simpler design variants for the practically relevant fork and inverted fork special cases. we show that there are cases where dsss pruning is exponentially more effective than both, decoupled search and sss pruning, exhibiting true synergy where the whole is more than the sum of its parts. empirically, dsss pruning reliably inherits the best of its components, and sometimes outperforms both.",2019
weighted matching markets with budget constraints,"we investigate markets with a set of students on one side and a set of colleges on the other. a student and college can be linked by a weighted contract that defines the student's wage, while a college's budget for hiring students is limited. stability is a crucial requirement for matching mechanisms to be applied in the real world. a standard stability requirement is coalitional stability, i.e., no pair of a college and group of students has any incentive to deviate. we find that a coalitionally stable matching is not guaranteed to exist, verifying the coalitional stability for a given matching is conp-complete, and the problem of finding whether a coalitionally stable matching exists in a given market, is sigmap2-complete: npnp-complete. other negative results also hold when blocking coalitions contain at most two students and one college. given these computational hardness results, we pursue a weaker stability requirement called pairwise stability, where no pair of a college and single student has an incentive to deviate. unfortunately, a pairwise stable matching is not guaranteed to exist either. thus, we consider a restricted market called a typed weighted market, in which students are partitioned into types that induce their possible wages. we then design a strategy-proof and pareto efficient mechanism that works in polynomial-time for computing a pairwise stable matching in typed weighted markets.",2019
optstream: releasing time series privately,"many applications of machine learning and optimization operate on data streams. while these datasets are fundamental to fuel decision-making algorithms, often they contain sensitive information about individuals, and their usage poses significant privacy risks. motivated by an application in energy systems, this paper presents optstream, a novel algorithm for releasing differentially private data streams under the w-event model of privacy. optstream is a 4-step procedure consisting of sampling, perturbation, reconstruction, and post-processing modules. first, the sampling module selects a small set of points to access in each period of interest. then, the perturbation module adds noise to the sampled data points to guarantee privacy. next, the reconstruction module re-assembles non-sampled data points from the perturbed sample points. finally, the post-processing module uses convex optimization over the privacy-preserving output of the previous modules, as well as the privacy-preserving answers of additional queries on the data stream, to improve accuracy by redistributing the added noise. optstream is evaluated on a test case involving the release of a real data stream from the largest european transmission operator. experimental results show that optstream may not only improve the accuracy of state-of-the-art methods by at least one order of magnitude but also supports accurate load forecasting on the privacy-preserving data.",2019
ikbt: solving symbolic inverse kinematics with behavior tree,"inverse kinematics solves the problem of how to control robot arm joints to achieve desired end effector positions, which is critical to any robot arm design and implementations of control algorithms. it is a common misunderstanding that closed-form inverse kinematics analysis is solved. popular software and algorithms, such as gradient descent or any multi-variant equations solving algorithm, claims solving inverse kinematics but only on the numerical level. while the numerical inverse kinematics solutions are relatively straightforward to obtain, these methods often fail, due to dependency on specific numerical values, even when the inverse kinematics solutions exist. therefore, closed-form inverse kinematics analysis is superior, but there is no generalized automated algorithm. up till now, the high-level logical reasoning involved in solving closed-form inverse kinematics made it hard to automate, so it's handled by human experts. we developed ikbt, a knowledge-based intelligent system that can mimic human experts' behaviors in solving closed-from inverse kinematics using behavior tree. knowledge and rules used by engineers when solving closed-from inverse kinematics are encoded as actions in behavior tree. the order of applying these rules is governed by higher level composite nodes, which resembles the logical reasoning process of engineers. it is also the first time that the dependency of joint variables, an important issue in inverse kinematics analysis, is automatically tracked in graph form. besides generating closed-form solutions, ikbt also explains its solving strategies in human (engineers) interpretable form. this is a proof-of-concept of using behavior trees to solve high-cognitive problems.",2019
unifying system health management and automated decision making,"health management of complex dynamic systems has evolved from simple automated alarms into a subfield of artificial intelligence with techniques for analyzing off-nominal conditions and generating responses. this evolution took place largely apart from the development of automated system control, planning, and scheduling (generally referred to in this work as decision making). while there have been efforts to establish an information exchange between system health management and decision making, successful practical implementations of integrated architectures remain limited. this article proposes that rather than being treated as connected yet distinct entities, system health management and decision making should be unified in their formulations. enabled by advances in modeling and algorithms, we believe that a unified approach will increase systems' resilience to faults and improve their effectiveness. we overview the prevalent system health management methodology, illustrate its limitations through numerical examples, and describe a proposed unified approach. we then show how typical system health management concepts are accommodated in the proposed approach without loss of functionality or generality. a computational complexity analysis of the unified approach is also provided.",2019
autonomous target search with multiple coordinated uavs,"search and tracking is the problem of locating a moving target and following it to its destination. in this work, we consider a scenario in which the target moves across a large geographical area by following a road network and the search is performed by a team of unmanned aerial vehicles (uavs). we formulate search and tracking as a combinatorial optimization problem and prove that the objective function is submodular. we exploit this property to devise a greedy algorithm. although this algorithm does not offer strong theoretical guarantees because of the presence of temporal constraints that limit the feasibility of the solutions, it presents remarkably good performance, especially when several uavs are available for the mission. as the greedy algorithm suffers when resources are scarce, we investigate two alternative optimization techniques: constraint programming (cp) and ai planning. both approaches struggle to cope with large problems, and so we strengthen them by leveraging the greedy algorithm. we use the greedy solution to warm start the cp model and to devise a domain-dependent heuristic for planning. our extensive experimental evaluation studies the scalability of the different techniques and identifies the conditions under which one approach becomes preferable to the others.",2019
a survey of cross-lingual word embedding models,"cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. in this survey, we provide a comprehensive typology of cross-lingual word embedding models. we compare their data requirements and objective functions. the recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent, modulo optimization strategies, hyper-parameters, and such. we also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.",2019
computing multi-modal journey plans under uncertainty,"multi-modal journey planning, which allows multiple types of transport within a single trip, is becoming increasingly popular, due to a strong practical interest and an increasing availability of data. in real life, transport networks feature uncertainty. yet, most approaches assume a deterministic environment, making plans more prone to failures such as missed connections and major delays in the arrival. this paper presents an approach to computing optimal contingent plans in multi-modal journey planning. the problem is modeled as a search in an and/or state space. we describe search enhancements used on top of the ao* algorithm. enhancements include admissible heuristics, multiple types of pruning that preserve the completeness and the optimality, and a hybrid search approach with a deterministic and a nondeterministic search. we demonstrate an np-hardness result, with the hardness stemming from the dynamically changing distributions of the travel time random variables. we perform a detailed empirical analysis on realistic transport networks from cities such as montpellier, rome and dublin. the results demonstrate the effectiveness of our algorithmic contributions, and the benefits of contingent plans as compared to standard sequential plans, when the arrival and departure times of buses are characterized by uncertainty.",2019
automatic language identification in texts: a survey,"language identification (“li”) is the problem of determining the natural language that a document or part thereof is written in. automatic li has been extensively researched for over fifty years. today, li is a key part of many text processing pipelines, as text processing techniques generally assume that the language of the input text is known. research in this area has recently been especially active. this article provides a brief history of li research, and an extensive survey of the features and methods used in the li literature. we describe the features and methods using a unified notation, to make the relationships between methods clearer. we discuss evaluation methods, applications of li, as well as off-the-shelfli systems that do not require training by the end user. finally, we identify open issues, survey the work to date on each issue, and propose future directions for research in li.",2019
"the mathematics of changing one's mind, via jeffrey's or via pearl's update rule","evidence in probabilistic reasoning may be ‘hard’ or ‘soft’, that is, it may be of yes/no form, or it may involve a strength of belief, in the unit interval [0, 1]. reasoning with soft, [0, 1]-valued evidence is important in many situations but may lead to different, confusing interpretations. this paper intends to bring more mathematical and conceptual clarity to the field by shifting the existing focus from specification of soft evidence to accomodation of soft evidence. there are two main approaches, known as jeffrey’s rule and pearl’s method; they give different outcomes on soft evidence. this paper argues that they can be understood as correction and as improvement. it describes these two approaches as different ways of updating with soft evidence, highlighting their differences, similarities and applications. this account is based on a novel channel-based approach to bayesian probability. proper understanding of these two update mechanisms is highly relevant for inference, decision tools and probabilistic programming languages.",2019
fair allocation of indivisible goods to asymmetric agents,"we study fair allocation of indivisible goods to agents with unequal entitlements. fair allocation has been the subject of many studies in both divisible and indivisible settings. our emphasis is on the case where the goods are indivisible and agents have unequal entitlements. this problem is a generalization of the work by procaccia and wang (2014) wherein the agents are assumed to be symmetric with respect to their entitlements. although procaccia and wang show an almost fair (constant approximation) allocation exists in their setting, our main result is in sharp contrast to their observation. we show that, in some cases with n agents, no allocation can guarantee better than 1/n approximation of a fair allocation when the entitlements are not necessarily equal. furthermore, we devise a simple algorithm that ensures a 1/n approximation guarantee. our second result is for a restricted version of the problem where the valuation of every agent for each good is bounded by the total value he wishes to receive in a fair allocation. although this assumption might seem without loss of generality, we show it enables us to find a 1/2 approximation fair allocation via a greedy algorithm. finally, we run some experiments on real-world data and show that, in practice, a fair allocation is likely to exist. we also support our experiments by showing positive results for two stochastic variants of the problem, namely stochastic agents and stochastic items.",2019
on inductive abilities of latent factor models for relational learning,"latent factor models are increasingly popular for modeling multi-relational knowledge graphs. by their vectorial nature, it is not only hard to interpret why this class of models works so well, but also to understand where they fail and how they might be improved. we conduct an experimental survey of state-of-the-art models, not towards a purely comparative end, but as a means to get insight about their inductive abilities. to assess the strengths and weaknesses of each model, we create simple tasks that exhibit first, atomic properties of binary relations, and then, common inter-relational inference through synthetic genealogies. based on these experimental results, we propose new research directions to improve on existing models.",2019
rank pruning for dominance queries in cp-nets,"conditional preference networks (cp-nets) are a graphical representation of a person’s (conditional) preferences over a set of discrete features. in this paper, we introduce a novel method of quantifying preference for any given outcome based on a cp-net representation of a user’s preferences. we demonstrate that these values are useful for reasoning about user preferences. in particular, they allow us to order (any subset of) the possible outcomes in accordance with the user’s preferences. further, these values can be used to improve the efficiency of outcome dominance testing. that is, given a pair of outcomes, we can determine which the user prefers more efficiently. through experimental results, we show that this method is more effective than existing techniques for improving dominance testing efficiency. we show that the above results also hold for cp-nets that express indifference between variable values.",2019
new approximations for coalitional manipulation in scoring rules,"we study the problem of coalitional manipulation---where k manipulators try to manipulate an election on m candidates---for any scoring rule, with focus on the borda protocol. we do so in both the weighted and unweighted settings. for these problems, recent approximation approaches have tried to minimize k, the number of manipulators needed to make some preferred candidate p win (thus assuming that the number of manipulators is not limited in advance). in contrast, we focus on minimizing the score margin of p which is the difference between the maximum score of a candidate and the score of p.we provide algorithms that approximate the optimum score margin, which are applicable to any scoring rule. for the specific case of the borda protocol in the unweighted setting, our algorithm provides a superior approximation factor for lower values of k.our methods are novel and adapt techniques from multiprocessor scheduling by carefully rounding an exponentially-large configuration linear program that is solved by using the ellipsoid method with an efficient separation oracle. we believe that such methods could be beneficial in other social choice settings as well.",2019
a generalisation of agm contraction and revision to fragments of first-order logic,"agm contraction and revision assume an underlying logic that contains propositional logic. consequently, this assumption excludes many useful logics such as the horn fragment of propositional logic and most description logics. our goal in this paper is to generalise agm contraction and revision to (near-)arbitrary fragments of classical first-order logic. to this end, we first define a very general logic that captures these fragments. in so doing, we make the modest assumptions that a logic contains conjunction and that information is expressed by closed formulas or sentences. the resulting logic is called first-order conjunctive logic or fc logic for short. we then take as the point of departure the agm approach of constructing contraction functions through epistemic entrenchment, that is the entrenchment-based contraction. we redefine entrenchment-based contraction in ways that apply to any fc logic, which we call fc contraction. we prove a representation theorem showing its compliance with all the agm contraction postulates except for the controversial recovery postulate. we also give methods for constructing revision functions through epistemic entrenchment which we call fc revision; which also apply to any fc logic. we show that if the underlying fc logic contains tautologies then fc revision complies with all the agm revision postulates. finally, in the context of fc logic, we provide three methods for generating revision functions via a variant of the levi identity, which we call contraction, withdrawal and cut generated revision, and explore the notion of revision equivalence. we show that withdrawal and cut generated revision coincide with fc revision and so does contraction generated revision under a finiteness condition.",2019
multi-scale hierarchical residual network for dense captioning,"recent research on dense captioning based on the recurrent neural network and the convolutional neural network has made a great progress. however, mapping from an image feature space to a description space is a nonlinear and multimodel task, which makes it difficult for the current methods to get accurate results. in this paper, we put forward a novel approach for dense captioning based on hourglass-structured residual learning. discriminant feature maps are obtained by incorporating dense connected networks and residual learning in our model. finally, the performance of the approach on the visual genome v1.0 dataset and the region labelled ms-coco (microsoft common objects in context) dataset are demonstrated. the experimental results have shown that our approach outperforms most current methods.",2019
cost-based goal recognition in navigational domains,"goal recognition is the problem of determining an agent's intent by observing her behaviour. contemporary solutions for general task-planning relate the probability of a goal to the cost of reaching it. we adapt this approach to goal recognition in the strict context of path-planning. we show (1) that a simpler formula provides an identical result to current state-of-the-art in less than half the time under all but one set of conditions. further, we prove (2) that the probability distribution based on this technique is independent of an agent's past behaviour and present a revised formula that achieves goal recognition by reference to the agent's starting point and current location only. building on this, we demonstrate (3) that a radius of maximum probability (i.e., the distance from a goal within which that goal is guaranteed to be the most probable) can be calculated from relative cost-distances between the candidate goals and a start location, without needing to calculate any actual probabilities. in this extended version of earlier work, we generalise our framework to the continuous domain and discuss our results, including the conditions under which our findings can be generalised back to goal recognition in general task-planning.",2019
viewpoint: human-in-the-loop artificial intelligence,"little by little, newspapers are revealing the bright future that artificial intelligence (ai) is building. intelligent machines will help everywhere. however, this bright future may have a possible dark side: a dramatic job market contraction before its unpredictable transformation. hence, in a near future, large numbers of job seekers may need financial support while catching up with these novel unpredictable jobs. this possible job market crisis has an antidote inside. in fact, the rise of ai is sustained by the biggest knowledge theft of the recent years. many learning ai machines are extracting knowledge from unaware skilled or unskilled workers by analyzing their interactions. by passionately doing their jobs, many of these workers are shooting themselves in the feet. in this paper, we propose human-in-the-loop artificial intelligence (hitai) as a fairer paradigm for ai systems. recognizing that any ai system has humans in the loop, hitai will reward these aware and unaware knowledge producers with a different scheme: decisions of ai systems generating revenues will repay the legitimate owners of the knowledge used for taking those decisions. as modern merry men, hitai researchers should fight for a fairer robin hood artificial intelligence that gives back what it steals. this article is part of the special track on ai and society.",2019
logical foundations of linked data anonymisation,"the widespread adoption of the linked data paradigm has been driven by the increasing demand for information exchange between organisations, as well as by regulations in domains such as health care and governance that require certain data to be published. in this setting, sensitive information is at high risk of disclosure since published data can be often seamlessly linkedwith arbitrary external data sources.in this paper we lay the logical foundations of anonymisation in the context of linked data. we consider anonymisations of rdf graphs (and, more generally, relational datasets with labelled nulls) and define notions of policy-compliant and linkage-safe anonymisations. policy compliance ensures that an anonymised dataset does not reveal any sensitive information as specified by a policy query. linkage safety ensures that an anonymised dataset remains compliant even if it is linked to (possibly unknown) external datasets available on the web, thus providing provable protection guarantees against data linkage attacks. we establish the computational complexity of the underpinning decision problems both under the open-world semantics inherent to rdf and under the assumption that an attacker has complete, closed-world knowledge over some parts of the original data.",2019
iterative local voting for collective decision-making in continuous spaces,"many societal decision problems lie in high-dimensional continuous spaces not amenable to the voting techniques common for their discrete or single-dimensional counterparts. these problems are typically discretized before running an election or decided upon through negotiation by representatives. we propose a algorithm called iterative local voting for collective decision-making in this setting. in this algorithm, voters are sequentially sampled and asked to modify a candidate solution within some local neighborhood of its current value, as defined by a ball in some chosen norm, with the size of the ball shrinking at a specified rate.we first prove the convergence of this algorithm under appropriate choices of neighborhoods to pareto optimal solutions with desirable fairness properties in certain natural settings: when the voters' utilities can be expressed in terms of some form of distance from their ideal solution, and when these utilities are additively decomposable across dimensions. in many of these cases, we obtain convergence to the societal welfare maximizing solution.we then describe an experiment in which we test our algorithm for the decision of the u.s. federal budget on mechanical turk with over 2,000 workers, employing neighborhoods defined by various l-norm balls. we make several observations that inform future implementations of such a procedure.",2019
level-0 models for predicting human behavior in games,"behavioral game theory seeks to describe the way actual people (as compared to idealized, ""rational"" agents) act in strategic situations. our own recent work has identified iterative models, such as quantal cognitive hierarchy, as the state of the art for predicting human play in unrepeated, simultaneous-move games. iterative models predict that agents reason iteratively about their opponents, building up from a specification of nonstrategic behavior called level-0. a modeler is in principle free to choose any description of level-0 behavior that makes sense for a given setting. however, in practice almost all existing work specifies this behavior as a uniform distribution over actions. in most games it is not plausible that even nonstrategic agents would choose an action uniformly at random, nor that other agents would expect them to do so. a more accurate model for level-0 behavior has the potential to dramatically improve predictions of human behavior, since a substantial fraction of agents may play level-0 strategies directly, and furthermore since iterative models ground all higher-level strategies in responses to the level-0 strategy. our work considers models of the way in which level-0 agents construct a probability distribution over actions, given an arbitrary game. we considered a large space of alternatives and, in the end, recommend a model that achieved excellent performance across the board: a linear weighting of four binary features, each of which is general in the sense that it can be computed from any normal form game. adding real-valued variants of the same four features yielded further improvements in performance, albeit with a corresponding increase in the number of parameters needing to be estimated. we evaluated the effects of combining these new level-0 models with several iterative models and observed large improvements in predictive accuracy.",2019
a sampling approach for proactive project scheduling under generalized time-dependent workability uncertainty,"in real-world project scheduling applications, activity durations are often uncertain. proactive scheduling can effectively cope with the duration uncertainties, by generating robust baseline solutions according to a priori stochastic knowledge. however, most of the existing proactive approaches assume that the duration uncertainty of an activity is not related to its scheduled start time, which may not hold in many real-world scenarios. in this paper, we relax this assumption by allowing the duration uncertainty to be time-dependent, which is caused by the uncertainty of whether the activity can be executed on each time slot. we propose a stochastic optimization model to find an optimal partial-order schedule (pos) that minimizes the expected makespan. this model can cover both the time-dependent uncertainty studied in this paper and the traditional time-independent duration uncertainty. to circumvent the underlying complexity in evaluating a given solution, we approximate the stochastic optimization model based on sample average approximation (saa). finally, we design two efficient branch-and-bound algorithms to solve the np-hard saa problem. empirical evaluation confirms that our approach can generate high-quality proactive solutions for a variety of uncertainty distributions.",2019
revisiting cfr+ and alternating updates,"the cfr+ algorithm for solving imperfect information games is a variant of the popular cfr algorithm, with faster empirical performance on a range of problems. it was introduced with a theoretical upper bound on solution error, but subsequent work showed an error in one step of the proof. we provide updated proofs to recover the original bound.",2019
dynamic controllability of controllable conditional temporal problems with uncertainty,"dynamic controllability (dc) of a simple temporal problem with uncertainty (stpu) uses a dynamic decision strategy, rather than a fixed schedule, to tackle temporal uncertainty. we extend this concept to the controllable conditional temporal problem with uncertainty (cctpu), which extends the stpu by conditioning temporal constraints on the assignment of controllable discrete variables. we define dynamic controllability of a cctpu as the existence of a strategy that decides on both the values of discrete choice variables and the scheduling of controllable time points dynamically. this contrasts with previous work, which made a static assignment of choice variables and dynamic decisions over time points only. we propose an algorithm to find such a fully dynamic strategy. the algorithm computes the ""envelope"" of outcomes of temporal uncertainty in which a particular assignment of discrete variables is feasible, and aggregates these over all choices. when an aggregated envelope covers all uncertain situations of the cctpu, the problem is dynamically controllable. however, the algorithm is complete only under certain assumptions. experiments on an existing set of cctpu benchmarks show that there are cases in which making both discrete and temporal decisions dynamically it is feasible to satisfy the problem constraints while assigning the discrete variables statically it is not.",2019
implicitly coordinated multi-agent path finding under destination uncertainty: success guarantees and computational complexity,"in multi-agent path finding (mapf), it is usually assumed that planning is performed centrally and that the destinations of the agents are common knowledge. we will drop both assumptions and analyze under which conditions it can be guaranteed that the agents reach their respective destinations using implicitly coordinated plans without communication. furthermore, we will analyze what the computational costs associated with such a coordination regime are. as it turns out, guarantees can be given assuming that the agents are of a certain type. however, the implied computational costs are quite severe. in the distributed setting, we either have to solve a sequence of np-complete problems or have to tolerate exponentially longer executions. in the setting with destination uncertainty, bounded plan existence becomes pspace-complete. this clearly demonstrates the value of communicating about plans before execution starts.",2019
ai generality and spearman's law of diminishing returns,"many areas of ai today use benchmarks and competitions with larger and wider sets of tasks. this tries to deter ai systems (and research effort) from specialising to a single task, and encourage them to be prepared to solve previously unseen tasks. it is unclear, however, whether the methods with best performance are actually those that are most general and, in perspective, whether the trend moves towards more general ai systems. this question has a striking similarity with the analysis of the so-called positive manifold and general factors in the area of human intelligence. in this paper, we first show how the existence of a manifold (positive average pairwise task correlation) can also be analysed in ai, and how this relates to the notion of agent generality, from the individual and the populational points of view. from the populational perspective, we analyse the following question: is this manifold correlation higher for the most or for the least able group of agents? we contrast this analysis with one of the most controversial issues in human intelligence research, the so-called spearman's law of diminishing returns (slodr), which basically states that the relevance of a general factor diminishes for most able human groups. we perform two empirical studies on these issues in ai. we analyse the results of the 2015 general video game ai (gvgai) competition, with games as tasks and ""controllers"" as agents, and the results of a synthetic setting, with modified elementary cellular automata (eca) rules as tasks and simple interactive programs as agents. in both cases, we see that slodr doesnot appear. the data, and the use of just two scenarios, does not clearly support the reverse either, a universal law of augmenting returns (uloar), but calls for more experiments on this question.",2019
computing and explaining query answers over inconsistent dl-lite knowledge bases,"several inconsistency-tolerant semantics have been introduced for querying inconsistent description logic knowledge bases. the first contribution of this paper is a practical approach for computing the query answers under three well-known such semantics, namely the ar, iar and brave semantics, in the lightweight description logic dl-liter. we show that query answering under the intractable ar semantics can be performed efficiently by using iar and brave semantics as tractable approximations and encoding the ar entailment problem as a propositional satisfiability (sat) problem. the second issue tackled in this work is explaining why a tuple is a (non-)answer to a query under these semantics. we define explanations for positive and negative answers under the brave, ar and iar semantics. we then study the computational properties of explanations in dl-liter. for each type of explanation, we analyze the data complexity of recognizing (preferred) explanations and deciding if a given assertion is relevant or necessary. we establish tight connections between intractable explanation problems and variants of sat, enabling us to generate explanations by exploiting solvers for boolean satisfaction and optimization problems. finally, we empirically study the efficiency of our query answering and explanation framework using a benchmark we built upon the well-established lubm benchmark.",2019
a survey on transfer learning for multiagent reinforcement learning systems,"multiagent reinforcement learning (rl) solves complex tasks that require coordination with other agents through autonomous exploration of the environment. however, learning a complex task from scratch is impractical due to the huge sample complexity of rl algorithms. for this reason, reusing knowledge that can come from previous experience or other agents is indispensable to scale up multiagent rl algorithms. this survey provides a unifying view of the literature on knowledge reuse in multiagent rl. we define a taxonomy of solutions for the general knowledge reuse problem, providing a comprehensive discussion of recent progress on knowledge reuse in multiagent systems (mas) and of techniques for knowledge reuse across agents (that may be actuating in a shared environment or not). we aim at encouraging the community to work towards reusing all the knowledge sources available in a mas. for that, we provide an in-depth discussion of current lines of research and open questions.",2019
distributed gibbs: a linear-space sampling-based dcop algorithm,"researchers have used distributed constraint optimization problems (dcops) to model various multi-agent coordination and resource allocation problems. very recently, ottens et al. proposed a promising new approach to solve dcops that is based on confidence bounds via their distributed uct (duct) sampling-based algorithm. unfortunately, its memory requirement per agent is exponential in the number of agents in the problem, which prohibits it from scaling up to large problems. thus, in this article, we introduce two new sampling-based dcop algorithms called sequential distributed gibbs (sd-gibbs) and parallel distributed gibbs (pd-gibbs). both algorithms have memory requirements per agent that is linear in the number of agents in the problem. our empirical results show that our algorithms can find solutions that are better than duct, run faster than duct, and solve some large problems that duct failed to solve due to memory limitations.",2019
polynomial and exponential bounded logic programs with function symbols: some new decidable classes,"a logic program with function symbols is called finitely ground if there is a finite propositional logic program whose stable models are exactly the same as the stable models of this program. finite groundability is an important property for logic programs with function symbols because it makes feasible to compute such programs' stable models using traditional asp solvers. in this paper, we introduce new decidable classes of finitely ground programs called poly-bounded and k-exp-bounded programs, which, to the best of our knowledge, strictly contain all other decidable classes of finitely ground programs discovered so far in the literature. we also study the relevant complexity properties for these classes of programs. we prove that the membership complexities for poly-bounded and k-exp-bounded programs are exptime-complete and (k+1)-exptime-complete, respectively.",2019
modeling and planning with macro-actions in decentralized pomdps,"decentralized partially observable markov decision processes (dec-pomdps) are general models for decentralized multi-agent decision making under uncertainty. however, they typically model a problem at a low level of granularity, where each agent's actions are primitive operations lasting exactly one time step. we address the case where each agent has macro-actions: temporally extended actions that may require different amounts of time to execute. we model macro-actions as options in a dec-pomdp, focusing on actions that depend only on information directly available to the agent during execution. therefore, we model systems where coordination decisions only occur at the level of deciding which macro-actions to execute. the core technical difficulty in this setting is that the options chosen by each agent no longer terminate at the same time. we extend three leading dec-pomdp algorithms for policy generation to the macro-action case, and demonstrate their effectiveness in both standard benchmarks and a multi-robot coordination problem. the results show that our new algorithms retain agent coordination while allowing high-quality solutions to be generated for significantly longer horizons and larger state-spaces than previous dec-pomdp methods. furthermore, in the multi-robot domain, we show that, in contrast to most existing methods that are specialized to a particular problem class, our approach can synthesize control policies that exploit opportunities for coordination while balancing uncertainty, sensor information, and information about other agents.",2019
pitfalls and best practices in algorithm configuration,"good parameter settings are crucial to achieve high performance in many areas of artificial intelligence (ai), such as propositional satisfiability solving, ai planning, scheduling, and machine learning (in particular deep learning). automated algorithm configuration methods have recently received much attention in the ai community since they replace tedious, irreproducible and error-prone manual parameter tuning and can lead to new state-of-the-art performance. however, practical applications of algorithm configuration are prone to several (often subtle) pitfalls in the experimental design that can render the procedure ineffective. we identify several common issues and propose best practices for avoiding them. as one possibility for automatically handling as many of these as possible, we also propose a tool called genericwrapper4ac.",2019
negotiable votes,"we study voting games on binary issues, where voters hold an objective over the outcome of the collective decision and are allowed, before the vote takes place, to negotiate their ballots with the other participants. we analyse the voters' rational behaviour in the resulting two-phase game when ballots are aggregated via non-manipulable rules and, more specifically, quota rules. we show under what conditions undesirable equilibria can be removed and desirable ones sustained as a consequence of the pre-vote phase.",2019
conditional simple temporal networks with uncertainty and resources,"conditional simple temporal networks with uncertainty (cstnus) allow for the representation of temporal plans subject to both conditional constraints and uncertain durations. dynamic controllability (dc) of cstnus ensures the existence of an execution strategy able to execute the network in real time (i.e., scheduling the time points under control) depending on how these two uncontrollable parts behave. however, cstnus do not deal with resources. in this paper, we define conditional simple temporal networks with uncertainty and resources (cstnurs) by injecting resources and runtime resource constraints (rrcs) into the specification. resources are mandatory for executing the time points and their availability is represented through temporal expressions, whereas rrcs restrict resource availability by further temporal constraints among resources. we provide a fully-automated encoding to translate any cstnur into an equivalent timed game automaton in polynomial time for a sound and complete dc-checking.",2019
using collective behavior of coupled oscillators for solving dcop,"the distributed constraint optimization problem (dcop) has emerged as one of the most promising coordination techniques in multiagent systems. however, because dcop is known to be np-hard, the existing dcop techniques are often unsuitable for large-scale applications, which require distributed and scalable algorithms to deal with severely limited computing and communication. in this paper, we present a novel approach to provide approximate solutions for large-scale, complex dcops. this approach introduces concepts of synchronization of coupled oscillators for speeding up the convergence process towards high-quality solutions. we propose a new anytime local search dcop algorithm, called coupled oscillator optimization (coopt), which amounts to iteratively solving a dcop by agents exchanging local information that brings them to a consensus. we empirically evaluate coopt on constraint networks involving hundreds of variables with different topologies, domains, and densities. our experimental results demonstrate that coopt outperforms other incomplete state-of-the-art dcop algorithms, especially in terms of the agents' communication cost and solution quality.",2019
human-machine collaborative optimization via apprenticeship scheduling,"coordinating agents to complete a set of tasks with intercoupled temporal and resource constraints is computationally challenging, yet human domain experts can solve these difficult scheduling problems using paradigms learned through years of apprenticeship. a process for manually codifying this domain knowledge within a computational framework is necessary to scale beyond the ""single-expert, single-trainee"" apprenticeship model. however, human domain experts often have difficulty describing their decision-making processes. we propose a new approach for capturing this decision-making process through counterfactual reasoning in pairwise comparisons. our approach is model-free and does not require iterating through the state space. we demonstrate that this approach accurately learns multifaceted heuristics on a synthetic and real world data sets. we also demonstrate that policies learned from human scheduling demonstration via apprenticeship learning can substantially improve the efficiency of schedule optimization. we employ this human-machine collaborative optimization technique on a variant of the weapon-to-target assignment problem. we demonstrate that this technique generates optimal solutions up to 9.5 times faster than a state-of-the-art optimization algorithm.",2018
a core method for the weak completion semantics with skeptical abduction,"the weak completion semantics is a novel cognitive theory which has been successfully applied to the suppression task, the selection task, syllogistic reasoning, the belief bias effect, spatial reasoning as well as reasoning with conditionals. it is based on logic programming with skeptical abduction. each program admits a least model under the three-valued lukasiewicz logic, which can be computed as the least fixed point of an appropriate semantic operator. the semantic operator can be represented by a three-layer feed-forward network using the core method. its least fixed point is the unique stable state of a recursive network which is obtained from the three-layer feed-forward core by mapping the activation of the output layer back to the input layer. the recursive network is embedded into a novel network to compute skeptical abduction. this paper presents a fully connectionist realization of the weak completion semantics.",2018
belief integration and source reliability assessment,"merging beliefs requires the plausibility of the sources of the information to be merged. they are typically assumed equally reliable when nothing suggests otherwise. a recent line of research has spun from the idea of deriving this information from the revision process itself. in particular, the history of previous revisions and previous merging examples provide information for performing subsequent merging operations.yet, no examples or previous revisions may be available. in spite of the apparent lack of information, something can still be inferred by a try-and-check approach: a relative reliability ordering is assumed, the sources are integrated according to it and the result is compared with the original information. the final check may contradict the original ordering, like when the result of merging implies the negation of a formula coming from a source initially assumed reliable, or it implies a formula coming from a source assumed unreliable. in such cases, the reliability ordering assumed in the first place can be excluded from consideration.such a scenario is proved real under the classifications of source reliability and definitions of belief integration considered in this article: sources divided in two, three or multiple reliability classes; integration is mostly by maximal consistent subsets but also weighted distance is considered. other results mainly concern the integration by maximal consistent subsets and partitions of two and three reliability classes.",2018
efficient computation of semivalues for game-theoretic network centrality,"some game-theoretic solution concepts such as the shapley value and the banzhaf index have recently gained popularity as measures of node centrality in networks. while this direction of research is promising, the computational problems that surround it are challenging and have largely been left open. to date there are only a few positive results in the literature, which show that some game-theoretic extensions of degree-, closeness- and betweenness-centrality measures are computable in polynomial time, i.e., without the need to enumerate the exponential number of all possible coalitions. in this article, we show that these results can be extended to a much larger class of centrality measures that are based on a family of solution concepts known as semivalues. the family of semivalues includes, among others, the shapley value and the banzhaf index. to this end, we present a generic framework for defining game-theoretic network centralities and prove that all centrality measures that can be expressed in this framework are computable in polynomial time. using our framework, we present a number of new and polynomial-time computable game-theoretic centrality measures.",2018
query answering with transitive and linear-ordered data,"we consider entailment problems involving powerful constraint languages such as frontier-guarded existential rules in which we impose additional semantic restrictions on a set of distinguished relations. we consider restricting a relation to be transitive, restricting a relation to be the transitive closure of another relation, and restricting a relation to be a linear order. we give some natural variants of guardedness that allow inference to be decidable in each case, and isolate the complexity of the corresponding decision problems. finally we show that slight changes in these conditions lead to undecidability.",2018
revisiting the approximation bound for stochastic submodular cover,"deshpande et al. presented a k(ln r + 1) approximation bound for stochastic submodular cover, where k is the state set size, r is the maximum utility of a single item, and the utility function is integer-valued. this bound is similar to the ln q/(eta+1) bound given by golovin and krause, whose analysis was recently found to have an error. here q &gt;= r is the goal utility and eta is the minimum gap between q and any attainable utility q' &lt; q. we revisit the proof of the k(ln r + 1) bound of deshpande et al., fill in the details of the proof of a key lemma, and prove two bounds for real-valued utility functions: k(ln r_1 + 1) and (ln r_e + 1). here r_1 equals the maximum ratio between the largest increase in utility attainable from a single item, and the smallest non-zero increase attainable from that same item (in the same state). the quantity r_e equals the maximum ratio between the largest expected increase in utility from a single item, and the smallest non-zero expected increase in utility from that same item. our bounds apply only to the stochastic setting with independent states.",2018
watching and acting together: concurrent plan recognition and adaptation for human-robot teams,"there is huge demand for robots to work alongside humans in heterogeneous teams. to achieve a high degree of fluidity, robots must be able to (1) recognize their human co-worker's intent, and (2) adapt to this intent accordingly, providing useful aid as a teammate. the literature to date has made great progress in these two areas -- recognition and adaptation -- but largely as separate research activities. in this work, we present a unified approach to these two problems, in which recognition and adaptation occur concurrently and holistically within the same framework. we introduce pike, an executive for human-robot teams, that allows the robot to continuously and concurrently reason about what a human is doing as execution proceeds, as well as adapt appropriately. the result is a mixed-initiative execution where humans and robots interact fluidly to complete task goals.key to our approach is our task model: a contingent, temporally-flexible team-plan with explicit choices for both the human and robot. this allows a single set of algorithms to find implicit constraints between sets of choices for the human and robot (as determined via causal link analysis and temporal reasoning), narrowing the possible decisions a rational human would take (hence achieving intent recognition) as well as the possible actions a robot could consistently take (hence achieving adaptation). pike makes choices based on the preconditions of actions in the plan, temporal constraints, unanticipated disturbances, and choices made previously (by either agent).innovations of this work include (1) a framework for concurrent intent recognition and adaptation for contingent, temporally-flexible plans, (2) the generalization of causal links for contingent, temporally-flexible plans along with related extraction algorithms, and (3) extensions to a state-of-the-art dynamic execution system to utilize these causal links for decision making.",2018
"cooperative, dynamics-based, and abstraction-guided multi-robot motion planning","this paper presents an effective, cooperative, and probabilistically-complete multi-robot motion planner that enables each robot to move to a desired location while avoiding collisions with obstacles and other robots. the approach takes into account not only the geometric constraints arising from collision avoidance, but also the differential constraints imposed by the motion dynamics of each robot. this makes it possible to generate collision-free and dynamically-feasible trajectories that can be executed in the physical world.the salient aspect of the approach is the coupling of sampling-based motion planning to handle the complexity arising from the obstacles and robot dynamics with multi-agent search to find solutions over a suitable discrete abstraction. the discrete abstraction is obtained by constructing roadmaps to solve a relaxed problem that accounts for the obstacles but not the dynamics. sampling-based motion planning expands a motion tree in the composite state space of all the robots by adding collision-free and dynamically-feasible trajectories as branches. efficiency is obtained by using multi-agent search to find non-conflicting routes over the discrete abstraction which serve as heuristics to guide the motion-tree expansion. when little or no progress is made, the routes are penalized and the multi-agent search is invoked again to find alternative routes. this synergistic coupling makes it possible to effectively plan collision-free and dynamically-feasible motions that enable each robot to reach its goal. experiments using vehicle models with nonlinear dynamics operating in complex environments, where cooperation among robots is required, show significant speedups over related work.",2018
robust text classification under confounding shift,"as statistical classifiers become integrated into real-world applications, it is important to consider not only their accuracy but also their robustness to changes in the data distribution. although identifying and controlling for confounding variables z - correlated with both the input x of a classifier and its output y - has been assiduously studied in empirical social science, it is often neglected in text classification. this can be understood by the fact that, if we assume that the impact of confounding variables does not change between the time we fit a model and the time we use it, then prediction accuracy should only be slightly affected. we show in this paper that this assumption often does not hold and that when the influence of a confounding variable changes from training time to prediction time (i.e. under confounding shift), the classifier accuracy can degrade rapidly. we use pearl's back-door adjustment as a predictive framework to develop a model robust to confounding shift under the condition that z is observed at training time. our approach does not make any causal conclusions but by experimenting on 6 datasets, we show that our approach is able to outperform baselines 1) in controlled cases where confounding shift is manually injected between fitting time and prediction time 2) in natural experiments where confounding shift appears either abruptly or gradually 3) in cases where there is one or multiple confounders. finally, we discuss multiple issues we encountered during this research such as the effect of noise in the observation of z and the importance of only controlling for confounding variables.",2018
graphical model market maker for combinatorial prediction markets,"we describe algorithms for use by prediction markets in forming a crowd consensus joint probability distribution over thousands of related events. equivalently, we describe market mechanisms to efficiently crowdsource both structure and parameters of a bayesian network. prediction markets are among the most accurate methods to combine forecasts; forecasters form a consensus probability distribution by trading contingent securities. a combinatorial prediction market forms a consensus joint distribution over many related events by allowing conditional trades or trades on boolean combinations of events. explicitly representing the joint distribution is infeasible, but standard inference algorithms for graphical probability models render it tractable for large numbers of base events. we show how to adapt these algorithms to compute expected assets conditional on a prospective trade, and to find the conditional state where a trader has minimum assets, allowing full asset reuse. we compare the performance of three algorithms: the straightforward algorithm from the daggre (decomposition-based aggregation) prediction market for geopolitical events, the simple block-merge model from the scicast market for science and technology forecasting, and a more sophisticated algorithm we developed for future markets.",2018
proximal gradient temporal difference learning: stable reinforcement learning with polynomial sample complexity,"in this paper, we introduce proximal gradient temporal difference learning, which provides a principled way of designing and analyzing true stochastic gradient temporal difference learning algorithms. we show how gradient td (gtd) reinforcement learning methods can be formally derived, not by starting from their original objective functions, as previously attempted, but rather from a primal-dual saddle-point objective function. we also conduct a saddle-point error analysis to obtain finite-sample bounds on their performance. previous analyses of this class of algorithms use stochastic approximation techniques to prove asymptotic convergence, and do not provide any finite-sample analysis. we also propose an accelerated algorithm, called gtd2-mp, that uses proximal ""mirror maps"" to yield an improved convergence rate. the results of our theoretical analysis imply that the gtd family of algorithms are comparable and may indeed be preferred over existing least squares td methods for off-policy learning, due to their linear complexity. we provide experimental results showing the improved performance of our accelerated gradient td methods.",2018
approximation and parameterized complexity of minimax approval voting,"we present three results on the complexity of minimax approval voting. first, we study minimax approval voting parameterized by the hamming distance d from the solution to the votes. we show minimax approval voting admits no algorithm running in time o*(2o(d log d)), unless the exponential time hypothesis (eth) fails. this means that the o*(d2d) algorithm of misra, nabeel and singh is essentially optimal. motivated by this, we then show a parameterized approximation scheme, running in time o*((3/ε)2d), which is essentially tight assuming eth. finally, we get a new polynomial-time randomized approximation scheme for minimax approval voting, which runs in time no(1/ε2⋅log(1/ε))⋅poly(m), where n is a number of voters and m is a number of alternatives. it almost matches the running time of the fastest known ptas for closest string due to ma and sun.",2018
a complexity approach for core-selecting exchange under conditionally lexicographic preferences,"core-selection is a crucial property of rules in the literature of resource allocation. it is also desirable, from the perspective of mechanism design, to address the incentive of agents to cheat by misreporting their preferences. this paper investigates the exchange problem where (i) each agent is initially endowed with (possibly multiple) indivisible goods, (ii) agents' preferences are assumed to be conditionally lexicographic, and (iii) side payments are prohibited. we propose an exchange rule called augmented top-trading-cycles (attc), based on the original ttc procedure. we first show that attc is core-selecting and runs in polynomial time with respect to the number of goods. we then show that finding a beneficial misreport under attc is np-hard. we finally clarify relationship of misreporting with splitting and hiding, two different types of manipulations, under attc.",2018
ltl on finite and process traces: complexity results and a practical reasoner,"linear temporal logic (ltl) is a modal logic where formulas are built over temporal operators relating events happening in different time instants. according to the standard semantics, ltl formulas are interpreted on traces spanning over an infinite timeline. however, applications related to the specification and verification of business processes have recently pointed out the need for defining and reasoning about a variant of ltl, which we name ltlp, whose semantics is defined over process traces, that is, over finite traces such that, at each time instant, precisely one propositional variable (standing for the execution of some given activity) evaluates true. the paper investigates the theoretical underpinnings of ltlp and of a related logic formalism, named ltlf, which had already attracted attention in the literature and where formulas have the same syntax as in ltlp and are evaluated over finite traces, but without any constraint on the number of variables simultaneously evaluating true. the two formalisms are comparatively analyzed, by pointing out similarities and differences. in addition, a thorough complexity analysis has been conducted for reasoning problems about ltlp and ltlf, by considering arbitrary formulas as well as classes of formulas defined in terms of restrictions on the temporal operators that are allowed. finally, based on the theoretical findings of the paper, a practical reasoner specifically tailored for ltlp and ltlf has been developed by leveraging state-of-the-art sat solvers. the behavior of the reasoner has been experimentally compared with other systems available in the literature.",2018
consequence-based reasoning for description logics with disjunctions and number restrictions,"classification of description logic (dl) ontologies is a key computational problem in modern data management applications, so considerable effort has been devoted to the development and optimisation of practical reasoning calculi. consequence-based calculi combine ideas from hypertableau and resolution in a way that has proved very effective in practice. however, existing consequence-based calculi can handle either horn dls (which do not support disjunction) or dls without number restrictions. in this paper, we overcome this important limitation and present the first consequence-based calculus for deciding concept subsumption in the dl alchiq+. our calculus runs in exponential time assuming unary coding of numbers, and on elh ontologies it runs in polynomial time. the extension to disjunctions and number restrictions is technically involved: we capture the relevant consequences using first-order clauses, and our inference rules adapt paramodulation techniques from first-order theorem proving. by using a well-known preprocessing step, the calculus can also decide concept subsumptions in sriq---a rich dl that covers all features of owl 2 dl apart from nominals and datatypes. we have implemented our calculus in a new reasoner called sequoia. we present the architecture of our reasoner and discuss several novel and important implementation techniques such as clause indexing and redundancy elimination. finally, we present the results of an extensive performance evaluation, which revealed sequoia to be competitive with existing reasoners. thus, the calculus and the techniques we present in this paper provide an important addition to the repertoire of practical implementation techniques for description logic reasoning.",2018
data-driven conceptual spaces: creating semantic representations for linguistic descriptions of numerical data,"there is an increasing need to derive semantics from real-world observations to facilitate natural information sharing between machine and human. conceptual spaces theory is a possible approach and has been proposed as mid-level representation between symbolic and sub-symbolic representations, whereby concepts are represented in a geometrical space that is characterised by a number of quality dimensions. currently, much of the work has demonstrated how conceptual spaces are created in a knowledge-driven manner, relying on prior knowledge to form concepts and identify quality dimensions. this paper presents a method to create semantic representations using data-driven conceptual spaces which are then used to derive linguistic descriptions of numerical data. our contribution is a principled approach to automatically construct a conceptual space from a set of known observations wherein the quality dimensions and domains are not known a priori. this novelty of the approach is the ability to select and group semantic features to discriminate between concepts in a data-driven manner while preserving the semantic interpretation that is needed to infer linguistic descriptions for interaction with humans. two data sets representing leaf images and time series signals are used to evaluate the method. an empirical evaluation for each case study assesses how well linguistic descriptions generated from the conceptual spaces identify unknown observations. furthermore, comparisons are made with descriptions derived on alternative approaches for generating semantic models.",2018
from word to sense embeddings: a survey on vector representations of meaning,"over the past years, distributed semantic representations have proved to be effective and flexible keepers of prior knowledge to be integrated into downstream applications. this survey focuses on the representation of meaning. we start from the theoretical background behind word vector space models and highlight one of their major limitations: the meaning conflation deficiency, which arises from representing a word with all its possible meanings as a single vector. then, we explain how this deficiency can be addressed through a transition from the word level to the more fine-grained level of word senses (in its broader acceptation) as a method for modelling unambiguous lexical meaning. we present a comprehensive overview of the wide range of techniques in the two main branches of sense representation, i.e., unsupervised and knowledge-based. finally, this survey covers the main evaluation procedures and applications for this type of representation, and provides an analysis of four of its important aspects: interpretability, sense granularity, adaptability to different domains and compositionality.",2018
state-space abstractions for probabilistic inference: a systematic review,"tasks such as social network analysis, human behavior recognition, or modeling biochemical reactions, can be solved elegantly by using the probabilistic inference framework. however, standard probabilistic inference algorithms work at a propositional level, and thus cannot capture the symmetries and redundancies that are present in these tasks. algorithms that exploit those symmetries have been devised in different research fields, for example by the lifted inference-, multiple object tracking-, and modeling and simulation-communities. the common idea, that we call state space abstraction, is to perform inference over compact representations of sets of symmetric states. although they are concerned with a similar topic, the relationship between these approaches has not been investigated systematically. this survey provides the following contributions. we perform a systematic literature review to outline the state of the art in probabilistic inference methods exploiting symmetries. from an initial set of more than 4,000 papers, we identify 116 relevant papers. furthermore, we provide new high-level categories that classify the approaches, based on common properties of the approaches. the research areas underlying each of the categories are introduced concisely. researchers from different fields that are confronted with a state space explosion problem in a probabilistic system can use this classification to identify possible solutions. finally, based on this conceptualization, we identify potentials for future research, as some relevant application domains are not addressed by current approaches.",2018
grounding language for transfer in deep reinforcement learning,"in this paper, we explore the utilization of natural language to drive transfer for reinforcement learning (rl). despite the wide-spread application of deep rl techniques, learning generalized policy representations that work across domains remains a challenging problem. we demonstrate that textual descriptions of environments provide a compact intermediate channel to facilitate effective policy transfer. specifically, by learning to ground the meaning of text to the dynamics of the environment such as transitions and rewards, an autonomous agent can effectively bootstrap policy learning on a new domain given its description. we employ a model-based rl approach consisting of a differentiable planning module, a model-free component and a factorized state representation to effectively use entity descriptions. our model outperforms prior work on both transfer and multi-task scenarios in a variety of different environments. for instance, we achieve up to 14% and 11.5% absolute improvement over previously existing models in terms of average and initial rewards, respectively.",2018
and/or search for marginal map,"mixed inference such as the marginal map query (some variables marginalized by summation and others by maximization) is key to many prediction and decision models. it is known to be extremely hard; the problem is nppp-complete while the decision problem for map is only np-complete and the summation problem is #p-complete. consequently, approximation anytime schemes are essential. in this paper, we show that the framework of heuristic and/or search, which exploits conditional independence in the graphical model, coupled with variational-based mini-bucket heuristics can be extended to this task and yield powerful state-of-the-art schemes. specifically, we explore the complementary properties of best-first search for reducing the number of conditional sums and providing time-improving upper bounds, with depth-first search for rapidly generating and improving solutions and lower bounds. we show empirically that a class of solvers that interleaves depth-first with best-first schemes emerges as the most competitive anytime scheme.",2018
transition-based neural word segmentation using word-level features,"character-based and word-based methods are two different solutions for chinese word segmentation, the former exploiting sequence labeling models over characters and the latter using word-level features. neural models have been exploited for character-based chinese word segmentation, giving high accuracies by making use of external character embeddings, yet requiring less feature engineering. in this paper, we study a neural model for word-based chinese word segmentation, by replacing the manually-designed discrete features with neural features in a transition-based word segmentation framework. experimental results demonstrate that word features lead to comparable performance to the best systems in the literature, and a further combination of discrete and neural features obtains top accuracies on several benchmarks.",2018
optimal torpedo scheduling,"we consider the torpedo scheduling problem in steel production, which is concerned with the transport of hot metal from a blast furnace to an oxygen converter. a schedule must satisfy, amongst other considerations, resource capacity constraints along the path and the locations traversed as well as the sulfur level of the hot metal. the goal is first to minimize the number of torpedo cars used during the planning horizon and second to minimize the time spent desulfurizing the hot metal. we propose an exact solution method based on logic based benders decomposition using mixed-integer and constraint programming, which optimally solves and proves, for the first time, the optimality of all instances from the acp challenge 2016 within 10 minutes. in addition, we adapted our method to handle large-scale instances and instances with a more general rail network. this adaptation optimally solved all challenge instances within one minute and was able to solve instances of up to 100,000 hot metal pickups.",2018
bounds on the cost of stabilizing a cooperative game,"a key issue in cooperative game theory is coalitional stability, usually captured by the notion of the core---the set of outcomes that are resistant to group deviations. however, some coalitional games have empty cores, and any outcome in such a game is unstable. we investigate the possibility of stabilizing a coalitional game by using subsidies. we consider scenarios where an external party that is interested in having the players work together offers a supplemental payment to the grand coalition, or, more generally, a particular coalition structure. this payment is conditional on players not deviating from this coalition structure, and may be divided among the players in any way they wish. we define the cost of stability as the minimum external payment that stabilizes the game. we provide tight bounds on the cost of stability, both for games where the coalitional values are nonnegative (profit-sharing games) and for games where the coalitional values are nonpositive (cost-sharing games), under natural assumptions on the characteristic function, such as superadditivity, anonymity, or both. we also investigate the relationship between the cost of stability and several variants of the least core. finally, we study the computational complexity of problems related to the cost of stability, with a focus on weighted voting games.",2018
an exhaustive dpll algorithm for model counting,"state-of-the-art model counters are based on exhaustive dpll algorithms, and have been successfully used in probabilistic reasoning, one of the key problems in ai. in this article, we present a new exhaustive dpll algorithm with a formal semantics, a proof of correctness, and a modular design. the modular design is based on the separation of the core model counting algorithm from sat solving techniques. we also show that the trace of our algorithm belongs to the language of sentential decision diagrams (sdds), which is a subset of decision-dnnfs, the trace of existing state-of-the-art model counters. still, our experimental analysis shows comparable results against state-of-the-art model counters. furthermore, we obtain the first top-down sdd compiler, and show orders-of-magnitude improvements in sdd construction time against the existing bottom-up sdd compiler.",2018
axiomatic characterization of game-theoretic centrality,"one of the fundamental research challenges in network science is centrality analysis, i.e., identifying the nodes that play the most important roles in the network. in this article, we focus on the game-theoretic approach to centrality analysis. while various centrality indices have been recently proposed based on this approach, it is still unknown how general is the game-theoretic approach to centrality and what distinguishes some game-theoretic centralities from others. in this article, we attempt to answer this question by providing the first axiomatic characterization of game-theoretic centralities. specifically, we show that every possible centrality measure can be obtained following the game-theoretic approach. furthermore, we study three natural classes of game-theoretic centrality, and prove that they can be characterized by certain intuitive properties pertaining to the well-known notion of fairness due to myerson.",2018
lifted relational neural networks: efficient learning of latent relational structures,"we propose a method to combine the interpretability and expressive power of firstorder logic with the effectiveness of neural network learning. in particular, we introduce a lifted framework in which first-order rules are used to describe the structure of a given problem setting. these rules are then used as a template for constructing a number of neural networks, one for each training and testing example. as the different networks corresponding to different examples share their weights, these weights can be efficiently learned using stochastic gradient descent. our framework provides a flexible way for implementing and combining a wide variety of modelling constructs. in particular, the use of first-order logic allows for a declarative specification of latent relational structures, which can then be efficiently discovered in a given data set using neural network learning. experiments on 78 relational learning benchmarks clearly demonstrate the effectiveness of the framework.",2018
verification of distributed epistemic gossip protocols,"gossip protocols aim at arriving, by means of point-to-point or group communications, at a situation in which all the agents know each other secrets. distributed epistemic gossip protocols use as guards formulas from a simple epistemic logic and as statements calls between the agents. they are natural examples of knowledge based programs.we prove here that these protocols are implementable, that their partial correctness is decidable and that termination and two forms of fair termination of these protocols are decidable, as well. to establish these results we show that the definition of semantics and of truth of the underlying logic are decidable.",2018
a cop model for graph-constrained coalition formation,"we consider graph-constrained coalition formation (gccf), a widely studied subproblem of coalition formation in which the set of valid coalitions is restricted by a graph. we propose cop-gccf, a novel approach that models gccf as a cop, and we solve such cop with a highly-parallel approach based on bucket elimination executed on the gpu, which is able to exploit the high constraint tightness of cop-gccf. results show that our approach outperforms state of the art algorithms (i.e., dyce and idpg) by at least one order of magnitude on realistic graphs, i.e., a crawl of the twitter social graph, both in terms of runtime and memory.",2018
resource-bounded norm monitoring in multi-agent systems,"norms allow system designers to specify the desired behaviour of a sociotechnical system. in this way, norms regulate what the social and technical agents in a sociotechnical system should (not) do. in this context, a vitally important question is the development of mechanisms for monitoring whether these agents comply with norms. proposals on norm monitoring often assume that monitoring has no costs and/or that monitors have unlimited resources to observe the environment and the actions performed by agents. in this paper, we challenge this assumption and propose the first practical resource-bounded norm monitor. our monitor is capable of selecting the resources to be deployed and use them to check norm compliance with incomplete information about the actions performed and the state of the world. we formally demonstrate the correctness and soundness of our norm monitor and study its complexity. we also demonstrate in randomised simulations and benchmark experiments that our monitor can select monitored resources effectively and efficiently, detecting more norm violations and fulfilments than other tractable optimization approaches and obtaining slightly worse results than intractable optimal approaches.",2018
mcts-minimax hybrids with state evaluations,"monte-carlo tree search (mcts) has been found to show weaker play than minimax-based search in some tactical game domains. this is partly due to its highly selective search and averaging value backups, which make it susceptible to traps. in order to combine the strategic strength of mcts and the tactical strength of minimax, mcts-minimax hybrids have been introduced, embedding shallow minimax searches into the mcts framework. their results have been promising even without making use of domain knowledge such as heuristic evaluation functions. this article continues this line of research for the case where evaluation functions are available. three different approaches are considered, employing minimax with an evaluation function in the rollout phase of mcts, as a replacement for the rollout phase, and as a node prior to bias move selection. the latter two approaches are newly proposed. furthermore, all three hybrids are enhanced with the help of move ordering and k-best pruning for minimax. results show that the use of enhanced minimax for computing node priors results in the strongest mcts-minimax hybrid investigated in the three test domains of othello, breakthrough, and catch the lion. this hybrid, called mcts-ip-m-k, also outperforms enhanced minimax as a standalone player in breakthrough, demonstrating that at least in this domain, mcts and minimax can be combined to an algorithm stronger than its parts. using enhanced minimax for computing node priors is therefore a promising new technique for integrating domain knowledge into an mcts framework.",2018
solving large problems with heuristic search: general-purpose parallel external-memory search,"classic best-first heuristic search algorithms, like a*, record every unique state they encounter in ram, making them infeasible for solving large problems. in this paper, we demonstrate how best-first search can be scaled to solve much larger problems by exploiting disk storage and parallel processing and, in some cases, slightly relaxing the strict best-first node expansion order. some previous disk-based search algorithms abandon best-first search order in an attempt to increase efficiency. we present two case studies showing that a*, when augmented with delayed duplicate detection, can actually be more efficient than these non-best-first search orders. first, we present a straightforward external variant of a*, called pedal, that slightly relaxes best-first order in order to be i/o efficient in both theory and practice, even on problems featuring real-valued node costs. because it is easy to parallelize, pedal can be faster than in-memory ida* even on domains with few duplicate states, such as the sliding-tile puzzle. second, we present a variant of pedal, called pe2a*, that uses partial expansion to handle problems that have large branching factors. when tested on the problem of multiple sequence alignment, pe2a* is the first algorithm capable of solving the entire reference set 1 of the standard balibase benchmark using a biologically accurate cost function. this work shows that classic best-first algorithms like a* can be applied to large real-world problems. we also provide a detailed implementation guide with source code both for generic parallel disk-based best-first search and for multiple sequence alignment with a biologically accurate cost function. given its effectiveness as a general-purpose problem-solving method, we hope that this makes parallel and disk-based search accessible to a wider audience.",2018
viewpoint: artificial intelligence government (gov. 3.0): the uae leading model,"the united arab emirates (uae) is the first country in the world to appoint a state minister for artificial intelligence (ai). the uae is embracing ai in society at the governmental level, which is leading to a new generations of digital government (which we are labeling gov. 3.0). this paper argues that the decision to embrace ai will lead to positive impacts on society, including businesses, organizations and individuals, as well as on the ai industry itself. this paper discusses the societal impacts of ai at a macro (country-wide) level. this article is part of the special track on ai and society.",2018
solving multi-agent path finding on strongly biconnected digraphs,"much of the literature on suboptimal, polynomial-time algorithms for multi-agent path finding focuses on undirected graphs, where motion is permitted in both directions along a graph edge. despite this, traveling on directed graphs is relevant in navigation domains, such as path finding in games, and asymmetric communication networks.we consider multi-agent path finding on strongly biconnected directed graphs. we show that all instances with at least two unoccupied positions have a solution, except for a particular, degenerate subclass where the graph has a cyclic shape. we present dibox, an algorithm for multi-agent path finding on strongly biconnected directed graphs. dibox runs in polynomial time, computes suboptimal solutions and is complete for instances on strongly biconnected digraphs with at least two unoccupied positions. we theoretically analyze properties of the algorithm and properties of strongly biconnected directed graphs that are relevant to our approach. we perform a detailed empirical analysis of dibox, showing a good scalability. to our knowledge, our work is the first study of multi-agent path finding focused on directed graphs.",2018
"nash stable outcomes in fractional hedonic games: existence, efficiency and computation","we consider fractional hedonic games, a subclass of coalition formation games that can be succinctly modeled by means of a graph in which nodes represent agents and edge weights the degree of preference of the corresponding endpoints. the happiness or utility of an agent for being in a coalition is the average value she ascribes to its members. we adopt nash stable outcomes as the target solution concept; that is we focus on states in which no agent can improve her utility by unilaterally changing her own group. we provide existence, efficiency and complexity results for games played on both general and specific graph topologies. as to the efficiency results, we mainly study the quality of the best nash stable outcome and refer to the ratio between the social welfare of an optimal coalition structure and the one of such an equilibrium as to the price of stability. in this respect, we remark that a best nash stable outcome has a natural meaning of stability, since it is the optimal solution among the ones which can be accepted by selfish agents. we provide upper and lower bounds on the price of stability for different topologies, both in case of weighted and unweighted edges. beside the results for general graphs, we give refined bounds for various specific cases, such as triangle-free, bipartite graphs and tree graphs. for these families, we also show how to efficiently compute nash stable outcomes with provable good social welfare.",2018
extending classical planning with state constraints: heuristics and search for optimal planning,"we present a principled way of extending a classical ai planning formalism with systems of state constraints, which relate - sometimes determine - the values of variables in each state traversed by the plan. this extension occupies an attractive middle ground between expressivity and complexity. it enables modelling a new range of problems, as well as formulating more efficient models of classical planning problems. an example of the former is planning-based control of networked physical systems - power networks, for example - in which a local, discrete control action can have global effects on continuous quantities, such as altering flows across the entire network. at the same time, our extension remains decidable as long as the satisfiability of sets of state constraints is decidable, including in the presence of numeric state variables, and we demonstrate that effective techniques for cost-optimal planning known in the classical setting - in particular, relaxation-based admissible heuristics - can be adapted to the extended formalism. in this paper, we apply our approach to constraints in the form of linear or non-linear equations over numeric state variables, but the approach is independent of the type of state constraints, as long as there exists a procedure that decides their consistency. the planner and the constraint solver interact through a well-defined, narrow interface, in which the solver requires no specialisation to the planning context.",2018
incentive-compatible mechanisms for norm monitoring in open multi-agent systems,"we consider the problem of detecting norm violations in open multi-agent systems (mas). we show how, using ideas from scrip systems, we can design mechanisms where the agents comprising the mas are incentivised to monitor the actions of other agents for norm violations. the cost of providing the incentives is not borne by the mas and does not come from fines charged for norm violations (fines may be impossible to levy in a system where agents are free to leave and rejoin again under a different identity). instead, monitoring incentives come from (scrip) fees for accessing the services provided by the mas. in some cases, perfect monitoring (and hence enforcement) can be achieved: no norms will be violated in equilibrium. in other cases, we show that, while it is impossible to achieve perfect enforcement, we can get arbitrarily close; we can make the probability of a norm violation in equilibrium arbitrarily small. we show using simulations that our theoretical results, which apply to systems with a large number of agents, hold for multi-agent systems with as few as 1000 agents–the system rapidly converges to the steady-state distribution of scrip tokens necessary to ensure monitoring and then remains close to the steady state.",2018
the power of verification for greedy mechanism design,"greedy algorithms are known to provide, in polynomial time, near optimal approximation guarantees for combinatorial auctions (cas) with multidimensional bidders. it is known that truthful greedy-like mechanisms for cas with multi-minded bidders do not achieve good approximation guarantees. in this work, we seek a deeper understanding of greedy mechanism design and investigate under which general assumptions, we can have efficient and truthful greedy mechanisms for cas. towards this goal, we use the framework of priority algorithms and weak and strong verification, where the bidders are not allowed to overbid on their winning set or on any subset of this set, respectively. we provide a complete characterization of the power of weak verification showing that it is sufficient and necessary for any greedy fixed priority algorithm to become truthful with the use of money or not, depending on the ordering of the bids. moreover, we show that strong verification is sufficient and necessary to obtain a 2-approximate truthful mechanism with money, based on a known greedy algorithm, for the problem of submodular cas in finite bidding domains. our proof is based on an interesting structural analysis of the strongly connected components of the declaration graph.",2018
column generation algorithms for constrained pomdps,"in several real-world domains it is required to plan ahead while there are finite resources available for executing the plan. the limited availability of resources imposes constraints on the plans that can be executed, which need to be taken into account while computing a plan. a constrained partially observable markov decision process (constrained pomdp) can be used to model resource-constrained planning problems which include uncertainty and partial observability. constrained pomdps provide a framework for computing policies which maximize expected reward, while respecting constraints on a secondary objective such as cost or resource consumption. column generation for linear programming can be used to obtain constrained pomdp solutions. this method incrementally adds columns to a linear program, in which each column corresponds to a pomdp policy obtained by solving an unconstrained subproblem. column generation requires solving a potentially large number of pomdps, as well as exact evaluation of the resulting policies, which is computationally difficult. we propose a method to solve subproblems in a two-stage fashion using approximation algorithms. first, we use a tailored point-based pomdp algorithm to obtain an approximate subproblem solution. next, we convert this approximate solution into a policy graph, which we can evaluate efficiently. the resulting algorithm is a new approximate method for constrained pomdps in single-agent settings, but also in settings in which multiple independent agents share a global constraint. experiments based on several domains show that our method outperforms the current state of the art.",2018
counterexample-guided cartesian abstraction refinement for classical planning,"counterexample-guided abstraction refinement (cegar) is a method for incrementally computing abstractions of transition systems. we propose a cegar algorithm for computing abstraction heuristics for optimal classical planning. starting from a coarse abstraction of the planning task, we iteratively compute an optimal abstract solution, check if and why it fails for the concrete planning task and refine the abstraction so that the same failure cannot occur in future iterations. a key ingredient of our approach is a novel class of abstractions for classical planning tasks that admits efficient and very fine-grained refinement. since a single abstraction usually cannot capture enough details of the planning task, we also introduce two methods for producing diverse sets of heuristics within this framework, one based on goal atoms, the other based on landmarks. in order to sum their heuristic estimates admissibly we introduce a new cost partitioning algorithm called saturated cost partitioning. we show that the resulting heuristics outperform other state-of-the-art abstraction heuristics in many benchmark domains.",2018
scottyactivity: mixed discrete-continuous planning with convex optimization,"the state of the art practice in robotics planning is to script behaviors manually, where each behavior is typically generated using trajectory optimization. however, in order for robots to be able to act robustly and adapt to novel situations, they need to plan these activity sequences autonomously. since the conditions and effects of these behaviors are tightly coupled through time, state and control variables, many problems require that the tasks of activity planning and trajectory optimization are considered together. there are two key issues underlying effective hybrid activity and trajectory planning: the sufficiently accurate modeling of robot dynamics and the capability of planning over long horizons. hybrid activity and trajectory planners that employ mixed integer programming within a discrete time formulation are able to accurately model complex dynamics for robot vehicles, but are often restricted to relatively short horizons. on the other hand, current hybrid activity planners that employ continuous time formulations can handle longer horizons but they only allow actions to have continuous effects with constant rate of change, and restrict the allowed state constraints to linear inequalities. this is insufficient for many robotic applications and it greatly limits the expressivity of the problems that these approaches can solve. in this work we present the scottyactivity planner, that is able to generate practical hybrid activity and motion plans over long horizons by employing recent methods in convex optimization combined with methods for planning with relaxed plan graphs and heuristic forward search. unlike other continuous time planners, scottyactivity can solve a broad class of robotic planning problems by supporting convex quadratic constraints on state variables and control variables that are jointly constrained and that affect multiple state variables simultaneously. in order to support planning over long horizons, scottyactivity does not resort to time, state or control variable discretization. while straightforward formulations of consistency checks are not convex and do not scale, we present an efficient convex formulation, in the form of a second order cone program (socp), that is very fast to solve. we also introduce several new realistic domains that demonstrate the capabilities and scalability of our approach, and their simplified linear versions, that we use to compare with other state of the art planners. this work demonstrates the power of integrating advanced convex optimization techniques with discrete search methods and paves the way for extensions dealing with non-convex disjoint constraints, such as obstacle avoidance.",2018
exploiting partial assignments for efficient evaluation of answer set programs with external source access,"answer set programming (asp) is a well-known declarative problem solving approach based on nonmonotonic logic programs, which has been successfully applied to a wide range of applications in artificial intelligence and beyond. to address the needs of modern applications, hex-programs were introduced as an extension of asp with external atoms for accessing information outside programs via an api style bi-directional interface mechanism. to evaluate such programs, conflict-driving learning algorithms for sat and asp solving have been extended in order to capture the semantics of external atoms. however, a drawback of the state-of-the-art approach is that external atoms are only evaluated under complete assignments (i.e., input to the external source) while in practice, their values often can be determined already based on partial assignments alone (i.e., from incomplete input to the external source). this prevents early backtracking in case of conflicts, and hinders more efficient evaluation of hex-programs. we thus extend the notion of external atoms to allow for three-valued evaluation under partial assignments, while the two-valued semantics of the overall hex-formalism remains unchanged. this paves the way for three enhancements: first, to evaluate external sources at any point during model search, which can trigger learning knowledge about the source behavior and/or early backtracking in the spirit of theory propagation in sat modulo theories (smt). second, to optimize the knowledge learned in terms of so-called nogoods, which roughly speaking are impossible input-output configurations. shrinking nogoods to their relevant input part leads to more effective search space pruning. and third, to make a necessary minimality check of candidate answer sets more efficient by exploiting early external evaluation calls. as this check usually accounts for a large share of the total runtime, optimization is here particularly important. we further present an experimental evaluation of an implementation of a novel hex-algorithm that incorporates these enhancements using a benchmark suite. our results demonstrate a clear efficiency gain over the state-of-the-art hex-solver for the benchmarks, and provide insights regarding the most effective combinations of solver configurations.",2018
viewpoint: when will ai exceed human performance? evidence from ai experts,"advances in artificial intelligence (ai) will transform modern life by reshaping transportation, health, science, finance, and the military. to adapt public policy, we need to better anticipate these advances. here we report the results from a large survey of machine learning researchers on their beliefs about progress in ai. researchers predict ai will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). researchers believe there is a 50% chance of ai outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with asian respondents expecting these dates much sooner than north americans. these results will inform discussion amongst researchers and policymakers about anticipating and managing trends in ai. this article is part of the special track on ai and society.",2018
computing hierarchical finite state controllers with classical planning,"finite state controllers (fscs) are an effective way to compactly represent sequential plans. by imposing appropriate conditions on transitions, fscs can also represent generalized plans (plans that solve a range of planning problems from a given domain). in this paper we introduce the concept of hierarchical fscs for planning by allowing controllers to call other controllers. this call mechanism allows hierarchical fscs to represent generalized plans more compactly than individual fscs, to compute controllers in a modular fashion or even more, to compute recursive controllers. the paper introduces a classical planning compilation for computing hierarchical fscs that solve challenging generalized planning tasks. the compilation takes as input a finite set of classical planning problems from a given domain. the output of the compilation is a single classical planning problem whose solution induces: (1) a hierarchical fsc and (2), the corresponding validation of that controller on the input classical planning problems.",2018
a review of inference algorithms for hybrid bayesian networks,"hybrid bayesian networks have received an increasing attention during the last years. the difference with respect to standard bayesian networks is that they can host discrete and continuous variables simultaneously, which extends the applicability of the bayesian network framework in general. however, this extra feature also comes at a cost: inference in these types of models is computationally more challenging and the underlying models and updating procedures may not even support closed-form solutions. in this paper we provide an overview of the main trends and principled approaches for performing inference in hybrid bayesian networks. the methods covered in the paper are organized and discussed according to their methodological basis. we consider how the methods have been extended and adapted to also include (hybrid) dynamic bayesian networks, and we end with an overview of established software systems supporting inference in these types of models.",2018
querying log data with metric temporal logic,"we propose a novel framework for ontology-based access to temporal log data using a datalog extension datalogmtl of the horn fragment of the metric temporal logic mtl. we show that datalogmtl is expspace-complete even with punctual intervals, in which case full mtl is known to be undecidable. we also prove that nonrecursive datalogmtl is pspace-complete for combined complexity and in ac0 for data complexity. we demonstrate by two real-world use cases that nonrecursive datalogmtl programs can express complex temporal concepts from typical user queries and thereby facilitate access to temporal log data. our experiments with siemens turbine data and mesowest weather data show that datalogmtl ontology-mediated queries are efficient and scale on large datasets.",2018
learning explanatory rules from noisy data,"artificial neural networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. as their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data--which is not necessarily easily obtained--that sufficiently approximates the data distribution of the domain we wish to test on. in contrast, logic programming methods such as inductive logic programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. however, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. in this paper, we propose a differentiable inductive logic framework, which can not only solve tasks which traditional ilp systems are suited for, but shows a robustness to noise and error in the training data which ilp cannot cope with. furthermore, as it is trained by backpropagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ilp cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.",2018
"survey of the state of the art in natural language generation: core tasks, applications and evaluation","this paper surveys the current state of the art in natural language generation (nlg), defined as the task of generating text or speech from non-linguistic input. a survey of nlg is timely in view of the changes that the field has undergone over the past two decades, especially in relation to new (usually data-driven) methods, as well as new applications of nlg technology. this survey therefore aims to (a) give an up-to-date synthesis of research on the core tasks in nlg and the architectures adopted in which such tasks are organised; (b) highlight a number of recent research topics that have arisen partly as a result of growing synergies between nlg and other areas of artificial intelligence; (c) draw attention to the challenges in nlg evaluation, relating them to similar challenges faced in other areas of nlp, with an emphasis on different evaluation methods and the relationships between them.",2018
bisimulations on data graphs,"bisimulation provides structural conditions to characterize indistinguishability from an external observer between nodes on labeled graphs. it is a fundamental notion used in many areas, such as verification, graph-structured databases, and constraint satisfaction. however, several current applications use graphs where nodes also contain data (the so called ""data graphs""), and where observers can test for equality or inequality of data values (e.g., asking the attribute 'name' of a node to be different from that of all its neighbors). the present work constitutes a first investigation of ""data aware"" bisimulations on data graphs. we study the problem of computing such bisimulations, based on the observational indistinguishability for xpath ---a language that extends modal logics like pdl with tests for data equality--- with and without transitive closure operators. we show that in general the problem is pspace-complete, but identify several restrictions that yield better complexity bounds (conp, ptime) by controlling suitable parameters of the problem, namely the amount of non-locality allowed, and the class of models considered (graphs, dags, trees). in particular, this analysis yields a hierarchy of tractable fragments.",2018
from skills to symbols: learning symbolic representations for abstract high-level planning,"we consider the problem of constructing abstract representations for planning in high-dimensional, continuous environments. we assume an agent equipped with a collection of high-level actions, and construct representations provably capable of evaluating plans composed of sequences of those actions. we first consider the deterministic planning case, and show that the relevant computation involves set operations performed over sets of states. we define the specific collection of sets that is necessary and sufficient for planning, and use them to construct a grounded abstract symbolic representation that is provably suitable for deterministic planning. the resulting representation can be expressed in pddl, a canonical high-level planning domain language; we construct such a representation for the playroom domain and solve it in milliseconds using an off-the-shelf planner. we then consider probabilistic planning, which we show requires generalizing from sets of states to distributions over states. we identify the specific distributions required for planning, and use them to construct a grounded abstract symbolic representation that correctly estimates the expected reward and probability of success of any plan. in addition, we show that learning the relevant probability distributions corresponds to specific instances of probabilistic density estimation and probabilistic classification. we construct an agent that autonomously learns the correct abstract representation of a computer game domain, and rapidly solves it. finally, we apply these techniques to create a physical robot system that autonomously learns its own symbolic representation of a mobile manipulation task directly from sensorimotor data---point clouds, map locations, and joint angles---and then plans using that representation. together, these results establish a principled link between high-level actions and abstract representations, a concrete theoretical foundation for constructing abstract representations with provable properties, and a practical mechanism for autonomously learning abstract high-level representations.",2018
linear satisfiability preserving assignments,"in this paper, we study several classes of satisfiability preserving assignments to the constraint satisfaction problem (csp). in particular, we consider fixable, autark and satisfying assignments. since it is in general np-hard to find a nontrivial (i.e., nonempty) satisfiability preserving assignment, we introduce linear satisfiability preserving assignments, which are defined by polyhedral cones in an associated vector space. the vector space is obtained by the identification, introduced by kullmann, of assignments with real vectors. we consider arbitrary polyhedral cones, where only restricted classes of cones for autark assignments are considered in the literature. we reveal that cones in certain classes are maximal as a convex subset of the set of the associated vectors, which can be regarded as extensions of kullmann's results for autark assignments of cnfs. as algorithmic results, we present a pseudo-polynomial time algorithm that computes a linear fixable assignment for a given integer linear system, which implies the well known pseudo-polynomial solvability for integer linear systems such as two-variable-per-inequality (tvpi), horn and q-horn systems.",2018
kaboum: knowledge-level action and bounding geometry motion planner,"for robots to solve real world tasks, they often require the ability to reason about both symbolic and geometric knowledge. we present a framework, called kaboum, for integrating knowledge-level task planning and motion planning in a bounding geometry. by representing symbolic information at the knowledge level, we can model incomplete information, sensing actions and information gain; by representing all geometric entities--objects, robots and swept volumes of motions--by sets of convex polyhedra, we can efficiently plan manipulation actions and raise reasoning about geometric predicates, such as collisions, to the symbolic level. at the geometric level, we take advantage of our bounded convex decomposition and swept volume computation with quadratic convergence, and fast collision detection of convex bodies. we evaluate our approach on a wide set of problems using real robots, including tasks with multiple manipulators, sensing and branched plans, and mobile manipulation.",2018
actively estimating crowd annotation consensus,"the rapid growth of storage capacity and processing power has caused machine learning applications to increasingly rely on using immense amounts of labeled data. it has become more important than ever to have fast and inexpensive ways to annotate vast amounts of data. with the emergence of crowdsourcing services, the research direction has gravitated toward putting the wisdom of crowds to better use. unfortunately, spammers and inattentive annotators pose a threat to the quality and trustworthiness of the consensus. thus, high quality consensus estimation from crowd annotated data requires a meticulous choice of the candidate annotator and the sample in need of a new annotation. due to time and budget limitations, it is of utmost importance that this choice is carried out while the annotation collection is in progress. we call this process active crowd-labeling. to this end, we propose an active crowd-labeling approach for actively estimating consensus from continuous-valued crowd annotations. our method is based on annotator models with unknown parameters, and bayesian inference is employed to reach a consensus in the form of ordinal, binary, or continuous values. we introduce ranking functions for choosing the candidate annotator and sample pair for requesting an annotation. in addition, we propose a penalizing method for preventing annotator domination, investigate the explore-exploit trade-off for incorporating new annotators into the system, and study the effects of inducing a stopping criterion based on consensus quality. we also introduce the crowd-labeled head pose annotations datasets. experimental results on the benchmark datasets used in the literature and the head pose annotations datasets suggest that our method provides high-quality consensus by using as few as one fifth of the annotations (~80% cost reduction), thereby providing a budget and time-sensitive solution to the crowd-labeling problem.",2018
cycles and intractability in a large class of aggregation rules,"we introduce the (j,k)-kemeny rule -- a generalization of kemeny's voting rule that aggregates j-chotomous weak orders into a k-chotomous weak order. special cases of (j,k)-kemeny include approval voting, the mean rule and borda mean rule, as well as the borda count and plurality voting. why, then, is the winner problem computationally tractable for each of these other rules, but intractable for kemeny? we show that intractability of winner determination for the (j,k)-kemeny rule first appears at the j=3, k=3 level. the proof rests on a reduction of max cut to a related problem on weighted tournaments, and reveals that computational complexity arises from the cyclic part in the fundamental decomposition of a weighted tournament into cyclic and cocyclic components. thus the existence of majority cycles -- the engine driving both arrow's impossibility theorem and the gibbard-satterthwaite theorem -- also serves as a source of computational complexity in social choice.",2018
coordinating measurements in uncertain participatory sensing settings,"environmental monitoring allows authorities to understand the impact of potentially harmful phenomena, such as air pollution, excessive noise, and radiation. recently, there has been considerable interest in participatory sensing as a paradigm for such large-scale data collection because it is cost-effective and able to capture more fine-grained data than traditional approaches that use stationary sensors scattered in cities. in this approach, ordinary citizens (non-expert contributors) collect environmental data using low-cost mobile devices. however, these participants are generally self-interested actors that have their own goals and make local decisions about when and where to take measurements. this can lead to highly inefficient outcomes, where observations are either taken redundantly or do not provide sufficient information about key areas of interest. to address these challenges, it is necessary to guide and to coordinate participants, so they take measurements when it is most informative. to this end, we develop a computationally-efficient coordination algorithm (adaptive best-match) that suggests to users when and where to take measurements. our algorithm exploits probabilistic knowledge of human mobility patterns, but explicitly considers the uncertainty of these patterns and the potential unwillingness of people to take measurements when requested to do so. in particular, our algorithm uses a local search technique, clustering and random simulations to map participants to measurements that need to be taken in space and time. we empirically evaluate our algorithm on a real-world human mobility and air quality dataset and show that it outperforms the current state of the art by up to 24% in terms of utility gained.",2018
fact-alternating mutex groups for classical planning,"mutex groups are defined in the context of strips planning as sets of facts out of which, maximally, one can be true in any state reachable from the initial state. the importance of computing and exploiting mutex groups was repeatedly pointed out in many studies. however, the theoretical analysis of mutex groups is sparse in current literature. this work provides a complexity analysis showing that inference of mutex groups is as hard as planning itself (pspace-complete) and it also shows a tight relationship between mutex groups and graph cliques. this result motivates us to propose a new type of mutex group called a fact-alternating mutex group (fam-group) of which inference is np-complete. moreover, we introduce an algorithm for the inference of fam-groups based on integer linear programming that is complete with respect to the maximal fam-groups and we demonstrate how beneficial fam-groups can be in the translation of planning tasks into finite domain representation. finally, we show that fam-groups can be used for the detection of dead-end states and we propose a simple algorithm for the pruning of operators and facts as a preprocessing step that takes advantage of the properties of fam-groups. the experimental evaluation of the pruning algorithm shows a substantial increase in a number of solved tasks in domains from the optimal deterministic track of the last two planning competitions (ipc 2011 and 2014).",2018
revisiting the arcade learning environment: evaluation protocols and open problems for general agents,"the arcade learning environment (ale) is an evaluation platform that poses the challenge of building ai agents with general competency across dozens of atari 2600 games. it supports a variety of different problem settings and it has been receiving increasing attention from the scientific community, leading to some high-profile success stories such as the much publicized deep q-networks (dqn). in this article we take a big picture look at how the ale is being used by the research community. we show how diverse the evaluation methodologies in the ale have become with time, and highlight some key concerns when evaluating agents in the ale. we use this discussion to present some methodological best practices and provide new benchmark results using these best practices. to further the progress in the field, we introduce a new version of the ale that supports multiple game modes and provides a form of stochasticity we call sticky actions. we conclude this big picture look by revisiting challenges posed when the ale was introduced, summarizing the state-of-the-art in various problems and highlighting problems that remain open.",2018
on the behavior of convolutional nets for feature extraction,"deep neural networks are representation learning techniques. during training, a deep net is capable of generating a descriptive language of unprecedented size and detail in machine learning. extracting the descriptive language coded within a trained cnn model (in the case of image data), and reusing it for other purposes is a field of interest, as it provides access to the visual descriptors previously learnt by the cnn after processing millions of images, without requiring an expensive training phase. contributions to this field (commonly known as feature representation transfer or transfer learning) have been purely empirical so far, extracting all cnn features from a single layer close to the output and testing their performance by feeding them to a classifier. this approach has provided consistent results, although its relevance is limited to classification tasks. in a completely different approach, in this paper we statistically measure the discriminative power of every single feature found within a deep cnn, when used for characterizing every class of 11 datasets. we seek to provide new insights into the behavior of cnn features, particularly the ones from convolutional layers, as this can be relevant for their application to knowledge representation and reasoning. our results confirm that low and middle level features may behave differently to high level features, but only under certain conditions. we find that all cnn features can be used for knowledge representation purposes both by their presence or by their absence, doubling the information a single cnn feature may provide. we also study how much noise these features may include, and propose a thresholding approach to discard most of it. all these insights have a direct application to the generation of cnn embedding spaces.",2018
fully observable non-deterministic planning as assumption-based reactive synthesis,"we contribute to recent efforts in relating two approaches to automatic synthesis, namely, automated planning and discrete reactive synthesis. first, we develop a declarative characterization of the standard ""fairness"" assumption on environments in non-deterministic planning, and show that strong-cyclic plans are correct solution concepts for fair environments. this complements, and arguably completes, the existing foundational work on non-deterministic planning, which focuses on characterizing (and computing) plans enjoying special ""structural"" properties, namely loopy but closed policy structures. second, we provide an encoding suitable for reactive synthesis that avoids the naive exponential state space blowup. to do so, special care has to be taken to specify the fairness assumption on the environment in a succinct manner.",2018
distributed constraint optimization problems and applications: a survey,"the field of multi-agent system (mas) is an active area of research within artificial intelligence, with an increasingly important impact in industrial and other real-world applications. in a mas, autonomous agents interact to pursue personal interests and/or to achieve common objectives. distributed constraint optimization problems (dcops) have emerged as a prominent agent model to govern the agents' autonomous behavior, where both algorithms and communication models are driven by the structure of the specific problem. during the last decade, several extensions to the dcop model have been proposed to enable support of mas in complex, real-time, and uncertain environments. this survey provides an overview of the dcop model, offering a classification of its multiple extensions and addressing both resolution methods and applications that find a natural mapping within each class of dcops. the proposed classification suggests several future perspectives for dcop extensions and identifies challenges in the design of efficient resolution algorithms, possibly through the adaptation of strategies from different areas.",2018
trust as a precursor to belief revision,"belief revision is concerned with incorporating new information into a pre-existing set of beliefs. when the new information comes from another agent, we must first determine if that agent should be trusted. in this paper, we define trust as a pre-processing step before revision. we emphasize that trust in an agent is often restricted to a particular domain of expertise. we demonstrate that this form of trust can be captured by associating a state partition with each agent, then relativizing all reports to this partition before revising. we position the resulting family of trust-sensitive revision operators within the class of selective revision operators of ferme and hansson, and we prove a representation result that characterizes the class of trust-sensitive revision operators in terms of a set of postulates. we also show that trust-sensitive revision is manipulable, in the sense that agents can sometimes have incentive to pass on misleading information.",2018
"when subgraph isomorphism is really hard, and why this matters for graph databases","the subgraph isomorphism problem involves deciding whether a copy of a pattern graph occurs inside a larger target graph. the non-induced version allows extra edges in the target, whilst the induced version does not. although both variants are np-complete, algorithms inspired by constraint programming can operate comfortably on many real-world problem instances with thousands of vertices. however, they cannot handle arbitrary instances of this size. we show how to generate ""really hard"" random instances for subgraph isomorphism problems, which are computationally challenging with a couple of hundred vertices in the target, and only twenty pattern vertices. for the non-induced version of the problem, these instances lie on a satisfiable / unsatisfiable phase transition, whose location we can predict; for the induced variant, much richer behaviour is observed, and constrainedness gives a better measure of difficulty than does proximity to a phase transition. these results have practical consequences: we explain why the widely researched ""filter / verify"" indexing technique used in graph databases is founded upon a misunderstanding of the empirical hardness of np-complete problems, and cannot be beneficial when paired with any reasonable subgraph isomorphism algorithm.",2018
rademacher complexity bounds for a penalized multi-class semi-supervised algorithm,"we propose rademacher complexity bounds for multi-class classifiers trained with a two-step semi-supervised model. in the first step, the algorithm partitions the partially labeled data and then identifies dense clusters containing k predominant classes using the labeled training examples such that the proportion of their non-predominant classes is below a fixed threshold stands for clustering consistency. in the second step, a classifier is trained by minimizing a margin empirical loss over the labeled training set and a penalization term measuring the disability of the learner to predict the k predominant classes of the identified clusters. the resulting data-dependent generalization error bound involves the margin distribution of the classifier, the stability of the clustering technique used in the first step and rademacher complexity terms corresponding to partially labeled training data. our theoretical result exhibit convergence rates extending those proposed in the literature for the binary case, and experimental results on different multi-class classification problems show empirical evidence that supports the theory.",2018
symbol grounding association in multimodal sequences with missing elements,"in this paper, we extend a symbolic association framework for being able to handle missing elements in multimodal sequences. the general scope of the work is the symbolic associations of object-word mappings as it happens in language development in infants. in other words, two different representations of the same abstract concepts can associate in both directions. this scenario has been long interested in artificial intelligence, psychology, and neuroscience. in this work, we extend a recent approach for multimodal sequences (visual and audio) to also cope with missing elements in one or both modalities. our method uses two parallel long short-term memories (lstms) with a learning rule based on em-algorithm. it aligns both lstm outputs via dynamic time warping (dtw). we propose to include an extra step for the combination with the max operation for exploiting the common elements between both sequences. the motivation behind is that the combination acts as a condition selector for choosing the best representation from both lstms. we evaluated the proposed extension in the following scenarios: missing elements in one modality (visual or audio) and missing elements in both modalities (visual and sound). the performance of our extension reaches better results than the original model and similar results to individual lstm trained in each modality.",2018
belief update within propositional fragments,"belief change within the framework of fragments of propositional logic is one of the main and recent challenges in the knowledge representation research area. while previous research works focused on belief revision, belief merging, and belief contraction, the problem of belief update within fragments of classical logic has not been addressed so far. in the context of revision, it has been proposed to refine existing operators so that they operate within propositional fragments, and that the result of revision remains in the fragment under consideration. this approach is not restricted to the horn fragment but also applicable to other propositional fragments like krom and affine fragments. we generalize this notion of refinement to any belief change operator. we then focus on a specific belief change operation, namely belief update. we investigate the behavior of the refined update operators with respect to satisfaction of the km postulates and highlight differences between revision and update in this context.",2018
corpus-level fine-grained entity typing,"extracting information about entities remains an important research area. this paper addresses the problem of corpus-level entity typing, i.e., inferring from a large corpus that an entity is a member of a class, such as ""food"" or ""artist"". the application of entity typing we are interested in is knowledge base completion, specifically, to learn which classes an entity is a member of. we propose figment to tackle this problem. figment is embedding-based and combines (i) a global model that computes scores based on global information of an entity and (ii) a context model that first evaluates the individual occurrences of an entity and then aggregates the scores. each of the two proposed models has specific properties. for the global model, learning high-quality entity representations is crucial because it is the only source used for the predictions. therefore, we introduce representations using the name and contexts of entities on the three levels of entity, word, and character. we show that each level provides complementary information and a multi-level representation performs best. for the context model, we need to use distant supervision since there are no context-level labels available for entities. distantly supervised labels are noisy and this harms the performance of models. therefore, we introduce and apply new algorithms for noise mitigation using multi-instance learning. we show the effectiveness of our models on a large entity typing dataset built from freebase.",2018
"smote for learning from imbalanced data: progress and challenges, marking the 15-year anniversary","the synthetic minority oversampling technique (smote) preprocessing algorithm is considered ""de facto"" standard in the framework of learning from imbalanced data. this is due to its simplicity in the design of the procedure, as well as its robustness when applied to different type of problems. since its publication in 2002, smote has proven successful in a variety of applications from several different domains. smote has also inspired several approaches to counter the issue of class imbalance, and has also significantly contributed to new supervised learning paradigms, including multilabel classification, incremental learning, semi-supervised learning, multi-instance learning, among others. it is standard benchmark for learning from imbalanced data. it is also featured in a number of different software packages - from open source to commercial. in this paper, marking the fifteen year anniversary of smote, we reflect on the smote journey, discuss the current state of affairs with smote, its applications, and also identify the next set of challenges to extend smote for big data problems.",2018
visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure,"we investigate how neural networks can learn and process languages with hierarchical, compositional semantics. to this end, we define the artificial task of processing nested arithmetic expressions, and study whether different types of neural networks can learn to compute their meaning. we find that recursive neural networks can implement a generalising solution to this problem, and we visualise this solution by breaking it up in three steps: project, sum and squash. as a next step, we investigate recurrent neural networks, and show that a gated recurrent unit, that processes its input incrementally, also performs very well on this task: the network learns to predict the outcome of the arithmetic expressions with high accuracy, although performance deteriorates somewhat with increasing length. to develop an understanding of what the recurrent network encodes, visualisation techniques alone do not suffice. therefore, we develop an approach where we formulate and test multiple hypotheses on the information encoded and processed by the network. for each hypothesis, we derive predictions about features of the hidden state representations at each time step, and train 'diagnostic classifiers' to test those predictions. our results indicate that the networks follow a strategy similar to our hypothesised 'cumulative strategy', which explains the high accuracy of the network on novel expressions, the generalisation to longer expressions than seen in training, and the mild deterioration with increasing length. this in turn shows that diagnostic classifiers can be a useful technique for opening up the black box of neural networks. we argue that diagnostic classification, unlike most visualisation techniques, does scale up from small networks in a toy domain, to larger and deeper recurrent networks dealing with real-life data, and may therefore contribute to a better understanding of the internal dynamics of current state-of-the-art models in natural language processing.",2018
pre-wiring and pre-training: what does a neural network need to learn truly general identity rules?,"in an influential paper (“rule learning by seven-month-old infants”), marcus, vijayan, rao and vishton claimed that connectionist models cannot account for human success at learning tasks that involved generalization of abstract knowledge such as grammatical rules. this claim triggered a heated debate, centered mostly around variants of the simple recurrent network model. in our work, we revisit this unresolved debate and analyze the underlying issues from a different perspective. we argue that, in order to simulate human-like learning of grammatical rules, a neural network model should not be used as a tabula rasa, but rather, the initial wiring of the neural connections and the experience acquired prior to the actual task should be incorporated into the model. we present two methods that aim to provide such initial state: a manipulation of the initial connections of the network in a cognitively plausible manner (concretely, by implementing a “delay-line” memory), and a pre-training algorithm that incrementally challenges the network with novel stimuli. we implement such techniques in an echo state network (esn), and we show that only when combining both techniques the esn is able to learn truly general identity rules. finally, we discuss the relation between these cognitively motivated techniques and recent advances in deep learning.",2018
from feature to paradigm: deep learning in machine translation,"in the last years, deep learning algorithms have highly revolutionized several areas including speech, image and natural language processing. the speciﬁc ﬁeld of machine translation (mt) has not remained invariant. integration of deep learning in mt varies from re-modeling existing features into standard statistical systems to the development of a new architecture. among the diﬀerent neural networks, research works use feed-forward neural networks, recurrent neural networks and the encoder-decoder schema. these architectures are able to tackle challenges as having low-resources or morphology variations. this manuscript focuses on describing how these neural networks have been integrated to enhance diﬀerent aspects and models from statistical mt, including language modeling, word alignment, translation, reordering, and rescoring. then, we report the new neural mt approach together with a description of the foundational related works and recent approaches on using subword, characters and training with multilingual languages, among others. finally, we include an analysis of the corresponding challenges and future work in using deep learning in mt.",2018
complexity results and algorithms for extension enforcement in abstract argumentation,"argumentation is an active area of modern artificial intelligence (ai) research, with connections to a range of fields, from computational complexity theory and knowledge representation and reasoning to philosophy and social sciences, as well as application-oriented work in domains such as legal reasoning, multi-agent systems, and decision support. argumentation frameworks (afs) of abstract argumentation have become the graph-based formal model of choice for many approaches to argumentation in ai, with semantics defining sets of jointly acceptable arguments, i.e., extensions. understanding the dynamics of afs has been recently recognized as an important topic in the study of argumentation in ai. in this work, we focus on the so-called extension enforcement problem in abstract argumentation as a recently proposed form of argumentation dynamics. we provide a nearly complete computational complexity map of argument-fixed extension enforcement under various major af semantics, with results ranging from polynomial-time algorithms to completeness for the second level of the polynomial hierarchy. complementing the complexity results, we propose algorithms for np-hard extension enforcement based on constraint optimization under the maximum satisfiability (maxsat) paradigm. going beyond np, we propose novel maxsat-based counterexample-guided abstraction refinement procedures for the second-level complete problems and present empirical results on a prototype system constituting the first approach to extension enforcement in its generality.",2017
the sixth answer set programming competition,"answer set programming (asp) is a well-known paradigm of declarative programming with roots in logic programming and non-monotonic reasoning. similar to other closely related problem-solving technologies, such as sat/smt, qbf, planning and scheduling, advancements in asp solving are assessed in competition events. in this paper, we report about the design and results of the sixth asp competition, which was jointly organized by the university of calabria (italy), aalto university (finland), and the university of genoa (italy), in affiliation with the 13th international conference on logic programming and non-monotonic reasoning. this edition maintained some of the design decisions introduced in 2014, e.g., the conception of sub-tracks, the scoring scheme, and the adherence to a fixed modeling language in order to push the adoption of the asp-core-2 standard. on the other hand, it featured also some novelties, like a benchmark selection stage classifying instances according to their empirical hardness, and a ""marathon"" track where the top-performing systems are given more time for solving hard benchmarks.",2017
kernel contraction and base dependence,"the agm paradigm of belief change studies the dynamics of belief states in light of new information. finding, or even approximating, those beliefs that are dependent on or relevant to a change is valuable because, for example, it can narrow the set of beliefs considered during belief change operations. a strong intuition in this area is captured by g&#228;rdenforss preservation criterion (gpc), which suggests that formulas independent of a belief change should remain intact. gpc thus allows one to build dependence relations that are linked with belief change. such dependence relations can in turn be used as a theoretical benchmark against which to evaluate other approximate dependence or relevance relations. fari&#241;as and herzig axiomatize a dependence relation with respect to a belief set, and, based on gpc, they characterize the correspondence between agm contraction functions and dependence relations. in this paper, we introduce base dependence as a relation between formulas with respect to a belief base, and prove a more general characterization that shows the correspondence between kernel contraction and base dependence. at this level of generalization, different types of base dependence emerge, which we show to be a result of possible redundancy in the belief base. we further show that one of these relations that emerge, strong base dependence, is parallel to saturated kernel contraction. we then prove that our latter characterization is a reversible generalization of fari&#241;as and herzigs characterization. that is, in the special case when the underlying belief base is deductively closed (i.e., it is a belief set), strong base dependence reduces to dependence, and so do their respective characterizations. finally, an intriguing feature of fari&#241;as and herzigs formalism is that it meets other criteria for dependence, namely, keyness conjunction criterion for dependence (ccd) and g&#228;rdenforss conjunction criterion for independence (cci). we prove that our base dependence formalism also meets these criteria. even more interestingly, we offer a more specific criterion that implies both ccd and cci, and show our base dependence formalism also meets this new criterion.",2017
rationalisation of profiles of abstract argumentation frameworks: characterisation and complexity,"different agents may have different points of view. following a popular approach in the artificial intelligence literature, this can be modeled by means of different abstract argumentation frameworks, each consisting of a set of arguments the agent is contemplating and a binary attack-relation between them. a question arising in this context is whether the diversity of views observed in such a profile of argumentation frameworks is consistent with the assumption that every individual argumentation framework is induced by a combination of, first, some basic factual attack-relation between the arguments and, second, the personal preferences of the agent concerned regarding the moral or social values the arguments under scrutiny relate to. we treat this question of rationalisability of a profile as an algorithmic problem and identify tractable and intractable cases. in doing so, we distinguish different constraints on admissible rationalisations, e.g., concerning the types of preferences used or the number of distinct values involved. we also distinguish two different semantics for rationalisability, which differ in the assumptions made on how agents treat attacks between arguments they do not report. this research agenda, bringing together ideas from abstract argumentation and social choice, is useful for understanding what types of profiles can reasonably be expected to occur in a multiagent system.",2017
combining lexical and syntactic features for detecting content-dense texts in news,"content-dense news report important factual information about an event in direct, succinct manner. information seeking applications such as information extraction, question answering and summarization normally assume all text they deal with is content-dense. here we empirically test this assumption on news articles from the business, u.s. international relations, sports and science journalism domains. our findings clearly indicate that about half of the news texts in our study are in fact not content-dense and motivate the development of a supervised content-density detector. we heuristically label a large training corpus for the task and train a two-layer classifying model based on lexical and unlexicalized syntactic features. on manually annotated data, we compare the performance of domain-specific classifiers, trained on data only from a given news domain and a general classifier in which data from all four domains is pooled together. our annotation and prediction experiments demonstrate that the concept of content density varies depending on the domain and that naive annotators provide judgement biased toward the stereotypical domain label. domain-specific classifiers are more accurate for domains in which content-dense texts are typically fewer. domain independent classifiers reproduce better naive crowdsourced judgements. classification prediction is high across all conditions, around 80%.",2017
on the semantics and complexity of probabilistic logic programs,"we examine the meaning and the complexity of probabilistic logic programs that consist of a set of rules and a set of independent probabilistic facts (that is, programs based on sato's distribution semantics). we focus on two semantics, respectively based on stable and on well-founded models. we show that the semantics based on stable models (referred to as the ""credal semantics"") produces sets of probability measures that dominate infinitely monotone choquet capacities; we describe several useful consequences of this result. we then examine the complexity of inference with probabilistic logic programs. we distinguish between the complexity of inference when a probabilistic program and a query are given (the inferential complexity), and the complexity of inference when the probabilistic program is fixed and the query is given (the query complexity, akin to data complexity as used in database theory). we obtain results on the inferential and query complexity for acyclic, stratified, and normal propositional and relational programs; complexity reaches various levels of the counting hierarchy and even exponential levels.",2017
the linear programming approach to reach-avoid problems for markov decision processes,"one of the most fundamental problems in markov decision processes is analysis and control synthesis for safety and reachability specifications. we consider the stochastic reach-avoid problem, in which the objective is to synthesize a control policy to maximize the probability of reaching a target set at a given time, while staying in a safe set at all prior times. we characterize the solution to this problem through an infinite dimensional linear program. we then develop a tractable approximation to the infinite dimensional linear program through finite dimensional approximations of the decision space and constraints. for a large class of markov decision processes modeled by gaussian mixtures kernels we show that through a proper selection of the finite dimensional space, one can further reduce the computational complexity of the resulting linear program. we validate the proposed method and analyze its potential with numerical case studies.",2017
residual-guided look-ahead in and/or search for graphical models,"we introduce the concept of local bucket error for the mini-bucket heuristics and show how it can be used to improve the power of and/or search for combinatorial optimization tasks in graphical models (e.g. map/mpe or weighted csps). the local bucket error illuminates how the heuristic errors are distributed in the search space, guided by the mini-bucket heuristic. we present and analyze methods for compiling the local bucket-errors (exactly and approximately) and show that they can be used to yield an effective tool for balancing look-ahead overhead during search. this can be especially instrumental when memory is restricted, accommodating the generation of only weak compiled heuristics. we illustrate the impact of the proposed schemes in an extensive empirical evaluation for both finding exact solutions and anytime suboptimal solutions.",2017
preference-based inconsistency management in multi-context systems,"multi-context systems (mcs) are a powerful framework for interlinking possibly heterogeneous, autonomous knowledge bases, where information can be exchanged among knowledge bases by designated bridge rules with negation as failure. an acknowledged issue with mcs is inconsistency that arises due to the information exchange. to remedy this problem, inconsistency removal has been proposed in terms of repairs, which modify bridge rules based on suitable notions for diagnosis of inconsistency. in general, multiple diagnoses and repairs do exist; this leaves the user, who arguably may oversee the inconsistency removal, with the task of selecting some repair among all possible ones. to aid in this regard, we extend the mcs framework with preference information for diagnoses, such that undesired diagnoses are filtered out and diagnoses that are most preferred according to a preference ordering are selected. we consider preference information at a generic level and develop meta-reasoning techniques on diagnoses in mcs that can be exploited to reduce preference-based selection of diagnoses to computing ordinary subset-minimal diagnoses in an extended mcs. we describe two meta-reasoning encodings for preference orders: the first is conceptually simple but may incur an exponential blowup. the second is increasing only linearly in size and based on duplicating the original mcs. the latter requires nondeterministic guessing if a subset-minimal among all most preferred diagnoses should be computed. however, a complexity analysis of diagnoses shows that this is worst-case optimal, and that in general, preferred diagnoses have the same complexity as subset-minimal ordinary diagnoses. furthermore, (subset-minimal) filtered diagnoses and (subset-minimal) ordinary diagnoses also have the same complexity.",2017
resolving over-constrained temporal problems with uncertainty through conflict-directed relaxation,"over-subscription, that is, being assigned too many things to do, is commonly encountered in temporal scheduling problems. as human beings, we often want to do more than we can actually do, and underestimate how long it takes to perform each task. decision makers can benefit from aids that identify when these failure situations are likely, the root causes of these failures, and resolutions to these failures. in this paper, we present a decision assistant that helps users resolve over-subscribed temporal problems. the system works like an experienced advisor that can quickly identify the cause of failure underlying temporal problems and compute resolutions. the core of the decision assistant is the best-first conflict-directed relaxation (bcdr) algorithm, which can detect conflicting sets of constraints within temporal problems, and computes continuous relaxations for them that weaken constraints to the minimum extent, instead of removing them completely. bcdr is an extension to the conflict-directed a* algorithm, first developed in the model-based reasoning community to compute most likely system diagnoses or reconfigurations. it generalizes the discrete conflicts and relaxations, to hybrid conflicts and relaxations, which denote minimal inconsistencies and minimal relaxations to both discrete and continuous relaxable constraints. in addition, bcdr is capable of handling temporal uncertainty, expressed as either set-bounded or probabilistic durations, and can compute preferred trade-offs between the risk of violating a schedule requirement, versus the loss of utility by weakening those requirements. bcdr has been applied to several decision support applications in different domains, including deep-sea exploration, urban travel planning and transit system management. it has demonstrated its effectiveness in helping users resolve over-subscribed scheduling problems and evaluate the robustness of existing solutions. in our benchmark experiments, bcdr has also demonstrated its efficiency on solving large-scale scheduling problems in the aforementioned domains. thanks to its conflict-driven approach for computing relaxations, bcdr achieves one to two orders of magnitude improvements on runtime performance when compared to state-of-the-art numerical solvers.",2017
on hash-based work distribution methods for parallel best-first search,"parallel best-first search algorithms such as hash distributed a* (hda*) distribute work among the processes using a global hash function. we analyze the search and communication overheads of state-of-the-art hash-based parallel best-first search algorithms, and show that although zobrist hashing, the standard hash function used by hda*, achieves good load balance for many domains, it incurs significant communication overhead since almost all generated nodes are transferred to a different processor than their parents. we propose abstract zobrist hashing, a new work distribution method for parallel search which, instead of computing a hash value based on the raw features of a state, uses a feature projection function to generate a set of abstract features which results in a higher locality, resulting in reduced communications overhead. we show that abstract zobrist hashing outperforms previous methods on search domains using hand-coded, domain specific feature projection functions. we then propose grazhda*, a graph-partitioning based approach to automatically generating feature projection functions. grazhda* seeks to approximate the partitioning of the actual search space graph by partitioning the domain transition graph, an abstraction of the state space graph. we show that grazhda* outperforms previous methods on domain-independent planning.",2017
a survey on lexical simplification,"lexical simplification is the process of replacing complex words in a given sentence with simpler alternatives of equivalent meaning. this task has wide applicability both as an assistive technology for readers with cognitive impairments or disabilities, such as dyslexia and aphasia, and as a pre-processing tool for other natural language processing tasks, such as machine translation and summarisation. the problem is commonly framed as a pipeline of four steps: the identification of complex words, the generation of substitution candidates, the selection of those candidates that fit the context, and the ranking of the selected substitutes according to their simplicity. in this survey we review the literature for each step in this typical lexical simplification pipeline and provide a benchmarking of existing approaches for these steps on publicly available datasets. we also provide pointers for datasets and resources available for the task.",2017
time and space bounds for planning,"there is an extensive literature on the complexity of planning, but explicit bounds on time and space complexity are very rare. on the other hand, problems like the constraint satisfaction problem (csp) have been thoroughly analysed in this respect. we provide a number of upper- and lower-bound results (the latter based on various complexity-theoretic assumptions such as the exponential time hypothesis) for both satisficing and optimal planning. we show that many classes of planning instances exhibit a dichotomy: either they can be solved in polynomial time or they cannot be solved in subexponential time. in many cases, we can even prove closely matching upper and lower bounds. our results also indicate, analogously to csps, the existence of sharp phase transitions. we finally study and discuss the trade-off between time and space. in particular, we show that depth-first search may sometimes be a viable option for planning under severe space constraints.",2017
multi-organ exchange,"kidney exchange, where candidates with organ failure trade incompatible but willing donors, is a life-saving alternative to the deceased donor waitlist, which has inadequate supply to meet demand. while fielded kidney exchanges see huge benefit from altruistic kidney donors (who give an organ without a paired needy candidate), a significantly higher medical risk to the donor deters similar altruism with livers. in this paper, we begin by exploring the idea of large-scale liver exchange, and show on demographically accurate data that vetted kidney exchange algorithms can be adapted to clear such an exchange at the nationwide level. we then propose cross-organ donation where kidneys and livers can be bartered for each other. we show theoretically that this multi-organ exchange provides linearly more transplants than running separate kidney and liver exchanges. this linear gain is a product of altruistic kidney donors creating chains that thread through the liver pool; it exists even when only a small but constant portion of the donors on the kidney side of the pool are willing to donate a liver lobe. we support this result experimentally on demographically accurate multi-organ exchanges. we conclude with thoughts regarding the fielding of a nationwide liver or joint liver-kidney exchange from a legal and computational point of view.",2017
viewpoint: a critical view on smart cities and ai,"ai developments on smart cities, if not critical, risk making a flawed urban model more efficient. instead, we suggest that ai should challenge the mainstream techno-optimistic approach to solving urban problems by dialoguing with other academic fields, questioning the dominant urban paradigm, and creating transformative solutions. we claim that doing differently, rather than doing better, may be smarter for cities and the common good. this article is part of the special track on ai and society.",2017
chamberlin--courant rule with approval ballots: approximating the maxcover problem with bounded frequencies in fpt time,"we consider the problem of winner determination under chamberlin--courant's multiwinner voting rule with approval utilities. this problem is equivalent to the well-known np-complete maxcover problem and, so, the best polynomial-time approximation algorithm for it has approximation ratio 1 - 1/e. we show exponential-time/fpt approximation algorithms that, on one hand, achieve arbitrarily good approximation ratios and, on the other hand, have running times much better than known exact algorithms. we focus on the cases where the voters have to approve of at most/at least a given number of candidates.",2017
sample-based tree search with fixed and adaptive state abstractions,"sample-based tree search (sbts) is an approach to solving markov decision problems based on constructing a lookahead search tree using random samples from a generative model of the mdp. it encompasses monte carlo tree search (mcts) algorithms like uct as well as algorithms such as sparse sampling. sbts is well-suited to solving mdps with large state spaces due to the relative insensitivity of sbts algorithms to the size of the state space. the limiting factor in the performance of sbts tends to be the exponential dependence of sample complexity on the depth of the search tree. the number of samples required to build a search tree is o((|a|b)^d), where |a| is the number of available actions, b is the number of possible random outcomes of taking an action, and d is the depth of the tree. state abstraction can be used to reduce b by aggregating random outcomes together into abstract states. recent work has shown that abstract tree search often performs substantially better than tree search conducted in the ground state space. this paper presents a theoretical and empirical evaluation of tree search with both fixed and adaptive state abstractions. we derive a bound on regret due to state abstraction in tree search that decomposes abstraction error into three components arising from properties of the abstraction and the search algorithm. we describe versions of popular sbts algorithms that use fixed state abstractions, and we introduce the progressive abstraction refinement in sparse sampling (parss) algorithm, which adapts its abstraction during search. we evaluate parss as well as sparse sampling with fixed abstractions on 12 experimental problems, and find that parss outperforms search with a fixed abstraction and that search with even highly inaccurate fixed abstractions outperforms search without abstraction. these results establish progressive abstraction refinement as a promising basis for new tree search algorithms, and we propose directions for future work within the progressive refinement framework.",2017
on the equivalence between assumption-based argumentation and logic programming,"assumption-based argumentation (aba) has been shown to subsume various other non-monotonic reasoning formalisms, among them normal logic programming (lp). we re-examine the relationship between aba and lp and show that normal lp also subsumes (flat) aba. more precisely, we specify a procedure that given a (flat) aba framework yields an associated logic program with almost the same syntax whose semantics coincide with those of the aba framework. that is, the 3-valued stable (respectively well-founded, regular, 2-valued stable, and ideal) models of the associated logic program coincide with the complete (respectively grounded, preferred, stable, and ideal) assumption labellings and extensions of the aba framework. moreover, we show how our results on the translation from aba to lp can be reapplied for a reverse translation from lp to aba, and observe that some of the existing results in the literature are in fact special cases of our work. overall, we show that (flat) aba frameworks can be seen as normal logic programs with a slightly different syntax. this implies that methods developed for one of these formalisms can be equivalently applied to the other by simply modifying the syntax.",2017
prime implicate generation in equational logic,"we present an algorithm for the generation of prime implicates in equational logic, that is, of the most general consequences of formul&#230; containing equations and disequations between first-order terms. this algorithm is defined by a calculus that is proved to be correct and complete. we then focus on the case where the considered clause set is ground, i.e., contains no variables, and devise a specialized tree data structure that is designed to efficiently detect and delete redundant implicates. the corresponding algorithms are presented along with their termination and correctness proofs. finally, an experimental evaluation of this prime implicate generation method is conducted in the ground case, including a comparison with state-of-the-art propositional and first-order prime implicate generation tools.",2017
on monte carlo tree search and reinforcement learning,"fuelled by successes in computer go, monte carlo tree search (mcts) has achieved widespread adoption within the games community. its links to traditional reinforcement learning (rl) methods have been outlined in the past; however, the use of rl techniques within tree search has not been thoroughly studied yet. in this paper we re-examine in depth this close relation between the two fields; our goal is to improve the cross-awareness between the two communities. we show that a straightforward adaptation of rl semantics within tree search can lead to a wealth of new algorithms, for which the traditional mcts is only one of the variants. we confirm that planning methods inspired by rl in conjunction with online search demonstrate encouraging results on several classic board games and in arcade video game competitions, where our algorithm recently ranked first. our study promotes a unified view of learning, planning, and search.",2017
elections with few voters: candidate control can be easy,"we study the computational complexity of candidate control in elections with few voters, that is, we consider the parameterized complexity of candidate control in elections with respect to the number of voters as a parameter. we consider both the standard scenario of adding and deleting candidates, where one asks whether a given candidate can become a winner (or, in the destructive case, can be precluded from winning) by adding or deleting few candidates, as well as a combinatorial scenario where adding/deleting a candidate automatically means adding or deleting a whole group of candidates. considering several fundamental voting rules, our results show that the parameterized complexity of candidate control, with the number of voters as the parameter, is much more varied than in the setting with many voters.",2017
learning neural audio embeddings for grounding semantics in auditory perception,"multi-modal semantics, which aims to ground semantic representations in perception, has relied on feature norms or raw image data for perceptual input. in this paper we examine grounding semantic representations in raw auditory data, using standard evaluations for multi-modal semantics. after having shown the quality of such auditorily grounded representations, we show how they can be applied to tasks where auditory perception is relevant, including two unsupervised categorization experiments, and provide further analysis. we find that features transfered from deep neural networks outperform bag of audio words approaches. to our knowledge, this is the first work to construct multi-modal models from a combination of textual information and auditory information extracted from deep neural networks, and the first work to evaluate the performance of tri-modal (textual, visual and auditory) semantic models.",2017
confidence decision trees via online and active learning for streaming data,"decision tree classifiers are a widely used tool in data stream mining. the use of confidence intervals to estimate the gain associated with each split leads to very effective methods, like the popular hoeffding tree algorithm. from a statistical viewpoint, the analysis of decision tree classifiers in a streaming setting requires knowing when enough new information has been collected to justify splitting a leaf. although some of the issues in the statistical analysis of hoeffding trees have been already clarified, a general and rigorous study of confidence intervals for splitting criteria is missing. we fill this gap by deriving accurate confidence intervals to estimate the splitting gain in decision tree learning with respect to three criteria: entropy, gini index, and a third index proposed by kearns and mansour. we also extend our confidence analysis to a selective sampling setting, in which the decision tree learner adaptively decides which labels to query in the stream. we provide theoretical guarantees bounding the probability that the decision tree learned via our selective sampling strategy classifies suboptimally the next example in the stream. experiments on real and synthetic data in a streaming setting show that our trees are indeed more accurate than trees with the same number of leaves generated by state-of-the-art techniques. in addition to that, our active learning module empirically uses fewer labels without significantly hurting the performance.",2017
axiomatising incomplete preferences through sets of desirable gambles,"we establish the equivalence of two very general theories: the first is the decision-theoretic formalisation of incomplete preferences based on the mixture independence axiom; the second is the theory of coherent sets of desirable gambles (bounded variables) developed in the context of imprecise probability and extended here to vector-valued gambles. such an equivalence allows us to analyse the theory of incomplete preferences from the point of view of desirability. among other things, this leads us to uncover an unexpected and clarifying relation: that the notion of `state independence'---the traditional assumption that we can have separate models for beliefs (probabilities) and values (utilities)---coincides with that of `strong independence' in imprecise probability; this connection leads us also to propose much weaker, and arguably more realistic, notions of state independence. then we simplify the treatment of complete beliefs and values by putting them on a more equal footing. we study the role of the archimedean condition---which allows us to actually talk of expected utility---, identify some weaknesses and propose alternatives that solve these. more generally speaking, we show that desirability is a valuable alternative foundation to preferences for decision theory that streamlines and unifies a number of concepts while preserving great generality. in addition, the mentioned equivalence shows for the first time how to extend the theory of desirability to imprecise non-linear utility, thus enabling us to formulate one of the most powerful self-consistent theories of reasoning and decision-making available today.",2017
a game theoretic analysis of the adversarial retrieval setting,"the main goal of search engines is ad hoc retrieval: ranking documents in a corpus by their relevance to the information need expressed by a query. the probability ranking principle (prp) --- ranking the documents by their relevance probabilities --- is the theoretical foundation of most existing ad hoc document retrieval methods. a key observation that motivates our work is that the prp does not account for potential post-ranking effects; specifically, changes to documents that result from a given ranking. yet, in adversarial retrieval settings such as the web, authors may consistently try to promote their documents in rankings by changing them. we prove that, indeed, the prp can be sub-optimal in adversarial retrieval settings. we do so by presenting a novel game theoretic analysis of the adversarial setting. the analysis is performed for different types of documents (single-topic and multi-topic) and is based on different assumptions about the writing qualities of documents' authors. we show that in some cases, introducing randomization into the document ranking function yields an overall user utility that transcends that of applying the prp.",2017
a knowledge level account of forgetting,"forgetting is an operation on knowledge bases that has been addressed in different areas of knowledge representation and with respect to different formalisms, including classical propositional and first-order logic, modal logics, logic programming, and description logics. definitions of forgetting have been expressed in terms of manipulation of formulas, sets of postulates, isomorphisms between models, bisimulations, second-order quantification, elementary equivalence, and others. in this paper, forgetting is regarded as an abstract belief change operator, independent of the underlying logic. the central thesis is that forgetting amounts to a reduction in the language, specifically the signature, of a logic. the main definition is simple: the result of forgetting a portion of a signature in a theory is given by the set of logical consequences of this theory over the reduced language. this definition offers several advantages. foremost, it provides a uniform approach to forgetting, with a definition that is applicable to any logic with a well-defined consequence relation. hence it generalises a disparate set of logic-specific definitions with a general, high-level definition. results obtained in this approach are thus applicable to all subsumed formal systems, and many results are obtained much more straightforwardly. this view also leads to insights with respect to specific logics: for example, forgetting in first-order logic is somewhat different from the accepted approach. moreover, the approach clarifies the relation between forgetting and related operations, including belief contraction.",2017
indirect causes in dynamic bayesian networks revisited,"modeling causal dependencies often demands cycles at a coarse-grained temporal scale. if bayesian networks are to be used for modeling uncertainties, cycles are eliminated with dynamic bayesian networks, spreading indirect dependencies over time and enforcing an infinitesimal resolution of time. without a ``causal design,'' i.e., without anticipating indirect influences appropriately in time, we argue that such networks return spurious results. by identifying activator random variables, we propose activator dynamic bayesian networks (adbns) which are able to rapidly adapt to contexts under a causal use of time, anticipating indirect influences on a solid mathematical basis using familiar bayesian network semantics. adbns are well-defined dynamic probabilistic graphical models allowing one to model cyclic dependencies from local and causal perspectives while preserving a classical, familiar calculus and classically known algorithms, without introducing any overhead in modeling or inference.",2017
the price of anarchy in auctions,"this survey outlines a general and modular theory for proving approximation guarantees for equilibria of auctions in complex settings. this theory complements traditional economic techniques, which generally focus on exact and optimal solutions and are accordingly limited to relatively stylized settings. we highlight three user-friendly analytical tools: smoothness-type inequalities, which immediately yield approximation guarantees for many auction formats of interest in the special case of complete information and deterministic strategies; extension theorems, which extend such guarantees to randomized strategies, no-regret learning outcomes, and incomplete-information settings; and composition theorems, which extend such guarantees from simpler to more complex auctions. combining these tools yields tight worst-case approximation guarantees for the equilibria of many widely-used auction formats.",2017
learning discrete bayesian networks from continuous data,"learning bayesian networks from raw data can help provide insights into the relationships between variables. while real data often contains a mixture of discrete and continuous-valued variables, many bayesian network structure learning algorithms assume all random variables are discrete. thus, continuous variables are often discretized when learning a bayesian network. however, the choice of discretization policy has significant impact on the accuracy, speed, and interpretability of the resulting models. this paper introduces a principled bayesian discretization method for continuous variables in bayesian networks with quadratic complexity instead of the cubic complexity of other standard techniques. empirical demonstrations show that the proposed method is superior to the established minimum description length algorithm. in addition, this paper shows how to incorporate existing methods into the structure learning process to discretize all continuous variables and simultaneously learn bayesian network structures.",2017
"parliamentary voting procedures: agenda control, manipulation, and uncertainty","we study computational problems for two popular parliamentary voting procedures: the amendment procedure and the successive procedure. they work in multiple stages where the result of each stage may influence the result of the next stage. both procedures proceed according to a given linear order of the alternatives, an agenda. we obtain the following results for both voting procedures: on the one hand, deciding whether one can make a specific alternative win by reporting insincere preferences by the fewest number of voters, the manipulation problem, or whether there is a suitable ordering of the agenda, the agenda control problem, takes polynomial time. on the other hand, our experimental studies with real-world data indicate that most preference profiles cannot be manipulated by only few voters and a successful agenda control is typically impossible. if the voters' preferences are incomplete, then deciding whether an alternative can possibly win is np-hard for both procedures. whilst deciding whether an alternative necessarily wins is conp-hard for the amendment procedure, it is polynomial-time solvable for the successive procedure.",2017
market interfaces for electric vehicle charging,"we consider settings where owners of electric vehicles (evs) participate in a market mechanism to charge their vehicles. existing work on such mechanisms has typically assumed that participants are fully rational and can report their preferences accurately via some interface to the mechanism or to a software agent participating on their behalf. however, this may not be reasonable in settings with non-expert human end-users.thus, our overarching aim in this paper is to determine experimentally if a fully expressive market interface that enables accurate preference reports is suitable for the ev charging domain, or, alternatively, if a simpler, restricted interface that reduces the space of possible options is preferable. in doing this, we measure the performance of an interface both in terms of how it helps participants maximise their utility and how it affects deliberation time. our secondary objective is to contrast two different types of restricted interfaces that vary in how they restrict the space of preferences that can be reported. to enable this analysis, we develop a novel game that replicates key features of an abstract ev charging scenario. in two experiments with over 300 users, we show that restricting the users' preferences significantly reduces the time they spend deliberating (by up to half in some cases). an extensive usability survey confirms that this restriction is furthermore associated with a lower perceived cognitive burden on the users. more surprisingly, at the same time, using restricted interfaces leads to an increase in the users' performance compared to the fully expressive interface (by up to 70%). we also show that some restricted interfaces have the desirable effect of reducing the energy consumption of their users by up to 20% while achieving the same utility as other interfaces. finally, we find that a reinforcement learning agent displays similar performance trends to human users, enabling a novel methodology for evaluating market interfaces.",2017
sampling based approaches for minimizing regret in uncertain markov decision processes (mdps),"markov decision processes (mdps) are an effective model to represent decision processes in the presence of transitional uncertainty and reward tradeoffs. however, due to the difficulty in exactly specifying the transition and reward functions in mdps, researchers have proposed uncertain mdp models and robustness objectives in solving those models. most approaches for computing robust policies have focused on the computation of maximin policies which maximize the value in the worst case amongst all realisations of uncertainty. given the overly conservative nature of maximin policies, recent work has proposed minimax regret as an ideal alternative to the maximin objective for robust optimization. however, existing algorithms for handling minimax regret are restricted to models with uncertainty over rewards only and they are also limited in their scalability. therefore, we provide a general model of uncertain mdps that considers uncertainty over both transition and reward functions. furthermore, we also consider dependence of the uncertainty across different states and decision epochs. we also provide a mixed integer linear program formulation for minimizing regret given a set of samples of the transition and reward functions in the uncertain mdp. in addition, we provide two myopic variants of regret, namely cumulative expected myopic regret (cemr) and one step regret (osr) that can be optimized in a scalable manner. specifically, we provide dynamic programming and policy iteration based algorithms to optimize cemr and osr respectively. finally, to demonstrate the effectiveness of our approaches, we provide comparisons on two benchmark problems from literature. we observe that optimizing the myopic variants of regret, osr and cemr are better than directly optimizing the regret.",2017
adopting the cascade model in ad auctions: efficiency bounds and truthful algorithmic mechanisms,"sponsored search auctions (ssas) are one of the most successful applications of microeconomic mechanisms, with a revenue of about $72 billion in the us alone in 2016. however, the problem of designing the best economic mechanism for sponsored search auctions is far from being solved, and, given the amount at stake, it is no surprise that it has received growing attention over the past few years. the most common auction mechanism for ssas is the generalized second price (gsp). however, the gsp is known not to be truthful: the agents participating in the auction might have an incentive to report false values, generating economic inefficiency and suboptimal revenues in turn. superior, efficient truthful mechanisms, such as the vickrey-clarke-groves (vcg) auction, are well known in the literature. however, while the vcg auction is currently adopted for the strictly related scenario of contextual advertising, e.g., by google and facebook, companies are reluctant to extend it to ssas, fearing prohibitive switching costs. other than truthfulness, two issues are of paramount importance in designing effective ssas. first, the choice of the user model; not only does an accurate user model better target ads to users, it also is a critical factor in reducing the inefficiency of the mechanism. often an antagonist to this, the second issue is the running time of the mechanism, given the performance pressure these mechanisms undertake in real-world applications. in our work, we argue in favor of adopting the vcg mechanism based on the cascade model with ad/position externalities (apdc-vcg). our study includes both the derivation of inefficiency bounds and the design and the experimental evaluation of exact and approximate algorithms.",2017
privacy preserving implementation of the max-sum algorithm and its variants,"one of the basic motivations for solving dcops is maintaining agents' privacy. thus, researchers have evaluated the privacy loss of dcop algorithms and defined corresponding notions of privacy preservation for secured dcop algorithms. however, no secured protocol was proposed for max-sum, which is among the most studied dcop algorithms. as part of the ongoing effort of designing secure dcop algorithms, we propose p-max-sum, the first private algorithm that is based on max-sum. the proposed algorithm has multiple agents preforming the role of each node in the factor graph, on which the max-sum algorithm operates. p-max-sum preserves three types of privacy: topology privacy, constraint privacy, and assignment/decision privacy. by allowing a single call to a trusted coordinator, p-max-sum also preserves agent privacy. the two main cryptographic means that enable this privacy preservation are secret sharing and homomorphic encryption. in addition, we design privacy-preserving implementations of four variants of max-sum. we conclude by analyzing the price of privacy in terns of runtime overhead, both theoretically and by extensive experimentation.",2017
and/or branch-and-bound on a computational grid,"we present a parallel and/or branch-and-bound scheme that uses the power of a computational grid to push the boundaries of feasibility for combinatorial optimization. two variants of the scheme are described, one of which aims to use machine learning techniques for parallel load balancing. in-depth analysis identifies two inherent sources of parallel search space redundancies that, together with general parallel execution overhead, can impede parallelization and render the problem far from embarrassingly parallel. we conduct extensive empirical evaluation on hundreds of cpus, the first of its kind, with overall positive results. in a significant number of cases parallel speedup is close to the theoretical maximum and we are able to solve many very complex problem instances orders of magnitude faster than before; yet analysis of certain results also serves to demonstrate the inherent limitations of the approach due to the aforementioned redundancies.",2017
game-theoretic question selection for tests,"conventionally, the questions on a test are assumed to be kept secret from test takers until the test. however, for tests that are taken on a large scale, particularly asynchronously, this is very hard to achieve. for example, toefl ibt and driver's license test questions are easily found online. this also appears likely to become an issue for massive open online courses (moocs, as offered for example by coursera, udacity, and edx). specifically, the test result may not reflect the true ability of a test taker if questions are leaked beforehand. in this paper, we take the loss of confidentiality as a fact. even so, not all hope is lost as the test taker can memorize only a limited set of questions' answers, and the tester can randomize which questions to let appear on the test. we model this as a stackelberg game, where the tester commits to a mixed strategy and the follower responds. informally, the goal of the tester is to best reveal the true ability of a test taker, while the test taker tries to maximize the test result (pass probability or score). we provide an exponential-size linear program formulation that computes the optimal test strategy, prove several np-hardness results on computing optimal test strategies in general, and give efficient algorithms for special cases (scored tests and single-question tests). experiments are also provided for those proposed algorithms to show their scalability and the increase of the tester's utility relative to that of the uniform-at-random strategy. the increase is quite significant when questions have some correlation---for example, when a test taker who can solve a harder question can always solve easier questions.",2017
"finding a small vertex cover in massive sparse graphs: construct, local search, and preprocess","the problem of finding a minimum vertex cover (minvc) in a graph is a well known np-hard combinatorial optimization problem of great importance in theory and practice. due to its np-hardness, there has been much interest in developing heuristic algorithms for finding a small vertex cover in reasonable time. previously, heuristic algorithms for minvc have focused on solving graphs of relatively small size, and they are not suitable for solving massive graphs as they usually have high-complexity heuristics. this paper explores techniques for solving minvc in very large scale real-world graphs, including a construction algorithm, a local search algorithm and a preprocessing algorithm. both the construction and search algorithms are based on low-complexity heuristics, and we combine them to develop a heuristic algorithm for minvc called fastvc. experimental results on a broad range of real-world massive graphs show that, our algorithms are very fast and have better performance than previous heuristic algorithms for minvc. we also develop a preprocessing algorithm to simplify graphs for minvc algorithms. by applying the preprocessing algorithm to local search algorithms, we obtain two efficient minvc solvers called numvc2+p and fastvc2+p, which show further improvement on the massive graphs.",2017
perturbation training for human-robot teams,"in this work, we design and evaluate a computational learning model that enables a human-robot team to co-develop joint strategies for performing novel tasks that require coordination. the joint strategies are learned through ""perturbation training,"" a human team-training strategy that requires team members to practice variations of a given task to help their team generalize to new variants of that task. we formally define the problem of human-robot perturbation training and develop and evaluate the first end-to-end framework for such training, which incorporates a multi-agent transfer learning algorithm, human-robot co-learning framework and communication protocol. our transfer learning algorithm, adaptive perturbation training (adapt), is a hybrid of transfer and reinforcement learning techniques that learns quickly and robustly for new task variants. we empirically validate the benefits of adapt through comparison to other hybrid reinforcement and transfer learning techniques aimed at transferring knowledge from multiple source tasks to a single target task. we also demonstrate that adapt's rapid learning supports live interaction between a person and a robot, during which the human-robot team trains to achieve a high level of performance for new task variants. we augment adapt with a co-learning framework and a computational bi-directional communication protocol so that the robot can co-train with a person during live interaction. results from large-scale human subject experiments (n=48) indicate that adapt enables an agent to learn in a manner compatible with a human's own learning process, and that a robot undergoing perturbation training with a human results in a high level of team performance. finally, we demonstrate that human-robot training using adapt in a simulation environment produces effective performance for a team incorporating an embodied robot partner.",2017
the length of shortest vertex paths in binary occupancy grids compared to shortest r-constrained ones,"we study the problem of finding a short path from a start to a goal within a two-dimensional continuous and isotropic terrain that has been discretized into an array of accessible and blocked cells. a classic approach obtains a grid path where each step is along the edge of an accessible cell or diagonally across one. grid paths suffer from `digitization bias' -- even if two locations have line-of-sight, the minimum travelling cost between them can be greater than the distance along the line-of-sight. in a vertex path, steps are allowed from a cell corner to any other cell corner if they have line-of-sight. while the `digitization bias' is smaller, shortest vertex paths are impractical to find by brute force. recent research has thus turned to methods for finding short (but not necessarily shortest) vertex paths. to establish the methods' potential utility, we calculate upper bounds on the difference in length between the shortest vertex paths versus the shortest r-constrained ones where an r-constrained path consists of line segments that each traverse at most r rows and at most r columns of cells. the difference in length reduces as r increases -- indeed the shortest vertex paths are at most 1 percent shorter than the shortest 4-constrained ones. this article will be useful to developers and users of short(est) vertex paths algorithms who want to trade path length for improved runtimes in a predictable manner.",2017
probabilistic reasoning with abstract argumentation frameworks,"abstract argumentation offers an appealing way of representing and evaluating arguments and counterarguments. this approach can be enhanced by considering probability assignments on arguments, allowing for a quantitative treatment of formal argumentation. in this paper, we regard the assignment as denoting the degree of belief that an agent has in an argument being acceptable. while there are various interpretations of this, an example is how it could be applied to a deductive argument. here, the degree of belief that an agent has in an argument being acceptable is a combination of the degree to which it believes the premises, the claim, and the derivation of the claim from the premises. we consider constraints on these probability assignments, inspired by crisp notions from classical abstract argumentation frameworks and discuss the issue of probabilistic reasoning with abstract argumentation frameworks. moreover, we consider the scenario when assessments on the probabilities of a subset of the arguments are given and the probabilities of the remaining arguments have to be derived, taking both the topology of the argumentation framework and principles of probabilistic reasoning into account. we generalise this scenario by also considering inconsistent assessments, i.e., assessments that contradict the topology of the argumentation framework. building on approaches to inconsistency measurement, we present a general framework to measure the amount of conflict of these assessments and provide a method for inconsistency-tolerant reasoning.",2017
welfare effects of market making in continuous double auctions,"we investigate the effects of market making on market performance, focusing on allocative efficiency as well as gains from trade accrued by background traders. we employ empirical simulation-based methods to evaluate heuristic strategies for market makers as well as background investors in a variety of complex trading environments. our market model incorporates private and common valuation elements, with dynamic fundamental value and asymmetric information. in this context, we compare the surplus achieved by background traders in strategic equilibrium, with and without a market maker. our findings indicate that the presence of the market maker strongly tends to increase total welfare across various environments. market-maker profit may or may not exceed the welfare gain, thus the effect on background-investor surplus is ambiguous. we find that market making tends to benefit investors in relatively thin markets, and situations where background traders are impatient, due to limited trading opportunities. the presence of additional market makers increases these benefits, as competition drives the market makers to provide liquidity at lower price spreads. a thorough sensitivity analysis indicates that these results are robust to reasonable changes in model parameters.",2017
logical formalizations of commonsense reasoning: a survey,"commonsense reasoning is in principle a central problem in artificial intelligence, but it is a very difficult one. one approach that has been pursued since the earliest days of the field has been to encode commonsense knowledge as statements in a logic-based representation language and to implement commonsense reasoning as some form of logical inference. this paper surveys the use of logic-based representations of commonsense knowledge in artificial intelligence research.",2017
decision-theoretic planning under anonymity in agent populations,"we study the problem of self-interested planning under uncertainty in settings shared with more than a thousand other agents, each of which plans at its own individual level. we refer to such large numbers of agents as an agent population. the decision-theoretic formalism of interactive partially observable markov decision process (i-pomdp) is used to model the agent's self-interested planning. the first contribution of this article is a method for drastically scaling the finitely-nested i-pomdp to certain agent populations for the first time. our method exploits two types of structure that is often exhibited by agent populations -- anonymity and context-specific independence. we present a variant called the many-agent i-pomdp that models both these types of structure to plan efficiently under uncertainty in multiagent settings. in particular, the complexity of the belief update and solution in the many-agent i-pomdp is polynomial in the number of agents compared with the exponential growth that challenges the original framework. while exploiting structure helps mitigate the curse of many agents, the well-known curse of history that afflicts i-pomdps continues to challenge scalability in terms of the planning horizon. the second contribution of this article is an application of the branch-and-bound scheme to reduce the exponential growth of the search tree for look ahead. for this, we introduce new fast-computing upper and lower bounds for the exact value function of the many-agent i-pomdp. this speeds up the look-ahead computations without trading off optimality, and reduces both memory and run time complexity. the third contribution is a comprehensive empirical evaluation of the methods on three new problems domains -- policing large protests, controlling traffic congestion at a busy intersection, and improving the ai for the popular clash of clans multiplayer game. we demonstrate the feasibility of exact self-interested planning in these large problems, and that our methods for speeding up the planning are effective. altogether, these contributions represent a principled and significant advance toward moving self-interested planning under uncertainty to real-world applications.",2017
uniform random generation and dominance testing for cp-nets,"the generation of preferences represented as cp-nets for experiments and empirical testing has typically been done in an ad hoc manner that may have introduced a large statistical bias in previous experimental work. we present novel polynomial-time algorithms for generating cp-nets with n nodes and maximum in-degree c uniformly at random. we extend this result to several statistical cultures commonly used in the social choice and preference reasoning literature. a cp-net is composed of both a graph and underlying cp-statements; our algorithm is the first to provably generate both the graph structure and cp-statements, and hence the underlying preference orders themselves, uniformly at random. we have released this code as a free and open source project. we use the uniform generation algorithm to investigate the maximum and expected flipping lengths, i.e., the maximum length over all outcomes o and o', of a minimal proof that o is preferred to o'. using our new statistical evidence, we conjecture that, for cp-nets with binary variables and complete conditional preference tables, the expected flipping length is polynomial in the number of preference variables. this has positive implications for the usability of cp-nets as compact preference models.",2017
complexity of n-queens completion,"the n-queens problem is to place n chess queens on an n by n chessboard so that no two queens are on the same row, column or diagonal. the n-queens completion problem is a variant, dating to 1850, in which some queens are already placed and the solver is asked to place the rest, if possible. we show that n-queens completion is both np-complete and #p-complete. a corollary is that any non-attacking arrangement of queens can be included as a part of a solution to a larger n-queens problem. we introduce generators of random instances for n-queens completion and the closely related blocked n-queens and excluded diagonals problem. we describe three solvers for these problems, and empirically analyse the hardness of randomly generated instances. for blocked n-queens and the excluded diagonals problem, we show the existence of a phase transition associated with hard instances as has been seen in other np-complete problems, but a natural generator for n-queens completion did not generate consistently hard instances. the significance of this work is that the n-queens problem has been very widely used as a benchmark in artificial intelligence, but conclusions on it are often disputable because of the simple complexity of the decision problem. our results give alternative benchmarks which are hard theoretically and empirically, but for which solving techniques designed for n-queens need minimal or no change.",2017
probabilistic description logics for subjective uncertainty,"we propose a family of probabilistic description logics (dls) that are derived in a principled way from halpern's probabilistic first-order logic. the resulting probabilistic dls have a two-dimensional semantics similar to temporal dls and are well-suited for representing subjective probabilities. we carry out a detailed study of reasoning in the new family of logics, concentrating on probabilistic extensions of the dls alc and el, and showing that the complexity ranges from ptime via exptime and 2exptime to undecidable.",2017
tie-breaking strategies for cost-optimal best first search,"best-first search algorithms such as a* need to apply tie-breaking strategies in order to decide which node to expand when multiple search nodes have the same evaluation score. we investigate and improve tie-breaking strategies for cost-optimal search using a*. we first experimentally analyze the performance of common tie-breaking strategies that break ties according to the heuristic value of the nodes. we find that the tie-breaking strategy has a significant impact on search algorithm performance when there are 0-cost operators that induce large plateau regions in the search space. based on this, we develop two new classes of tie-breaking strategies. we first propose a depth diversification strategy which breaks ties according to the distance from the entrance to the plateau, and then show that this new strategy significantly outperforms standard strategies on domains with 0-cost actions. next, we propose a new framework for interpreting a* search as a series of satisficing searches within plateaus consisting of nodes with the same f-cost. based on this framework, we investigate a second, new class of tie-breaking strategy, a multi-heuristic tie-breaking strategy which embeds inadmissible, distance-to-go variations of various heuristics within an admissible search. this is shown to further improve the performance in combination with the depth metric.",2017
subset selection via implicit utilitarian voting,"how should one aggregate ordinal preferences expressed by voters into a measurably superior social choice? a well-established approach -- which we refer to as implicit utilitarian voting -- assumes that voters have latent utility functions that induce the reported rankings, and seeks voting rules that approximately maximize utilitarian social welfare. we extend this approach to the design of rules that select a subset of alternatives. we derive analytical bounds on the performance of optimal (deterministic as well as randomized) rules in terms of two measures, distortion and regret. empirical results show that regret-based rules are more compelling than distortion-based rules, leading us to focus on developing a scalable implementation for the optimal (deterministic) regret-based rule. our methods underlie the design and implementation of robovote.org, a not-for-profit website that helps users make group decisions via ai-driven voting methods.",2017
controlled school choice with soft bounds and overlapping types,"school choice programs are implemented to give students/parents an opportunity to choose the public school the students attend. controlled school choice programs need to provide choices for students/parents while maintaining distributional constraints on the composition of students, typically in terms of socioeconomic status. previous works show that setting soft-bounds, which flexibly change the priorities of students based on their types, is more appropriate than setting hard-bounds, which strictly limit the number of accepted students for each type. we consider a case where soft-bounds are imposed and one student can belong to multiple types, e.g., financially-distressed and minority types. we first show that when we apply a model that is a straightforward extension of an existing model for disjoint types, there is a chance that no stable matching exists. thus we propose an alternative model and an alternative stability definition, where a school has reserved seats for each type. we show that a stable matching is guaranteed to exist in this model and develop a mechanism called deferred acceptance for overlapping types (da-ot). the da-ot mechanism is strategy-proof and obtains the student-optimal matching within all stable matchings. furthermore, we introduce an extended model that can handle both type-specific ceilings and floors and propose a extended mechanism da-ot* to handle the extended model. computer simulation results illustrate that da-ot outperforms an artificial cap mechanism where we set a hard-bound for each type in each school. da-ot* can achieve stability in the extended model without sacrificing students welfare.",2017
"bayesian network structure learning with integer programming: polytopes, facets and complexity","the challenging task of learning structures of probabilistic graphical models is an important problem within modern ai research. recent years have witnessed several major algorithmic advances in structure learning for bayesian networks - arguably the most central class of graphical models - especially in what is known as the score-based setting. a successful generic approach to optimal bayesian network structure learning (bnsl), based on integer programming (ip), is implemented in the gobnilp system. despite the recent algorithmic advances, current understanding of foundational aspects underlying the ip based approach to bnsl is still somewhat lacking. understanding fundamental aspects of cutting planes and the related separation problem is important not only from a purely theoretical perspective, but also since it holds out the promise of further improving the efficiency of state-of-the-art approaches to solving bnsl exactly. in this paper, we make several theoretical contributions towards these goals: (i) we study the computational complexity of the separation problem, proving that the problem is np-hard; (ii) we formalise and analyse the relationship between three key polytopes underlying the ip-based approach to bnsl; (iii) we study the facets of the three polytopes both from the theoretical and practical perspective, providing, via exhaustive computation, a complete enumeration of facets for low-dimensional family-variable polytopes; and, furthermore, (iv) we establish a tight connection of the bnsl problem to the acyclic subgraph problem.",2017
despot: online pomdp planning with regularization,"the partially observable markov decision process (pomdp) provides a principled general framework for planning under uncertainty, but solving pomdps optimally is computationally intractable, due to the ""curse of dimensionality"" and the ""curse of history"". to overcome these challenges, we introduce the determinized sparse partially observable tree (despot), a sparse approximation of the standard belief tree, for online planning under uncertainty. a despot focuses online planning on a set of randomly sampled scenarios and compactly captures the ""execution"" of all policies under these scenarios. we show that the best policy obtained from a despot is near-optimal, with a regret bound that depends on the representation size of the optimal policy. leveraging this result, we give an anytime online planning algorithm, which searches a despot for a policy that optimizes a regularized objective function. regularization balances the estimated value of a policy under the sampled scenarios and the policy size, thus avoiding overfitting. the algorithm demonstrates strong experimental results, compared with some of the best online pomdp algorithms available. it has also been incorporated into an autonomous driving system for real-time vehicle control. the source code for the algorithm is available online.",2017
local search for minimum weight dominating set with two-level configuration checking and frequency based scoring function,"the minimum weight dominating set (mwds) problem is an important generalization of the minimum dominating set (mds) problem with extensive applications. this paper proposes a new local search algorithm for the mwds problem, which is based on two new ideas. the first idea is a heuristic called two-level configuration checking (cc2), which is a new variant of a recent powerful configuration checking strategy (cc) for effectively avoiding the recent search paths. the second idea is a novel scoring function based on the frequency of being uncovered of vertices. our algorithm is called cc2fs, according to the names of the two ideas. the experimental results show that, cc2fs performs much better than some state-of-the-art algorithms in terms of solution quality on a broad range of mwds benchmarks.",2017
computational aspects of nearly single-peaked electorates,"manipulation, bribery, and control are well-studied ways of changing the outcome of an election. many voting rules are, in the general case, computationally resistant to some of these manipulative actions. however when restricted to single-peaked electorates, these rules suddenly become easy to manipulate. recently, faliszewski, hemaspaandra, and hemaspaandra studied the computational complexity of strategic behavior in nearly single-peaked electorates. these are electorates that are not single-peaked but close to it according to some distance measure. in this paper we introduce several new distance measures regarding single-peakedness. we prove that determining whether a given profile is nearly single-peaked is np-complete in many cases. for one case we present a polynomial-time algorithm. in case the single-peaked axis is given, we show that determining the distance is always possible in polynomial time. furthermore, we explore the relations between the new notions introduced in this paper and existing notions from the literature.",2017
a model-theoretic view on qualitative constraint reasoning,"qualitative reasoning formalisms are an active research topic in artificial intelligence. in this survey we present a model-theoretic perspective on qualitative constraint reasoning and explain some of the basic concepts and results in an accessible way. in particular, we discuss the significance of omega-categoricity for qualitative reasoning, of primitive positive interpretations for complexity analysis, and of datalog as a unifying language for describing local consistency algorithms.",2017
dynamic repositioning to reduce lost demand in bike sharing systems,"bike sharing systems (bsss) are widely adopted in major cities of the world due to concerns associated with extensive private vehicle usage, namely, increased carbon emissions, traffic congestion and usage of nonrenewable resources. in a bss, base stations are strategically placed throughout a city and each station is stocked with a pre-determined number of bikes at the beginning of the day. customers hire the bikes from one station and return them at another station. due to unpredictable movements of customers hiring bikes, there is either congestion (more than required) or starvation (fewer than required) of bikes at base stations. existing data has shown that congestion/starvation is a common phenomenon that leads to a large number of unsatisfied customers resulting in a significant loss in customer demand. in order to tackle this problem, we propose an optimisation formulation to reposition bikes using vehicles while also considering the routes for vehicles and future expected demand. furthermore, we contribute two approaches that rely on decomposability in the problem (bike repositioning and vehicle routing) and aggregation of base stations to reduce the computation time significantly. finally, we demonstrate the utility of our approach by comparing against two benchmark approaches on two real-world data sets of bike sharing systems. these approaches are evaluated using a simulation where the movements of customers are generated from real-world data sets.",2017
the computational complexity of structure-based causality,"halpern and pearl introduced a definition of actual causality; eiter and lukasiewicz showed that computing whether x = x is a cause of y = y is np-complete in binary models (where all variables can take on only two values) and &#931;^p_2 -complete in general models. in the final version of their paper, halpern and pearl slightly modified the definition of actual cause, in order to deal with problems pointed out by hopkins and pearl. as we show, this modification has a nontrivial impact on the complexity of computing whether {x} = {x} is a cause of y = y. to characterize the complexity, a new family d_k^p , k = 1, 2, 3, . . ., of complexity classes is introduced, which generalises the class dp introduced by papadimitriou and yannakakis (dp is just d_1^p). we show that the complexity of computing causality under the updated definition is d_2^p -complete. chockler and halpern extended the definition of causality by introducing notions of responsibility and blame, and characterized the complexity of determining the degree of responsibility and blame using the original definition of causality. here, we completely characterize the complexity using the updated definition of causality. in contrast to the results on causality, we show that moving to the updated definition does not result in a difference in the complexity of computing responsibility and blame.",2017
new canonical representations by augmenting obdds with conjunctive decomposition,"we identify two families of canonical knowledge compilation languages. both families augment robdd with conjunctive decomposition bounded by an integer i ranging from 0 to &infin;. in the former, the decomposition is finest and the decision respects a chain c of variables, while both the decomposition and decision of the latter respect a tree t of variables. in particular, these two families cover the three existing languages robdd, robdd with as many implied literals as possible, and and/or bdd. we demonstrate that each language in the first family is complete, while each one in the second family is incomplete with expressivity that does not decrease with incremental i. we also demonstrate that the succinctness does not decrease from the i-th language in the second family to the i-th language in the first family, and then to the (i+1)-th language in the first family. for the operating efficiency, on the one hand, we show that the two families of languages support a rich class of tractable logical operations, and particularly the tractability of each language in the second family is not less than that of robdd; and on the other hand, we introduce a new time efficiency criterion called rapidity which reflects the idea that exponential operations may be preferable if the language can be exponentially more succinct, and we demonstrate that the rapidity of each operation does not decrease from the i-th language in the second family to the i-th language in the first family, and then to the (i+1)-th language in the first family. furthermore, we develop a compiler for the last language in the first family (i = &infin;). empirical results show that the compiler significantly advances the compiling efficiency of canonical representations. in fact, its compiling efficiency is comparable with that of the state-of-the-art compilers of non-canonical representations. we also provide a compiler for the i-th language in the first family by translating the last language in the first family into the i-th language (i < &infin;). empirical results show that we can sometimes use the i-th language instead of the last language without any obvious loss of space efficiency.",2017
robots in retirement homes: applying off-the-shelf planning and scheduling to a team of assistive robots,"this paper investigates three different technologies for solving a planning and scheduling problem of deploying multiple robots in a retirement home environment to assist elderly residents. the models proposed make use of standard techniques and solvers developed in ai planning and scheduling, with two primary motivations. first, to find a planning and scheduling solution that we can deploy in our real-world application. second, to evaluate planning and scheduling technology in terms of the ``model-and-solve'' functionality that forms a major research goal in both domain-independent planning and constraint programming. seven variations of our application are studied using the following three technologies: pddl-based planning, time-line planning and scheduling, and constraint-based scheduling. the variations address specific aspects of the problem that we believe can impact the performance of the technologies while also representing reasonable abstractions of the real world application. we evaluate the capabilities of each technology and conclude that a constraint-based scheduling approach, specifically a decomposition using constraint programming, provides the most promising results for our application. pddl-based planning is able to find mostly low quality solutions while the timeline approach was unable to model the full problem without alterations to the solver code, thus moving away from the model-and-solve paradigm. it would be misleading to conclude that constraint programming is ``better'' than pddl-based planning in a general sense, both because we have examined a single application and because the approaches make different assumptions about the knowledge one is allowed to embed in a model. nonetheless, we believe our investigation is valuable for ai planning and scheduling researchers as it highlights these different modelling assumptions and provides insight into avenues for the application of ai planning and scheduling for similar robotics problems. in particular, as constraint programming has not been widely applied to robot planning and scheduling in the literature, our results suggest significant untapped potential in doing so.",2017
explicit document modeling through weighted multiple-instance learning,"representing documents is a crucial component in many nlp tasks, for instance predicting aspect ratings in reviews. previous methods for this task treat documents globally, and do not acknowledge that target categories are often assigned by their authors with generally no indication of the specific sentences that motivate them. to address this issue, we adopt a weakly supervised learning model, which jointly learns to focus on relevant parts of a document according to the context along with a classifier for the target categories. derived from the weighted multiple-instance regression (mir) framework, the model learns decomposable document vectors for each individual category and thus overcomes the representational bottleneck in previous methods due to a fixed-length document vector. during prediction, the estimated relevance or saliency weights explicitly capture the contribution of each sentence to the predicted rating, thus offering an explanation of the rating. our model achieves state-of-the-art performance on multi-aspect sentiment analysis, improving over several baselines. moreover, the predicted saliency weights are close to human estimates obtained by crowdsourcing, and increase the performance of lexical and topical features for review segmentation and summarization.",2017
a probabilistic formalization of the appraisal for the occ event-based emotions,"this article presents a logical formalization of the emotional appraisal theory, i.e., it formalizes the cognitive process of evaluation that elicits an emotion. this formalization is psychologically grounded on the occ cognitive model of emotions. more specifically, we are interested in event-based emotions, i.e., emotions that are elicited by the evaluation of the consequences of an event that either happened or will happen. the formal modelling presented here is based on the afpl probabilistic logic, a bdi-like probabilistic modal logic, which allows our model to verify whether the variables that determine the elicitation of emotions achieved the necessary threshold or not. the proposed logical formalization aims at addressing how the emotions are elicited by the agent cognitive mental states (desires, beliefs and intentions), and how to represent the intensity of the emotions. these are important initial points in the investigation of the dynamic interaction among emotions and other mental states.",2017
combinatorial multi-armed bandits for real-time strategy games,"games with large branching factors pose a significant challenge for game tree search algorithms. in this paper, we address this problem with a sampling strategy for monte carlo tree search (mcts) algorithms called ""naive sampling"", based on a variant of the multi-armed bandit problem called ""combinatorial multi-armed bandits"" (cmab). we analyze the theoretical properties of several variants of naive sampling, and empirically compare it against the other existing strategies in the literature for cmabs. we then evaluate these strategies in the context of real-time strategy (rts) games, a genre of computer games characterized by their very large branching factors. our results show that as the branching factor grows, naive sampling outperforms the other sampling strategies.",2017
a neural probabilistic structured-prediction method for transition-based natural language processing,"we propose a neural probabilistic structured-prediction method for transition-based natural language processing, which integrates beam search and contrastive learning. the method uses a global optimization model, which can leverage arbitrary features over non-local context. beam search is used for efficient heuristic decoding, and contrastive learning is performed for adjusting the model according to search errors. when evaluated on both chunking and dependency parsing tasks, the proposed method achieves significant accuracy improvements over the locally normalized greedy baseline on the two tasks, respectively.",2017
managing different sources of uncertainty in a bdi framework in a principled way with tractable fragments,"the belief-desire-intention (bdi) architecture is a practical approach for modelling large-scale intelligent systems. in the bdi setting, a complex system is represented as a network of interacting agents - or components - each one modelled based on its beliefs, desires and intentions. however, current bdi implementations are not well-suited for modelling more realistic intelligent systems which operate in environments pervaded by different types of uncertainty. furthermore, existing approaches for dealing with uncertainty typically do not offer syntactical or tractable ways of reasoning about uncertainty. this complicates their integration with bdi implementations, which heavily rely on fast and reactive decisions. in this paper, we advance the state-of-the-art w.r.t. handling different types of uncertainty in bdi agents. the contributions of this paper are, first, a new way of modelling the beliefs of an agent as a set of epistemic states. each epistemic state can use a distinct underlying uncertainty theory and revision strategy, and commensurability between epistemic states is achieved through a stratification approach. second, we present a novel syntactic approach to revising beliefs given unreliable input. we prove that this syntactic approach agrees with the semantic definition, and we identify expressive fragments that are particularly useful for resource-bounded agents. third, we introduce full operational semantics that extend can, a popular semantics for bdi, to establish how reasoning about uncertainty can be tightly integrated into the bdi framework. fourth, we provide comprehensive experimental results to highlight the usefulness and feasibility of our approach, and explain how the generic epistemic state can be instantiated into various representations.",2017
some properties of batch value of information in the selection problem,"given a set of items of unknown utility, we need to select one with a utility as high as possible (the selection problem). measurements (possibly noisy) of item values prior to selection are allowed, at a known cost. the goal is to optimize the overall sequential decision process of measurements and selection. value of information (voi) is a well-known scheme for selecting measurements, but the intractability of the problem typically leads to using myopic voi estimates. other schemes have also been proposed, some with approximation guarantees, based on submodularity criteria. however, it was observed that the voi is not submodular in general. in this paper we examine theoretical properties of voi for the selection problem, and identify cases of submodularity and supermodularity. we suggest how to use these properties to compute approximately optimal measurement batch policies, with an example based on a wine selection problem.",2017
randomized social choice functions under metric preferences,"we determine the quality of randomized social choice algorithms in a setting in which the agents have metric preferences: every agent has a cost for each alternative, and these costs form a metric. we assume that these costs are unknown to the algorithms (and possibly even to the agents themselves), which means we cannot simply select the optimal alternative, i.e. the alternative that minimizes the total agent cost (or median agent cost). however, we do assume that the agents know their ordinal preferences that are induced by the metric space. we examine randomized social choice functions that require only this ordinal information and select an alternative that is good in expectation with respect to the costs from the metric. to quantify how good a randomized social choice function is, we bound the distortion, which is the worst-case ratio between the expected cost of the alternative selected and the cost of the optimal alternative. we provide new distortion bounds for a variety of randomized algorithms, for both general metrics and for important special cases. our results show a sizable improvement in distortion over deterministic algorithms.",2017
improving the efficiency of dynamic programming on tree decompositions via machine learning,"dynamic programming (dp) over tree decompositions is a well-established method to solve problems - that are in general np-hard - efficiently for instances of small treewidth. experience shows that (i) heuristically computing a tree decomposition has negligible runtime compared to the dp step; and (ii) dp algorithms exhibit a high variance in runtime when using different tree decompositions; in fact, given an instance of the problem at hand, even decompositions of the same width might yield extremely diverging runtimes. we thus propose here a novel and general method that is based on selection of the best decomposition from an available pool of heuristically generated ones. for this purpose, we require machine learning techniques that provide automated selection based on features of the decomposition rather than on the actual problem instance. thus, one main contribution of this work is to propose novel features for tree decompositions. moreover, we report on extensive experiments in different problem domains which show a significant speedup when choosing the tree decomposition according to this concept over simply using an arbitrary one of the same width.",2017
logics of common ground,"according to clark's seminal work on common ground and grounding, participants collaborating in a joint activity rely on their shared information, known as common ground, to perform that activity successfully, and continually align and augment this information during their collaboration. similarly, teams of human and artificial agents require common ground to successfully participate in joint activities. indeed, without appropriate information being shared, using agent autonomy to reduce the workload on humans may actually increase workload as the humans seek to understand why the agents are behaving as they are. while many researchers have identified the importance of common ground in artificial intelligence, there is no precise definition of common ground on which to build the foundational aspects of multi-agent collaboration. in this paper, building on previously-defined modal logics of belief, we present logic definitions for four different types of common ground. we define modal logics for three existing notions of common ground and introduce a new notion of common ground, called salient common ground. salient common ground captures the common ground of a group participating in an activity and is based on the common ground that arises from that activity as well as on the common ground they shared prior to the activity. we show that the four definitions share some properties, and our analysis suggests possible refinements of the existing informal and semi-formal definitions.",2017
encoding domain transitions for constraint-based planning,we describe a constraint-based automated planner named transition constraints for parallel planning (tcpp). tcpp constructs its constraint model from a redefined version of the domain transition graphs (dtg) of a given planning problem. tcpp encodes state transitions in the redefined dtgs by using table constraints with cells containing don't cares or wild cards. tcpp uses minion the constraint solver to solve the constraint model and returns a parallel plan. we empirically compare tcpp with the other state-of-the-art constraint-based parallel planner pap2. pap2 encodes action successions in the finite state automata (fsa) as table constraints with cells containing sets of values. pap2 uses sicstus prolog as its constraint solver. we also improve pap2 by using dont cares and mutex constraints. our experiments on a number of standard classical planning benchmark domains demonstrate tcpp's efficiency over the original pap2 running on sicstus prolog and our reconstructed and enhanced versions of pap2 running on minion.,2017
learning continuous time bayesian networks in non-stationary domains,"non-stationary continuous time bayesian networks are introduced. they allow the parents set of each node to change over continuous time. three settings are developed for learning non-stationary continuous time bayesian networks from data: known transition times, known number of epochs and unknown number of epochs. a score function for each setting is derived and the corresponding learning algorithm is developed. a set of numerical experiments on synthetic data is used to compare the effectiveness of non-stationary continuous time bayesian networks to that of non-stationary dynamic bayesian networks. furthermore, the performance achieved by non-stationary continuous time bayesian networks is compared to that achieved by state-of-the-art algorithms on four real-world datasets, namely drosophila, saccharomyces cerevisiae, songbird and macroeconomics.",2016
pdt logic: a probabilistic doxastic temporal logic for reasoning about beliefs in multi-agent systems,"we present probabilistic doxastic temporal (pdt) logic, a formalism to represent and reason about probabilistic beliefs and their temporal evolution in multi-agent systems. this formalism enables the quantification of agents beliefs through probability intervals and incorporates an explicit notion of time. we discuss how over time agents dynamically change their beliefs in facts, temporal rules, and other agents beliefs with respect to any new information they receive. we introduce an appropriate formal semantics for pdt logic and show that it is decidable. alternative options of specifying problems in pdt logic are possible. for these problem specifications, we develop different satisfiability checking algorithms and provide complexity results for the respective decision problems. the use of probability intervals enables a formal representation of probabilistic knowledge without enforcing (possibly incorrect) exact probability values. by incorporating an explicit notion of time, pdt logic provides enriched possibilities to represent and reason about temporal relations.",2016
optimal partial-order plan relaxation via maxsat,"partial-order plans (pops) are attractive because of their least-commitment nature, which provides enhanced plan flexibility at execution time relative to sequential plans. current research on automated plan generation focuses on producing sequential plans, despite the appeal of pops. in this paper we examine pop generation by relaxing or modifying the action orderings of a sequential plan to optimize for plan criteria that promote flexibility. our approach relies on a novel partial weighted maxsat encoding of a sequential plan that supports the minimization of deordering or reordering of actions. using a similar technique, we further demonstrate how to remove redundant actions from the plan, and how to combine this criterion with the objective of maximizing a pop's flexibility. our partial weighted maxsat encoding allows us to compute a pop from a sequential plan effectively. we compare the efficiency of our approach to previous methods for pop generation via sequential-plan relaxation. our results show that while an existing heuristic approach consistently produces the optimal deordering of a sequential plan, our approach has greater flexibility when we consider reordering the actions in the plan while also providing a guarantee of optimality. we also investigate and confirm the accuracy of the standard flex metric typically used to predict the true flexibility of a pop as measured by the number of linearizations it represents.",2016
lightweight random indexing for polylingual text classification,"multilingual text classification (mltc) is a text classification task in which documents are written each in one among a set l of natural languages, and in which all documents must be classified under the same classification scheme, irrespective of language. there are two main variants of mltc, namely cross-lingual text classification (cltc) and polylingual text classification (pltc). in pltc, which is the focus of this paper, we assume (differently from cltc) that for each language in l there is a representative set of training documents; pltc consists of improving the accuracy of each of the |l| monolingual classifiers by also leveraging the training documents written in the other (|l| &#8722; 1) languages. the obvious solution, consisting of generating a single polylingual classifier from the juxtaposed monolingual vector spaces, is usually infeasible, since the dimensionality of the resulting vector space is roughly |l| times that of a monolingual one, and is thus often unmanageable. as a response, the use of machine translation tools or multilingual dictionaries has been proposed. however, these resources are not always available, or are not always free to use. one machine-translation-free and dictionary-free method that, to the best of our knowledge, has never been applied to pltc before, is random indexing (ri). we analyse ri in terms of space and time efficiency, and propose a particular configuration of it (that we dub lightweight random indexing  lri). by running experiments on two well known public benchmarks, reuters rcv1/rcv2 (a comparable corpus) and jrc-acquis (a parallel one), we show lri to outperform (both in terms of effectiveness and efficiency) a number of previously proposed machine-translation-free and dictionary-free pltc methods that we use as baselines.",2016
multi-objective reinforcement learning through continuous pareto manifold approximation,"many real-world control applications, from economics to robotics, are characterized by the presence of multiple conflicting objectives. in these problems, the standard concept of optimality is replaced by pareto-optimality and the goal is to find the pareto frontier, a set of solutions representing different compromises among the objectives. despite recent advances in multi-objective optimization, achieving an accurate representation of the pareto frontier is still an important challenge. in this paper, we propose a reinforcement learning policy gradient approach to learn a continuous approximation of the pareto frontier in multi-objective markov decision problems (momdps). differently from previous policy gradient algorithms, where n optimization routines are executed to have n solutions, our approach performs a single gradient ascent run, generating at each step an improved continuous approximation of the pareto frontier. the idea is to optimize the parameters of a function defining a manifold in the policy parameters space, so that the corresponding image in the objectives space gets as close as possible to the true pareto frontier. besides deriving how to compute and estimate such gradient, we will also discuss the non-trivial issue of defining a metric to assess the quality of the candidate pareto frontiers. finally, the properties of the proposed approach are empirically evaluated on two problems, a linear-quadratic gaussian regulator and a water reservoir control task.",2016
goal probability analysis in probabilistic planning: exploring and enhancing the state of the art,"unavoidable dead-ends are common in many probabilistic planning problems, e.g. when actions may fail or when operating under resource constraints. an important objective in such settings is maxprob, determining the maximal probability with which the goal can be reached, and a policy achieving that probability. yet algorithms for maxprob probabilistic planning are severely underexplored, to the extent that there is scant evidence of what the empirical state of the art actually is. we close this gap with a comprehensive empirical analysis. we design and explore a large space of heuristic search algorithms, systematizing known algorithms and contributing several new algorithm variants. we consider maxprob, as well as weaker objectives that we baptize atleastprob (requiring to achieve a given goal probabilty threshold) and approxprob (requiring to compute the maximum goal probability up to a given accuracy). we explore both the general case where there may be 0-reward cycles, and the practically relevant special case of acyclic planning, such as planning with a limited action-cost budget. we design suitable termination criteria, search algorithm variants, dead-end pruning methods using classical planning heuristics, and node selection strategies. we design a benchmark suite comprising more than 1000 instances adapted from the ippc, resource-constrained planning, and simulated penetration testing. our evaluation clarifies the state of the art, characterizes the behavior of a wide range of heuristic search algorithms, and demonstrates significant benefits of our new algorithm variants.",2016
effective heuristics for suboptimal best-first search,"suboptimal heuristic search algorithms such as weighted a* and greedy best-first search are widely used to solve problems for which guaranteed optimal solutions are too expensive to obtain. these algorithms crucially rely on a heuristic function to guide their search. however, most research on building heuristics addresses optimal solving. in this paper, we illustrate how established wisdom for constructing heuristics for optimal search can fail when considering suboptimal search. we consider the behavior of greedy best-first search in detail and we test several hypotheses for predicting when a heuristic will be effective for it. our results suggest that a predictive characteristic is a heuristic's goal distance rank correlation (gdrc), a robust measure of whether it orders nodes according to distance to a goal. we demonstrate that gdrc can be used to automatically construct abstraction-based heuristics for greedy best-first search that are more effective than those built by methods oriented toward optimal search. these results reinforce the point that suboptimal search deserves sustained attention and specialized methods of its own.",2016
scrubbing during learning in real-time heuristic search,"real-time agent-centered heuristic search is a well-studied problem where an agent that can only reason locally about the world must travel to a goal location using bounded computation and memory at each step. many algorithms have been proposed for this problem and theoretical results have also been derived for the worst-case performance with simple examples demonstrating worst-case performance in practice. lower bounds, however, have not been widely studied. in this paper we study best-case performance more generally and derive theoretical lower bounds for reaching the goal using lrta*, a canonical example of a real-time agent-centered heuristic search algorithm. the results show that, given some reasonable restrictions on the state space and the heuristic function, the number of steps an lrta*-like algorithm requires to reach the goal will grow asymptotically faster than the state space, resulting in ``scrubbing'' where the agent repeatedly visits the same state. we then show that while the asymptotic analysis does not hold for more complex real-time search algorithms, experimental results suggest that it is still descriptive of practical performance.",2016
a primer on neural network models for natural language processing,"over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. more recently, neural network models started to be applied also to textual natural language signals, again with very promising results. this tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. the tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.",2016
embarrassingly parallel search in constraint programming,"we introduce an embarrassingly parallel search (eps) method for solving constraint problems in parallel, and we show that this method matches or even outperforms state-of-the-art algorithms on a number of problems using various computing infrastructures. eps is a simple method in which a master decomposes the problem into many disjoint subproblems which are then solved independently by workers. our approach has three advantages: it is an efficient method; it involves almost no communication or synchronization between workers; and its implementation is made easy because the master and the workers rely on an underlying constraint solver, but does not require to modify it. this paper describes the method, and its applications to various constraint problems (satisfaction, enumeration, optimization). we show that our method can be adapted to different underlying solvers (gecode, choco2, or-tools) on different computing infrastructures (multi-core, data centers, cloud computing). the experiments cover unsatisfiable, enumeration and optimization problems, but do not cover first solution search because it makes the results hard to analyze. the same variability can be observed for optimization problems, but at a lesser extent because the optimality proof is required. eps offers good average performance, and matches or outperforms other available parallel implementations of gecode as well as some solvers portfolios. moreover, we perform an in-depth analysis of the various factors that make this approach efficient as well as the anomalies that can occur. last, we show that the decomposition is a key component for efficiency and load balancing.",2016
promoca: probabilistic modeling and analysis of agents in commitment protocols,"social commitment protocols regulate interactions of agents in multiagent systems. several methods have been developed to analyze properties of commitment protocols. however, analysis of an agent's behavior in a commitment protocol, which should take into account the agent's goals and beliefs, has received less attention. in this paper we present promoca framework to address this issue. firstly, we develop an expressive formal language to model agents with respect to their commitments. our language provides dedicated elements to define commitment protocols, and model agents in terms of their goals, behaviors, and beliefs. furthermore, our language provides probabilistic and non-deterministic elements to model uncertainty in agents' beliefs. secondly, we identify two essential properties of an agent with respect to a commitment protocol, namely compliance and goal satisfaction. we formalize these properties using a probabilistic variant of linear temporal logic. thirdly, we adapt a probabilistic model checking algorithm to automatically analyze compliance and goal satisfaction properties. finally, we present empirical results about efficiency and scalability of promoca.",2016
a survey of computational treatments of biomolecules by robotics-inspired methods modeling equilibrium structure and dynamic,"more than fifty years of research in molecular biology have demonstrated that the ability of small and large molecules to interact with one another and propagate the cellular processes in the living cell lies in the ability of these molecules to assume and switch between specific structures under physiological conditions. elucidating biomolecular structure and dynamics at equilibrium is therefore fundamental to furthering our understanding of biological function, molecular mechanisms in the cell, our own biology, disease, and disease treatments. by now, there is a wealth of methods designed to elucidate biomolecular structure and dynamics contributed from diverse scientific communities. in this survey, we focus on recent methods contributed from the robotics community that promise to address outstanding challenges regarding the disparate length and time scales that characterize dynamic molecular processes in the cell. in particular, we survey robotics-inspired methods designed to obtain efficient representations of structure spaces of molecules in isolation or in assemblies for the purpose of characterizing equilibrium structure and dynamics. while an exhaustive review is an impossible endeavor, this survey balances the description of important algorithmic contributions with a critical discussion of outstanding computational challenges. the objective is to spur further research to address outstanding challenges in modeling equilibrium biomolecular structure and dynamics.",2016
convergence of iterative scoring rules,"in multiagent systems, social choice functions can help aggregate the distinct preferences that agents have over alternatives, enabling them to settle on a single choice. despite the basic manipulability of all reasonable voting systems, it would still be desirable to find ways to reach plausible outcomes, which are stable states, i.e., a situation where no agent would wish to change its vote. one possibility is an iterative process in which, after everyone initially votes, participants may change their votes, one voter at a time. this technique, explored in previous work, converges to a nash equilibrium when plurality voting is used, along with a tie-breaking rule that chooses a winner according to a linear order of preferences over candidates. in this paper, we both consider limitations of the iterative voting method, as well as expanding upon it. we demonstrate the significance of tie-breaking rules, showing that no iterative scoring rule converges for all tie-breaking. however, using a restricted tie-breaking rule (such as the linear order rule used in previous work) does not by itself ensure convergence. we prove that in addition to plurality, the veto voting rule converges as well using a linear order tie-breaking rule. however, we show that these two voting rules are the only scoring rules that converge, regardless of tie-breaking mechanism.",2016
zero++: harnessing the power of zero appearances to detect anomalies in large-scale data sets,"this paper introduces a new unsupervised anomaly detector called zero++ which employs the number of zero appearances in subspaces to detect anomalies in categorical data. it is unique in that it works in regions of subspaces that are not occupied by data; whereas existing methods work in regions occupied by data. zero++ examines only a small number of low dimensional subspaces to successfully identify anomalies. unlike existing frequency-based algorithms, zero++ does not involve subspace pattern searching. we show that zero++ is better than or comparable with the state-of-the-art anomaly detection methods over a wide range of real-world categorical and numeric data sets; and it is efficient with linear time complexity and constant space complexity which make it a suitable candidate for large-scale data sets.",2016
p-syncbb: a privacy preserving branch and bound dcop algorithm,"distributed constraint optimization problems enable the representation of many combinatorial problems that are distributed by nature. an important motivation for such problems is to preserve the privacy of the participating agents during the solving process. the present paper introduces a novel privacy-preserving branch and bound algorithm for this purpose. the proposed algorithm, p-syncbb, preserves constraint, topology and decision privacy. the algorithm requires secure solutions to several multi-party computation problems. consequently, appropriate novel secure protocols are devised and analyzed. an extensive experimental evaluation on different benchmarks, problem sizes, and constraint densities shows that p-syncbb exhibits superior performance to other privacy-preserving complete dcop algorithms.",2016
a disaster response system based on human-agent collectives,"major natural or man-made disasters such as hurricane katrina or the 9/11 terror attacks pose significant challenges for emergency responders. first, they have to develop an understanding of the unfolding event either using their own resources or through third-parties such as the local population and agencies. second, based on the information gathered, they need to deploy their teams in a flexible manner, ensuring that each team performs tasks in the most effective way. third, given the dynamic nature of a disaster space, and the uncertainties involved in performing rescue missions, information about the disaster space and the actors within it needs to be managed to ensure that responders are always acting on up-to-date and trusted information. against this background, this paper proposes a novel disaster response system called hac-er. thus hac-er interweaves humans and agents, both robotic and software, in social relationships that augment their individual and collective capabilities. to design hac-er, we involved end-users including both experts and volunteers in a several participatory design workshops, lab studies, and field trials of increasingly advanced prototypes of individual components of hac-er as well as the overall system. this process generated a number of new quantitative and qualitative results but also raised a number of new research questions. hac-er thus demonstrates how such human-agent collectives (hacs) can address key challenges in disaster response. specifically, we show how hac-er utilises crowdsourcing combined with machine learning to obtain most important situational awareness from large streams of reports posted by members of the public and trusted organisations. we then show how this information can inform human-agent teams in coordinating multi-uav deployments, as well as task planning for responders on the ground. finally, hac-er incorporates an infrastructure and the associated intelligence for tracking and utilising the provenance of information shared across the entire system to ensure its accountability. we individually validate each of these elements of hac-er and show how they perform against standard (non-hac) baselines and also elaborate on the evaluation of the overall system.",2016
query and predicate emptiness in ontology-based data access,"in ontology-based data access (obda), database querying is enriched with an ontology that provides domain knowledge and additional vocabulary for query formulation. we identify query emptiness and predicate emptiness as two central reasoning services in this context. query emptiness asks whether a given query has an empty answer over all databases formulated in a given vocabulary. predicate emptiness is defined analogously, but quantifies universally over all queries that contain a given predicate. in this paper, we determine the computational complexity of query emptiness and predicate emptiness in the el, dl-lite, and alc-families of description logics, investigate the connection to ontology modules, and perform a practical case study to evaluate the new reasoning services.",2016
automatic wordnet development for low-resource languages using cross-lingual wsd,"&#8206;wordnets are an effective resource for natural language processing and information retrieval&#8206;, &#8206;especially for semantic processing and meaning related tasks&#8206;. &#8206;so far&#8206;, &#8206;wordnets have been constructed for many languages&#8206;. &#8206;however&#8206;, &#8206;the automatic development of wordnets for low-resource languages has not been well studied&#8206;. &#8206;in this paper&#8206;, &#8206;an expectation-maximization algorithm is used to create high quality and large scale wordnets for poor-resource languages&#8206;. &#8206;the proposed method benefits from possessing cross-lingual word sense disambiguation and develops a wordnet by only using a bi-lingual dictionary and a mono-lingual corpus&#8206;. &#8206;the proposed method has been executed with persian language and the resulting wordnet has been evaluated through several experiments&#8206;. &#8206;the results show that the induced wordnet has a precision score of 90% and a recall score of 35%&#8206;.",2016
optimal any-angle pathfinding in practice,"any-angle pathfinding is a fundamental problem in robotics and computer games. the goal is to find a shortest path between a pair of points on a grid map such that the path is not artificially constrained to the points of the grid. prior research has focused on approximate online solutions. a number of exact methods exist but they all require super-linear space and pre-processing time. in this study, we describe anya: a new and optimal any-angle pathfinding algorithm. where other works find approximate any-angle paths by searching over individual points from the grid, anya finds optimal paths by searching over sets of states represented as intervals. each interval is identified on-the-fly. from each interval anya selects a single representative point that it uses to compute an admissible cost estimate for the entire set. anya always returns an optimal path if one exists. moreover it does so without any offline pre-processing or the introduction of additional memory overheads. in a range of empirical comparisons we show that anya is competitive with several recent (sub-optimal) online and pre-processing based techniques and is up to an order of magnitude faster than the most common benchmark algorithm, a grid-based implementation of a*.",2016
budgeted optimization with constrained experiments,"motivated by a real-world problem, we study a novel budgeted optimization problem where the goal is to optimize an unknown function f(.) given a budget by requesting a sequence of samples from the function. in our setting, however, evaluating the function at precisely specified points is not practically possible due to prohibitive costs. instead, we can only request constrained experiments. a constrained experiment, denoted by q, specifies a subset of the input space for the experimenter to sample the function from. the outcome of q includes a sampled experiment x, and its function output f(x). importantly, as the constraints of q become looser, the cost of fulfilling the request decreases, but the uncertainty about the location x increases. our goal is to manage this trade-off by selecting a set of constrained experiments that best optimize f(.) within the budget. we study this problem in two different settings, the non-sequential (or batch) setting where a set of constrained experiments is selected at once, and the sequential setting where experiments are selected one at a time. we evaluate our proposed methods for both settings using synthetic and real functions. the experimental results demonstrate the efficacy of the proposed methods.",2016
global continuous optimization with error bound and fast convergence,"this paper considers global optimization with a black-box unknown objective function that can be non-convex and non-differentiable. such a difficult optimization problem arises in many real-world applications, such as parameter tuning in machine learning, engineering design problem, and planning with a complex physics simulator. this paper proposes a new global optimization algorithm, called locally oriented global optimization (logo), to aim for both fast convergence in practice and finite-time error bound in theory. the advantage and usage of the new algorithm are illustrated via theoretical analysis and an experiment conducted with 11 benchmark test functions. further, we modify the logo algorithm to specifically solve a planning problem via policy search with continuous state/action space and long time horizon while maintaining its finite-time error bound. we apply the proposed planning method to accident management of a nuclear power plant. the result of the application study demonstrates the practical utility of our method.",2016
two aspects of relevance in structured argumentation: minimality and paraconsistency,"this paper studies two issues concerning relevance in structured argumentation in the context of the aspic+ framework, arising from the combined use of strict and defeasible inference rules. one issue arises if the strict inference rules correspond to classical logic. a longstanding problem is how the trivialising effect of the classical ex falso principle can be avoided while satisfying consistency and closure postulates. in this paper, this problem is solved by disallowing chaining of strict rules, resulting in a variant of the aspic+ framework called aspic*, and then disallowing the application of strict rules to inconsistent sets of formulas. thus in effect rescher & manor's paraconsistent notion of weak consequence is embedded in aspic*. another issue is minimality of arguments. if arguments can apply defeasible inference rules, then they cannot be required to have subset-minimal premises, since defeasible rules based on more information may well make an argument stronger. in this paper instead minimality is required of applications of strict rules throughout an argument. it is shown that under some plausible assumptions this does not affect the set of conclusions. in addition, circular arguments are in the new aspic* framework excluded in a way that satisfies closure and consistency postulates and that generates finitary argumentation frameworks if the knowledge base and set of defeasible rules are finite. for the latter result the exclusion of chaining of strict rules is essential. finally, the combined results of this paper are shown to be a proper extension of classical-logic argumentation with preferences and defeasible rules.",2016
association discovery and diagnosis of alzheimers disease with bayesian multiview learning,"the analysis and diagnosis of alzheimers disease (ad) can be based on genetic variations, e.g., single nucleotide polymorphisms (snps) and phenotypic traits, e.g., magnetic resonance imaging (mri) features. we consider two important and related tasks: i) to select genetic and phenotypical markers for ad diagnosis and ii) to identify associations between genetic and phenotypical data. while previous studies treat these two tasks separately, they are tightly coupled because underlying associations between genetic variations and phenotypical features contain the biological basis for a disease. here we present a new sparse bayesian approach for joint association study and disease diagnosis. in this approach, common latent features are extracted from different data sources based on sparse projection matrices and used to predict multiple disease severity levels; in return, the disease status can guide the discovery of relationships between data sources. the sparse projection matrices not only reveal interactions between data sources but also select groups of biomarkers related to the disease. moreover, to take advantage of the linkage disequilibrium (ld) measuring the non-random association of alleles, we incorporate a graph laplacian type of prior in the model. to learn the model from data, we develop an efficient variational inference algorithm. analysis on an imaging genetics dataset for the study of alzheimers disease (ad) indicates that our model identifies biologically meaningful associations between genetic variations and mri features, and achieves significantly higher accuracy for predicting ordinal ad stages than the competing methods.",2016
combining the delete relaxation with critical-path heuristics: a direct characterization,"recent work has shown how to improve delete relaxation heuristics by computing relaxed plans, i.e., the hff heuristic, in a compiled planning task pic which represents a given set c of fact conjunctions explicitly. while this compilation view of such partial delete relaxation is simple and elegant, its meaning with respect to the original planning task is opaque, and the size of pic grows exponentially in |c|. we herein provide a direct characterization, without compilation, making explicit how the approach arises from a combination of the delete-relaxation with critical-path heuristics. designing equations characterizing a novel view on h+ on the one hand, and a generalized version hc of hm on the other hand, we show that h+(pic) can be characterized in terms of a combined hcplus equation. this naturally generalizes the standard delete-relaxation framework: understanding that framework as a relaxation over singleton facts as atomic subgoals, one can refine the relaxation by using the conjunctions c as atomic subgoals instead. thanks to this explicit view, we identify the precise source of complexity in hff(pic), namely maximization of sets of supported atomic subgoals during relaxed plan extraction, which is easy for singleton-fact subgoals but is np-complete in the general case. approximating that problem greedily, we obtain a polynomial-time hcff version of hff(pic), superseding the pic compilation, and superseding the modified picce compilation which achieves the same complexity reduction but at an information loss. experiments on ipc benchmarks show that these theoretical advantages can translate into empirical ones.",2016
dl-lite contraction and revision,"two essential tasks in managing description logic knowledge bases are eliminating problematic axioms and incorporating newly formed ones. such elimination and incorporation are formalised as the operations of contraction and revision in belief change. in this paper, we deal with contraction and revision for the dl-lite family through a model-theoretic approach. standard description logic semantics yields an infinite number of models for dl-lite knowledge bases, thus it is difficult to develop algorithms for contraction and revision that involve dl models. the key to our approach is the introduction of an alternative semantics called type semantics which can replace the standard semantics in characterising the standard inference tasks of dl-lite. type semantics has several advantages over the standard one. it is more succinct and importantly, with a finite signature, the semantics always yields a finite number of models. we then define model-based contraction and revision functions for dl-lite knowledge bases under type semantics and provide representation theorems for them. finally, the finiteness and succinctness of type semantics allow us to develop tractable algorithms for instantiating the functions.",2016
generating models of a matched formula with a polynomial delay,"a matched formula is a cnf formula whose incidence graph admits a matching which matches a distinct variable to every clause. such a formula is always satisfiable. matched formulas are used, for example, in the area of parametrized complexity. we prove that the problem of counting the number of the models (satisfying assignments) of a matched formula is #p-complete. on the other hand, we define a class of formulas generalizing the matched formulas and prove that for a formula in this class one can choose in polynomial time a variable suitable for splitting the tree for the search of the models of the formula. as a consequence, the models of a formula from this class, in particular of any matched formula, can be generated sequentially with a delay polynomial in the size of the input. on the other hand, we prove that this task cannot be performed efficiently for linearly satisfiable formulas, which is a generalization of matched formulas containing the class considered above.",2016
on the satisfiability problem for sparql patterns,"the satisfiability problem for sparql 1.0 patterns is undecidable in general, since the relational algebra can be emulated using such patterns. the goal of this paper is to delineate the boundary of decidability of satisfiability in terms of the constraints allowed in filter conditions. the classes of constraints considered are bound-constraints, negated bound- constraints, equalities, nonequalities, constant-equalities, and constant-nonequalities. the main result of the paper can be summarized by saying that, as soon as inconsistent filter conditions can be formed, satisfiability is undecidable. the key insight in each case is to find a way to emulate the set difference operation. undecidability can then be obtained from a known undecidability result for the algebra of binary relations with union, composition, and set difference. when no inconsistent filter conditions can be formed, satisfiability is decidable by syntactic checks on bound variables and on the use of literals. although the problem is shown to be np-complete, it is experimentally shown that the checks can be implemented efficiently in practice. the paper also points out that satisfiability for the so-called &lsquo;well-designed&rsquo; patterns can be decided by a check on bound variables and a check for inconsistent filter conditions.",2016
efficient mechanism design for online scheduling,"this paper concerns the mechanism design for online scheduling in a strategic setting. in this setting, each job is owned by a self-interested agent who may misreport the release time, deadline, length, and value of her job, while we need to determine not only the schedule of the jobs, but also the payment of each agent. we focus on the design of incentive compatible (ic) mechanisms, and study the maximization of social welfare (i.e., the aggregated value of completed jobs) by competitive analysis. we first derive two lower bounds on the competitive ratio of any deterministic ic mechanism to characterize the landscape of our research. we then propose a deterministic ic mechanism and show that such a simple mechanism works very well for both the preemption-restart model and the preemption-resume model. we show the mechanism can achieve the optimal competitive ratio of 5 for equal-length jobs and a near optimal competitive ratio (within a constant factor) for unequal-length jobs.",2016
computing repairs of inconsistent dl-programs over el ontologies,"description logic (dl) ontologies and non-monotonic rules are two prominent knowledge representation (kr) formalisms with complementary features that are essential for various applications. nonmonotonic description logic (dl) programs combine these formalisms thus providing support for rule-based reasoning on top of dl ontologies using a well-defined query interface represented by so-called dl-atoms. unfortunately, interaction of the rules and the ontology may incur inconsistencies such that a dl-program lacks answer sets (i.e., models), and thus yields no information. this issue is addressed by recently defined repair answer sets, for computing which an effective practical algorithm was proposed for dl-lite a ontologies that reduces a repair computation to constraint matching based on so-called support sets. however, the algorithm exploits particular features of dl-lite a and can not be readily applied to repairing dl-programs over other prominent dls like el. compared to dl-lite a , in el support sets may neither be small nor only few support sets might exist, and completeness of the algorithm may need to be given up when the support information is bounded. we thus provide an approach for computing repairs for dl-programs over el ontologies based on partial (incomplete) support families. the latter are constructed using datalog query rewriting techniques as well as ontology approximation based on logical difference between el-terminologies. we show how the maximal size and number of support sets for a given dl-atom can be estimated by analyzing the properties of a support hypergraph, which characterizes a relevant set of tbox axioms needed for query derivation. we present a declarative implementation of the repair approach and experimentally evaluate it on a set of benchmark problems; the promising results witness practical feasibility of our repair approach.",2016
time-sensitive bayesian information aggregation for crowdsourcing systems,"many aspects of the design of efficient crowdsourcing processes, such as defining workers bonuses, fair prices and time limits of the tasks, involve knowledge of the likely duration of the task at hand. in this work we introduce a new timesensitive bayesian aggregation method that simultaneously estimates a tasks duration and obtains reliable aggregations of crowdsourced judgments. our method, called bcctime, uses latent variables to represent the uncertainty about the workers completion time, the tasks duration and the workers accuracy. to relate the quality of a judgment to the time a worker spends on a task, our model assumes that each task is completed within a latent time window within which all workers with a propensity to genuinely attempt the labelling task (i.e., no spammers) are expected to submit their judgments. in contrast, workers with a lower propensity to valid labelling, such as spammers, bots or lazy labellers, are assumed to perform tasks considerably faster or slower than the time required by normal workers. specifically, we use efficient message-passing bayesian inference to learn approximate posterior probabilities of (i) the confusion matrix of each worker, (ii) the propensity to valid labelling of each worker, (iii) the unbiased duration of each task and (iv) the true label of each task. using two real- world public datasets for entity linking tasks, we show that bcctime produces up to 11% more accurate classifications and up to 100% more informative estimates of a tasks duration compared to stateoftheart methods.",2016
time-bounded best-first search for reversible and non-reversible search graphs,"time-bounded a* is a real-time, single-agent, deterministic search algorithm that expands states of a graph in the same order as a* does, but that unlike a* interleaves search and action execution. known to outperform state-of-the-art real-time search algorithms based on korf's learning real-time a* (lrta*) in some benchmarks, it has not been studied in detail and is sometimes not considered as a ``true'' real-time search algorithm since it fails in non-reversible problems even it the goal is still reachable from the current state. in this paper we propose and study time-bounded best-first search (tb(bfs)) a straightforward generalization of the time-bounded approach to any best-first search algorithm. furthermore, we propose restarting time-bounded weighted a* (tb_r(wa*)), an algorithm that deals more adequately with non-reversible search graphs, eliminating ``backtracking moves'' and incorporating search restarts and heuristic learning. in non-reversible problems we prove that tb(bfs) terminates and we deduce cost bounds for the solutions returned by time-bounded weighted a* (tb(wa*)), an instance of tb(bfs). furthermore, we prove tb_r(wa*), under reasonable conditions, terminates. we evaluate tb(wa) in both grid pathfinding and the 15-puzzle. in addition, we evaluate tb_r(wa*) on the racetrack problem. we compare our algorithms to lss-lrtwa*, a variant of lrta* that can exploit lookahead search and a weighted heuristic. a general observation is that the performance of both tb(wa*) and tb_r(wa*) improves as the weight parameter is increased. in addition, our time-bounded algorithms almost always outperform lss-lrtwa* by a significant margin.",2016
a study of proxies for shapley allocations of transport costs,"we survey existing rules of thumb, propose novel methods, and comprehensively evaluate a number of solutions to the problem of calculating the cost to serve each location in a single-vehicle transport setting. cost to serve analysis has applications both strategically and operationally in transportation settings. the problem is formally modeled as the traveling salesperson game (tsg), a cooperative transferable utility game in which agents correspond to locations in a traveling salesperson problem (tsp). the total cost to serve all locations in the tsp is the length of an optimal tour. an allocation divides the total cost among individual locations, thus providing the cost to serve each of them. as one of the most important normative division schemes in cooperative games, the shapley value gives a principled and fair allocation for a broad variety of games including the tsg. we consider a number of direct and sampling-based procedures for calculating the shapley value, and prove that approximating the shapley value of the tsg within a constant factor is np-hard. treating the shapley value as an ideal baseline allocation, we survey six proxies for it that are each relatively easy to compute. some of these proxies are rules of thumb and some are procedures international delivery companies use(d) as cost allocation methods. we perform an experimental evaluation using synthetic euclidean games as well as games derived from real-world tours calculated for scenarios involving fast-moving goods; where deliveries are made on a road network every day. we explore several computationally tractable allocation techniques that are good proxies for the shapley value in problem instances of a size and complexity that is commercially relevant.",2016
datalog+- ontology consolidation,"knowledge bases in the form of ontologies are receiving increasing attention as they allow to clearly represent both the available knowledge, which includes the knowledge in itself and the constraints imposed to it by the domain or the users. in particular, datalog&#177; ontologies are attractive because of their property of decidability and the possibility of dealing with the massive amounts of data in real world environments; however, as it is the case with many other ontological languages, their application in collaborative environments often lead to inconsistency related issues. in this paper we introduce the notion of incoherence regarding datalog&#177; ontologies, in terms of satisfiability of sets of constraints, and show how under specific conditions incoherence leads to inconsistent datalog&#177; ontologies. the main contribution of this work is a novel approach to restore both consistency and coherence in datalog&#177; ontologies. the proposed approach is based on kernel contraction and restoration is performed by the application of incision functions that select formulas to delete. nevertheless, instead of working over minimal incoherent/inconsistent sets encountered in the ontologies, our operators produce incisions over non-minimal structures called clusters. we present a construction for consolidation operators, along with the properties expected to be satisfied by them. finally, we establish the relation between the construction and the properties by means of a representation theorem. although this proposal is presented for datalog&#177; ontologies consolidation, these operators can be applied to other types of ontological languages, such as description logics, making them apt to be used in collaborative environments like the semantic web.",2016
the ibacop planning system: instance-based configured portfolios,"sequential planning portfolios are very powerful in exploiting the complementary strength of different automated planners. the main challenge of a portfolio planner is to define which base planners to run, to assign the running time for each planner and to decide in what order they should be carried out to optimize a planning metric. portfolio configurations are usually derived empirically from training benchmarks and remain fixed for an evaluation phase. in this work, we create a per-instance configurable portfolio, which is able to adapt itself to every planning task. the proposed system pre-selects a group of candidate planners using a pareto-dominance filtering approach and then it decides which planners to include and the time assigned according to predictive models. these models estimate whether a base planner will be able to solve the given problem and, if so, how long it will take. we define different portfolio strategies to combine the knowledge generated by the models. the experimental evaluation shows that the resulting portfolios provide an improvement when compared with non-informed strategies. one of the proposed portfolios was the winner of the sequential satisficing track of the international planning competition held in 2014.",2016
qualitative spatial logics for buffered geometries,"this paper describes a series of new qualitative spatial logics for checking consistency of sameas and partof matches between spatial objects from different geospatial datasets, especially from crowd-sourced datasets. since geometries in crowd-sourced data are usually not very accurate or precise, we buffer geometries by a margin of error or a level of tolerance, and define spatial relations for buffered geometries. the spatial logics formalize the notions of `buffered equal' (intuitively corresponding to `possibly sameas'), `buffered part of' (`possibly partof'), `near' (`possibly connected') and `far' (`definitely disconnected'). a sound and complete axiomatisation of each logic is provided with respect to models based on metric spaces. for each of the logics, the satisfiability problem is shown to be np-complete. finally, we briefly describe how the logics are used in a system for generating and debugging matches between spatial objects, and report positive experimental evaluation results for the system.",2016
introduction to the special issue on cross-language algorithms and applications,"with the increasingly global nature of our everyday interactions, the need for multilin- gual technologies to support efficient and effective information access and communication cannot be overemphasized. computational modeling of language has been the focus of natural language processing, a subdiscipline of artificial intelligence. one of the current challenges for this discipline is to design methodologies and algorithms that are cross- language in order to create multilingual technologies rapidly. the goal of this jair special issue on cross-language algorithms and applications (claa) is to present leading re- search in this area, with emphasis on developing unifying themes that could lead to the development of the science of multi- and cross-lingualism. in this introduction, we provide the reader with the motivation for this special issue and summarize the contributions of the papers that have been included. the selected papers cover a broad range of cross-lingual technologies including machine translation, domain and language adaptation for sentiment analysis, cross-language lexical resources, dependency parsing, information retrieval and knowledge representation. we anticipate that this special issue will serve as an invaluable resource for researchers interested in topics of cross-lingual natural language processing.",2016
integrating rules and dictionaries from shallow-transfer machine translation into phrase-based statistical machine translation,"we describe a hybridisation strategy whose objective is to integrate linguistic resources from shallow-transfer rule-based machine translation (rbmt) into phrase-based statistical machine translation (pbsmt). it basically consists of enriching the phrase table of a pbsmt system with bilingual phrase pairs matching transfer rules and dictionary entries from a shallow-transfer rbmt system. this new strategy takes advantage of how the linguistic resources are used by the rbmt system to segment the source-language sentences to be translated, and overcomes the limitations of existing hybrid approaches that treat the rbmt systems as a black box. experimental results confirm that our approach delivers translations of higher quality than existing ones, and that it is specially useful when the parallel corpus available for training the smt system is small or when translating out-of-domain texts that are well covered by the rbmt dictionaries. a combination of this approach with a recently proposed unsupervised shallow-transfer rule inference algorithm results in a significantly greater translation quality than that of a baseline pbsmt; in this case, the only hand-crafted resource used are the dictionaries commonly used in rbmt. moreover, the translation quality achieved by the hybrid system built with automatically inferred rules is similar to that obtained by those built with hand-crafted rules.",2016
cross-lingual bridges with models of lexical borrowing,"linguistic borrowing is the phenomenon of transferring linguistic constructions (lexical, phonological, morphological, and syntactic) from a donor language to a recipient language as a result of contacts between communities speaking different languages. borrowed words are found in all languages, andin contrast to cognate relationshipsborrowing relationships may exist across unrelated languages (for example, about 40% of swahilis vocabulary is borrowed from the unrelated language arabic). in this work, we develop a model of morpho-phonological transformations across languages. its features are based on universal constraints from optimality theory (ot), and we show that compared to several standardbut linguistically more na&#239;vebaselines, our ot-inspired model obtains good performance at predicting donor forms from borrowed forms with only a few dozen training examples, making this a cost-effective strategy for sharing lexical information across languages. we demonstrate applications of the lexical borrowing model in machine translation, using resource-rich donor language to obtain translations of out-of-vocabulary loanwords in a lower resource language. our framework obtains substantial improvements (up to 1.6 bleu) over standard baselines.",2016
how translation alters sentiment,"sentiment analysis research has predominantly been on english texts. thus there exist many sentiment resources for english, but less so for other languages. approaches to improve sentiment analysis in a resource-poor focus language include: (a) translate the focus language text into a resource-rich language such as english, and apply a powerful english sentiment analysis system on the text, and (b) translate resources such as sentiment labeled corpora and sentiment lexicons from english into the focus language, and use them as additional resources in the focus-language sentiment analysis system. in this paper we systematically examine both options. we use arabic social media posts as stand-in for the focus language text. we show that sentiment analysis of english translations of arabic texts produces competitive results, w.r.t. arabic sentiment analysis. we show that arabic sentiment analysis systems benefit from the use of automatically translated english sentiment lexicons. we also conduct manual annotation studies to examine why the sentiment of a translation is different from the sentiment of the source word or text. this is especially relevant for building better automatic translation systems. in the process, we create a state-of-the-art arabic sentiment analysis system, a new dialectal arabic sentiment lexicon, and the first arabic-english parallel corpus that is independently annotated for sentiment by arabic and english speakers.",2016
distributional correspondence indexing for cross-lingual and cross-domain sentiment classification.,"domain adaptation (da) techniques aim at enabling machine learning methods learn effective classifiers for a ""target'' domain when the only available training data belongs to a different ""source'' domain. in this paper we present the distributional correspondence indexing (dci) method for domain adaptation in sentiment classification. dci derives term representations in a vector space common to both domains where each dimension reflects its distributional correspondence to a pivot, i.e., to a highly predictive term that behaves similarly across domains. term correspondence is quantified by means of a distributional correspondence function (dcf). we propose a number of efficient dcfs that are motivated by the distributional hypothesis, i.e., the hypothesis according to which terms with similar meaning tend to have similar distributions in text. experiments show that dci obtains better performance than current state-of-the-art techniques for cross-lingual and cross-domain sentiment classification. dci also brings about a significantly reduced computational cost, and requires a smaller amount of human intervention. as a final contribution, we discuss a more challenging formulation of the domain adaptation problem, in which both the cross-domain and cross-lingual dimensions are tackled simultaneously.",2016
effectiveness of automatic translations for cross-lingual ontology mapping,"accessing or integrating data lexicalized in different languages is a challenge. multilingual lexical resources play a fundamental role in reducing the language barriers to map concepts lexicalized in different languages. in this paper we present a large-scale study on the effectiveness of automatic translations to support two key cross-lingual ontology mapping tasks: the retrieval of candidate matches and the selection of the correct matches for inclusion in the final alignment. we conduct our experiments using four different large gold standards, each one consisting of a pair of mapped wordnets, to cover four different families of languages. we categorize concepts based on their lexicalization (type of words, synonym richness, position in a subconcept graph) and analyze their distributions in the gold standards. leveraging this categorization, we measure several aspects of translation effectiveness, such as word-translation correctness, word sense coverage, synset and synonym coverage. finally, we thoroughly discuss several findings of our study, which we believe are helpful for the design of more sophisticated cross-lingual mapping algorithms.",2016
synthetic treebanking for cross-lingual dependency parsing,"how do we parse the languages for which no treebanks are available? this contribution addresses the cross-lingual viewpoint on statistical dependency parsing, in which we attempt to make use of resource-rich source language treebanks to build and adapt models for the under-resourced target languages. we outline the benefits, and indicate the drawbacks of the current major approaches. we emphasize synthetic treebanking: the automatic creation of target language treebanks by means of annotation projection and machine translation. we present competitive results in cross-lingual dependency parsing using a combination of various techniques that contribute to the overall success of the method. we further include a detailed discussion about the impact of part-of-speech label accuracy on parsing results that provide guidance in practical applications of cross-lingual methods for truly under-resourced languages.",2016
utilisation of metadata fields and query expansion in cross-lingual search of user-generated internet video,"recent years have seen significant efforts in the area of cross language information retrieval (clir) for text retrieval. this work initially focused on formally published content, but more recently research has begun to concentrate on clir for informal social media content. however, despite the current expansion in online multimedia archives, there has been little work on clir for this content. while there has been some limited work on cross-language video retrieval (clvr) for professional videos, such as documentaries or tv news broadcasts, there has to date, been no significant investigation of clvr for the rapidly growing archives of informal user generated (ugc) content. key differences between such ugc and professionally produced content are the nature and structure of the textual ugc metadata associated with it, as well as the form and quality of the content itself. in this setting, retrieval effectiveness may not only suffer from translation errors common to all clir tasks, but also recognition errors associated with the automatic speech recognition (asr) systems used to transcribe the spoken content of the video and with the informality and inconsistency of the associated user-created metadata for each video. this work proposes and evaluates techniques to improve clir effectiveness of such noisy ugc content. our experimental investigation shows that different sources of evidence, e.g. the content from different fields of the structured metadata, significantly affect clir effectiveness. results from our experiments also show that each metadata field has a varying robustness to query expansion (qe) and hence can have a negative impact on the clir effectiveness. our work proposes a novel adaptive qe technique that predicts the most reliable source for expansion and shows how this technique can be effective for improving the clir effectiveness for ugc content.",2016
news across languages - cross-lingual document similarity and event tracking,"in today's world, we follow news which is distributed globally. significant events are reported by different sources and in different languages. in this work, we address the problem of tracking of events in a large multilingual stream. within a recently developed system event registry we examine two aspects of this problem: how to compare articles in different languages and how to link collections of articles in different languages which refer to the same event. taking a multilingual stream and clusters of articles from each language, we compare different cross-lingual document similarity measures based on wikipedia. this allows us to compute the similarity of any two articles regardless of language. building on previous work, we show there are methods which scale well and can compute a meaningful similarity between articles from languages with little or no direct overlap in the training data. using this capability, we then propose an approach to link clusters of articles across languages which represent the same event. we provide an extensive evaluation of the system as a whole, as well as an evaluation of the quality and robustness of the similarity measure and the linking algorithm.",2016
adaptive contract design for crowdsourcing markets: bandit algorithms for repeated principal-agent problems,"crowdsourcing markets have emerged as a popular platform for matching available workers with tasks to complete. the payment for a particular task is typically set by the task's requester, and may be adjusted based on the quality of the completed work, for example, through the use of ""bonus"" payments. in this paper, we study the requester's problem of dynamically adjusting quality-contingent payments for tasks. we consider a multi-round version of the well-known principal-agent model, whereby in each round a worker makes a strategic choice of the effort level which is not directly observable by the requester. in particular, our formulation significantly generalizes the budget-free online task pricing problems studied in prior work. we treat this problem as a multi-armed bandit problem, with each ""arm"" representing a potential contract. to cope with the large (and in fact, infinite) number of arms, we propose a new algorithm, agnosticzooming, which discretizes the contract space into a finite number of regions, effectively treating each region as a single arm. this discretization is adaptively refined, so that more promising regions of the contract space are eventually discretized more finely. we analyze this algorithm, showing that it achieves regret sublinear in the time horizon and substantially improves over non-adaptive discretization (which is the only competing approach in the literature). our results advance the state of art on several different topics: the theory of crowdsourcing markets, principal-agent problems, multi-armed bandits, and dynamic pricing.",2016
bayesian optimization in a billion dimensions via random embeddings,"bayesian optimization techniques have been successfully applied to robotics, planning, sensor placement, recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. despite these successes, the approach is restricted to problems of moderate dimension, and several workshops on bayesian optimization have identified its scaling to high-dimensions as one of the holy grails of the field. in this paper, we introduce a novel random embedding idea to attack this problem. the resulting random embedding bayesian optimization (rembo) algorithm is very simple, has important invariance properties, and applies to domains with both categorical and continuous variables. we present a thorough theoretical analysis of rembo. empirical results confirm that rembo can effectively solve problems with billions of dimensions, provided the intrinsic dimensionality is low. they also show that rembo achieves state-of-the-art performance in optimizing the 47 discrete parameters of a popular mixed integer linear programming solver.",2016
predicting twitter user demographics using distant supervision from website traffic data,"understanding the demographics of users of online social networks has important applications for health, marketing, and public messaging. whereas most prior approaches rely on a supervised learning approach, in which individual users are labeled with demographics for training, we instead create a distantly labeled dataset by collecting audience measurement data for 1,500 websites (e.g., 50% of visitors to gizmodo.com are estimated to have a bachelor's degree). we then fit a regression model to predict these demographics from information about the followers of each website on twitter. using patterns derived both from textual content and the social network of each user, our final model produces an average held-out correlation of .77 across seven different variables (age, gender, education, ethnicity, income, parental status, and political preference). we then apply this model to classify individual twitter users by ethnicity, gender, and political preference, finding performance that is surprisingly competitive with a fully supervised approach.",2016
"automatic description generation from images: a survey of models, datasets, and evaluation measures","automatic description generation from natural images is a challenging problem that has recently received a large amount of interest from the computer vision and natural language processing communities. in this survey, we classify the existing approaches based on how they conceptualize this problem, viz., models that cast description as either generation problem or as a retrieval problem over a visual or multimodal representational space. we provide a detailed review of existing models, highlighting their advantages and disadvantages. moreover, we give an overview of the benchmark image datasets and the evaluation measures that have been developed to assess the quality of machine-generated image descriptions. finally we extrapolate future directions in the area of automatic image description generation.",2016
optimally solving dec-pomdps as continuous-state mdps,"decentralized partially observable markov decision processes (dec-pomdps) provide a general model for decision-making under uncertainty in decentralized settings, but are difficult to solve optimally (nexp-complete). as a new way of solving these problems, we introduce the idea of transforming a dec-pomdp into a continuous-state deterministic mdp with a piecewise-linear and convex value function. this approach makes use of the fact that planning can be accomplished in a centralized offline manner, while execution can still be decentralized. this new dec-pomdp formulation, which we call an occupancy mdp, allows powerful pomdp and continuous-state mdp methods to be used for the first time. to provide scalability, we refine this approach by combining heuristic search and compact representations that exploit the structure present in multi-agent domains, without losing the ability to converge to an optimal solution. in particular, we introduce a feature-based heuristic search value iteration (fb-hsvi) algorithm that relies on feature-based compact representations, point-based updates and efficient action selection. a theoretical analysis demonstrates that fb-hsvi terminates in finite time with an optimal solution. we include an extensive empirical analysis using well-known benchmarks, thereby demonstrating that our approach provides significant scalability improvements compared to the state of the art.",2016
module extraction in expressive ontology languages via datalog reasoning,"module extraction is the task of computing a (preferably small) fragment m of an ontology t that preserves a class of entailments over a signature of interest s. extracting modules of minimal size is well-known to be computationally hard, and often algorithmically infeasible, especially for highly expressive ontology languages. thus, practical techniques typically rely on approximations, where m provably captures the relevant entailments, but is not guaranteed to be minimal. existing approximations ensure that m preserves all second-order entailments of t w.r.t. s, which is a stronger condition than is required in many applications, and may lead to unnecessarily large modules in practice. in this paper we propose a novel approach in which module extraction is reduced to a reasoning problem in datalog. our approach generalises existing approximations in an elegant way. more importantly, it allows extraction of modules that are tailored to preserve only specific kinds of entailments, and thus are often significantly smaller. our evaluation on a wide range of ontologies confirms the feasibility and benefits of our approach in practice.",2016
finding strategyproof social choice functions via sat solving,"a promising direction in computational social choice is to address research problems using computer-aided proving techniques. in particular with sat solvers, this approach has been shown to be viable not only for proving classic impossibility theorems such as arrow's theorem but also for finding new impossibilities in the context of preference extensions. in this paper, we demonstrate that these computer-aided techniques can also be applied to improve our understanding of strategyproof irresolute social choice functions. these functions, however, requires a more evolved encoding as otherwise the search space rapidly becomes much too large. our contribution is two-fold: we present an efficient encoding for translating such problems to sat and leverage this encoding to prove new results about strategyproofness with respect to kelly's and fishburn's preference extensions. for example, we show that no pareto-optimal majoritarian social choice function satisfies fishburn-strategyproofness. furthermore, we explain how human-readable proofs of such results can be extracted from minimal unsatisfiable cores of the corresponding sat formulas.",2016
large-scale election campaigns: combinatorial shift bribery,"we study the complexity of a combinatorial variant of the shift bribery problem in elections. in the standard shift bribery problem, we are given an election where each voter has a preference order over the set of candidates and where an outside agent, the briber, can pay each voter to rank the briber's favorite candidate a given number of positions higher. the goal is to ensure the victory of the briber's preferred candidate. the combinatorial variant of the problem, introduced in this paper, models settings where it is possible to affect the position of the preferred candidate in multiple votes, either positively or negatively, with a single bribery action. this variant of the problem is particularly interesting in the context of large-scale campaign management problems (which, from the technical side, are modeled as bribery problems). we show that, in general, the combinatorial variant of the problem is highly intractable; specifically, np-hard, hard in the parameterized sense, and hard to approximate. nevertheless, we provide parameterized algorithms and approximation algorithms for natural restricted cases.",2016
exact algorithms for mre inference,most relevant explanation (mre) is an inference task in bayesian networks that finds the most relevant partial instantiation of target variables as an explanation for given evidence by maximizing the generalized bayes factor (gbf). no exact mre algorithm has been developed previously except exhaustive search. this paper fills the void by introducing two breadth-first branch-and-bound (bfbnb) algorithms for solving mre based on novel upper bounds of gbf. one upper bound is created by decomposing the computation of gbf using a target blanket decomposition of evidence variables. the other upper bound improves the first bound in two ways. one is to split the target blankets that are too large by converting auxiliary nodes into pseudo-targets so as to scale to large problems. the other is to perform summations instead of maximizations on some of the target variables in each target blanket. our empirical evaluations show that the proposed bfbnb algorithms make exact mre inference tractable in bayesian networks that could not be solved previously.,2016
quadratization and roof duality of markov logic networks,"this article discusses the quadratization of markov logic networks, which enables efficient approximate map computation by means of maximum flows. the procedure relies on a pseudo-boolean representation of the model, and allows handling models of any order. the employed pseudo-boolean representation can be used to identify problems that are guaranteed to be solvable in low polynomial-time. results on common benchmark problems show that the proposed approach finds optimal assignments for most variables in excellent computational time and approximate solutions that match the quality of ilp-based solvers.",2016
combining two and three-way embedding models for link prediction in knowledge bases,"this paper tackles the problem of endogenous link prediction for knowledge base completion. knowledge bases can be represented as directed graphs whose nodes correspond to entities and edges to relationships. previous attempts either consist of powerful systems with high capacity to model complex connectivity patterns, which unfortunately usually end up overfitting on rare relationships, or in approaches that trade capacity for simplicity in order to fairly model all relationships, frequent or not. in this paper, we propose tatec, a happy medium obtained by complementing a high-capacity model with a simpler one, both pre-trained separately and then combined. we present several variants of this model with different kinds of regularization and combination strategies and show that this approach outperforms existing methods on different types of relationships by achieving state-of-the-art results on four benchmarks of the literature.",2016
knowledge representation in probabilistic spatio-temporal knowledge bases,"we represent knowledge as integrity constraints in a formalization of probabilistic spatio-temporal knowledge bases. we start by defining the syntax and semantics of a formalization called pst knowledge bases. this definition generalizes an earlier version, called spot, which is a declarative framework for the representation and processing of probabilistic spatio-temporal data where probability is represented as an interval because the exact value is unknown. we augment the previous definition by adding a type of non-atomic formula that expresses integrity constraints. the result is a highly expressive formalism for knowledge representation dealing with probabilistic spatio-temporal data. we obtain complexity results both for checking the consistency of pst knowledge bases and for answering queries in pst knowledge bases, and also specify tractable cases. all the domains in the pst framework are finite, but we extend our results also to arbitrarily large finite domains.",2016
an exact algorithm based on maxsat reasoning for the maximum weight clique problem,"recently, maxsat reasoning is shown very effective in computing a tight upper bound for a maximum clique (mc) of a (unweighted) graph. in this paper, we apply maxsat reasoning to compute a tight upper bound for a maximum weight clique (mwc) of a wighted graph. we first study three usual encodings of mwc into weighted partial maxsat dealing with hard clauses, which must be satisfied in all solutions, and soft clauses, which are weighted and can be falsified. the drawbacks of these encodings motivate us to propose an encoding of mwc into a special weighted partial maxsat formalism, called lw (literal-weighted) encoding and dedicated for upper bounding an mwc, in which both soft clauses and literals in soft clauses are weighted. an optimal solution of the lw maxsat instance gives an upper bound for an mwc, instead of an optimal solution for mwc. we then introduce two notions called the top-k literal failed clause and the top-k empty clause to extend classical maxsat reasoning techniques, as well as two sound transformation rules to transform an lw maxsat instance. successive transformations of an lw maxsat instance driven by maxsat reasoning give a tight upper bound for the encoded mwc. the approach is implemented in a branch-and-bound algorithm called mwclq. experimental evaluations on the broadly used dimacs benchmark, bhoslib benchmark, random graphs and the benchmark from the winner determination problem show that our approach allows mwclq to reduce the search space significantly and to solve mwc instances effectively. consequently, mwclq outperforms state-of-the-art exact algorithms on the vast majority of instances. moreover, it is surprisingly effective in solving hard and dense instances.",2016
parallel model-based diagnosis on multi-core computers,"model-based diagnosis (mbd) is a principled and domain-independent way of analyzing why a system under examination is not behaving as expected. given an abstract description (model) of the system's components and their behavior when functioning normally, mbd techniques rely on observations about the actual system behavior to reason about possible causes when there are discrepancies between the expected and observed behavior. due to its generality, mbd has been successfully applied in a variety of application domains over the last decades. in many application domains of mbd, testing different hypotheses about the reasons for a failure can be computationally costly, e.g., because complex simulations of the system behavior have to be performed. in this work, we therefore propose different schemes of parallelizing the diagnostic reasoning process in order to better exploit the capabilities of modern multi-core computers. we propose and systematically evaluate parallelization schemes for reiter's hitting set algorithm for finding all or a few leading minimal diagnoses using two different conflict detection techniques. furthermore, we perform initial experiments for a basic depth-first search strategy to assess the potential of parallelization when searching for one single diagnosis. finally, we test the effects of parallelizing ""direct encodings"" of the diagnosis problem in a constraint solver.",2016
searching for the m best solutions in graphical models,"the paper focuses on finding the m best solutions to combinatorial optimization problems using best-first or depth-first branch and bound search. specifically, we present a new algorithm m-a*, extending the well-known a* to the m-best task, and for the first time prove that all its desirable properties, including soundness, completeness and optimal efficiency, are maintained. since best-first algorithms require extensive memory, we also extend the memory-efficient depth-first branch and bound to the m-best task. we adapt both algorithms to optimization tasks over graphical models (e.g., weighted csp and mpe in bayesian networks), provide complexity analysis and an empirical evaluation. our experiments confirm theory that the best-first approach is largely superior when memory is available, but depth-first branch and bound is more robust. we also show that our algorithms are competitive with related schemes recently developed for the m-best task.",2016
bilingual distributed word representations from document-aligned comparable data,"we propose a new model for learning bilingual word representations from non-parallel document-aligned data. following the recent advances in word representation learning, our model learns dense real-valued word vectors, that is, bilingual word embeddings (bwes). unlike prior work on inducing bwes which heavily relied on parallel sentence-aligned corpora and/or readily available translation resources such as dictionaries, the article reveals that bwes may be learned solely on the basis of document-aligned comparable data without any additional lexical resources nor syntactic information. we present a comparison of our approach with previous state-of-the-art models for learning bilingual word representations from comparable data that rely on the framework of multilingual probabilistic topic modeling (muptm), as well as with distributional local context-counting models. we demonstrate the utility of the induced bwes in two semantic tasks: (1) bilingual lexicon extraction, (2) suggesting word translations in context for polysemous words. our simple yet effective bwe-based models significantly outperform the muptm-based and context-counting representation models from comparable data as well as prior bwe-based models, and acquire the best reported results on both tasks for all three tested language pairs.",2016
a distributed representation-based framework for cross-lingual transfer parsing,"this paper investigates the problem of cross-lingual transfer parsing, aiming at inducing dependency parsers for low-resource languages while using only training data from a resource-rich language (e.g., english). existing model transfer approaches typically don't include lexical features, which are not transferable across languages. in this paper, we bridge the lexical feature gap by using distributed feature representations and their composition. we provide two algorithms for inducing cross-lingual distributed representations of words, which map vocabularies from two different languages into a common vector space. consequently, both lexical features and non-lexical features can be used in our model for cross-lingual transfer. furthermore, our framework is flexible enough to incorporate additional useful features such as cross-lingual word clusters. our combined contributions achieve an average relative error reduction of 10.9% in labeled attachment score as compared with the delexicalized parser, trained on english universal treebank and transferred to three other languages. it also significantly outperforms state-of-the-art delexicalized models augmented with projected cluster features on identical data. finally, we demonstrate that our models can be further boosted with minimal supervision (e.g., 100 annotated sentences) from target languages, which is of great significance for practical usage.",2016
semi-supervised learning with induced word senses for state of the art word sense disambiguation,"word sense disambiguation (wsd) aims to determine the meaning of a word in context, and successful approaches are known to benefit many applications in natural language processing. although supervised learning has been shown to provide superior wsd performance, current sense-annotated corpora do not contain a sufficient number of instances per word type to train supervised systems for all words. while unsupervised techniques have been proposed to overcome this data sparsity problem, such techniques have not outperformed supervised methods. in this paper, we propose a new approach to building semi-supervised wsd systems that combines a small amount of sense-annotated data with information from word sense induction, a fully-unsupervised technique that automatically learns the different senses of a word based on how it is used. in three experiments, we show how sense induction models may be effectively combined to ultimately produce high-performance semi-supervised wsd systems that exceed the performance of state-of-the-art supervised wsd techniques trained on the same sense-annotated data. we anticipate that our results and released software will also benefit evaluation practices for sense induction systems and those working in low-resource languages by demonstrating how to quickly produce accurate wsd systems with minimal annotation effort.",2016
learning concept graphs from online educational data,"this paper addresses an open challenge in educational data mining, i.e., the problem of automatically mapping online courses from different providers (universities, moocs, etc.) onto a universal space of concepts, and predicting latent prerequisite dependencies (directed links) among both concepts and courses. we propose a novel approach for inference within and across course-level and concept-level directed graphs. in the training phase, our system projects partially observed course-level prerequisite links onto directed concept-level links; in the testing phase, the induced concept-level links are used to infer the unknown course-level prerequisite links. whereas courses may be specific to one institution, concepts are shared across different providers. the bi-directional mappings enable our system to perform interlingua-style transfer learning, e.g. treating the concept graph as the interlingua and transferring the prerequisite relations across universities via the interlingua. experiments on our newly collected datasets of courses from mit, caltech, princeton and cmu show promising results.",2016
semantic visualization with neighborhood graph regularization,"visualization of high-dimensional data, such as text documents, is useful to map out the similarities among various data points. in the high-dimensional space, documents are commonly represented as bags of words, with dimensionality equal to the vocabulary size. classical approaches to document visualization directly reduce this into visualizable two or three dimensions. recent approaches consider an intermediate representation in topic space, between word space and visualization space, which preserves the semantics by topic modeling. while aiming for a good fit between the model parameters and the observed data, previous approaches have not considered the local consistency among data instances. we consider the problem of semantic visualization by jointly modeling topics and visualization on the intrinsic document manifold, modeled using a neighborhood graph. each document has both a topic distribution and visualization coordinate. specifically, we propose an unsupervised probabilistic model, called semafore, which aims to preserve the manifold in the lower-dimensional spaces through a neighborhood regularization framework designed for the semantic visualization task. to validate the efficacy of semafore, our comprehensive experiments on a number of real-life text datasets of news articles and web pages show that the proposed methods outperform the state-of-the-art baselines on objective evaluation metrics.",2016
exploiting causality for selective belief filtering in dynamic bayesian networks,"dynamic bayesian networks (dbns) are a general model for stochastic processes with partially observed states. belief filtering in dbns is the task of inferring the belief state (i.e. the probability distribution over process states) based on incomplete and noisy observations. this can be a hard problem in complex processes with large state spaces. in this article, we explore the idea of accelerating the filtering task by automatically exploiting causality in the process. we consider a specific type of causal relation, called passivity, which pertains to how state variables cause changes in other variables. we present the passivity-based selective belief filtering (psbf) method, which maintains a factored belief representation and exploits passivity to perform selective updates over the belief factors. psbf produces exact belief states under certain assumptions and approximate belief states otherwise, where the approximation error is bounded by the degree of uncertainty in the process. we show empirically, in synthetic processes with varying sizes and degrees of passivity, that psbf is faster than several alternative methods while achieving competitive accuracy. furthermore, we demonstrate how passivity occurs naturally in a complex system such as a multi-robot warehouse, and how psbf can exploit this to accelerate the filtering task.",2016
knowledge-based textual inference via parse-tree transformations,"textual inference is an important component in many applications for understanding natural language. classical approaches to textual inference rely on logical representations for meaning, which may be regarded as ""external"" to the natural language itself. however, practical applications usually adopt shallower lexical or lexical-syntactic representations, which correspond closely to language structure. in many cases, such approaches lack a principled meaning representation and inference framework. we describe an inference formalism that operates directly on language-based structures, particularly syntactic parse trees. new trees are generated by applying inference rules, which provide a unified representation for varying types of inferences. we use manual and automatic methods to generate these rules, which cover generic linguistic structures as well as specific lexical-based inferences. we also present a novel packed data-structure and a corresponding inference algorithm that allows efficient implementation of this formalism. we proved the correctness of the new algorithm and established its efficiency analytically and empirically. the utility of our approach was illustrated on two tasks: unsupervised relation extraction from a large corpus, and the recognizing textual entailment (rte) benchmarks.",2015
solving #sat and maxsat by dynamic programming,"we look at dynamic programming algorithms for propositional model counting, also called #sat, and maxsat. tools from graph structure theory, in particular treewidth, have been used to successfully identify tractable cases in many subfields of ai, including sat, constraint satisfaction problems (csp), bayesian reasoning, and planning. in this paper we attack #sat and maxsat using similar, but more modern, graph structure tools. the tractable cases will include formulas whose class of incidence graphs have not only unbounded treewidth but also unbounded clique-width. we show that our algorithms extend all previous results for maxsat and #sat achieved by dynamic programming along structural decompositions of the incidence graph of the input formula. we present some limited experimental results, comparing implementations of our algorithms to state-of-the-art #sat and maxsat solvers, as a proof of concept that warrants further research.",2015
word vs. class-based word sense disambiguation,"as empirically demonstrated by the word sense disambiguation (wsd) tasks of the last senseval/semeval exercises, assigning the appropriate meaning to words in context has resisted all attempts to be successfully addressed. many authors argue that one possible reason could be the use of inappropriate sets of word meanings. in particular, wordnet has been used as a de-facto standard repository of word meanings in most of these tasks. thus, instead of using the word senses defined in wordnet, some approaches have derived semantic classes representing groups of word senses. however, the meanings represented by wordnet have been only used for wsd at a very fine-grained sense level or at a very coarse-grained semantic class level (also called supersenses). we suspect that an appropriate level of abstraction could be on between both levels. the contributions of this paper are manifold. first, we propose a simple method to automatically derive semantic classes at intermediate levels of abstraction covering all nominal and verbal wordnet meanings. second, we empirically demonstrate that our automatically derived semantic classes outperform classical approaches based on word senses and more coarse-grained sense groupings. third, we also demonstrate that our supervised wsd system benefits from using these new semantic classes as additional semantic features while reducing the amount of training examples. finally, we also demonstrate the robustness of our supervised semantic class-based wsd system when tested on out of domain corpus.",2015
achieving goals quickly using real-time search: experimental results in video games,"in real-time domains such as video games, planning happens concurrently with execution and the planning algorithm has a strictly bounded amount of time before it must return the next action for the agent to execute. we explore the use of real-time heuristic search in two benchmark domains inspired by video games. unlike classic benchmarks such as grid pathfinding and the sliding tile puzzle, these new domains feature exogenous change and directed state space graphs. we consider the setting in which planning and acting are concurrent and we use the natural objective of minimizing goal achievement time. using both the classic benchmarks and the new domains, we investigate several enhancements to a leading real-time search algorithm, lss-lrta*. we show experimentally that 1) it is better to plan after each action or to use a dynamically sized lookahead, 2) a*-based lookahead can cause undesirable actions to be selected, and 3) on-line de-biasing of the heuristic can lead to improved performance. we hope this work encourages future research on applying real-time search in dynamic domains.",2015
leveraging online user feedback to improve statistical machine translation,"in this article we present a three-step methodology for dynamically improving a statistical machine translation (smt) system by incorporating human feedback in the form of free edits on the system translations. we target at feedback provided by casual users, which is typically error-prone. thus, we first propose a filtering step to automatically identify the better user-edited translations and discard the useless ones. a second step produces a pivot-based alignment between source and user-edited sentences, focusing on the errors made by the system. finally, a third step produces a new translation model and combines it linearly with the one from the original system. we perform a thorough evaluation on a real-world dataset collected from the reverso.net translation service and show that every step in our methodology contributes significantly to improve a general purpose smt system. interestingly, the quality improvement is not only due to the increase of lexical coverage, but to a better lexical selection, reordering, and morphology. finally, we show the robustness of the methodology by applying it to a different scenario, in which the new examples come from an automatically web-crawled parallel corpus. using exactly the same architecture and models provides again a significant improvement of the translation quality of a general purpose baseline smt system.",2015
expressiveness of two-valued semantics for abstract dialectical frameworks,"we analyse the expressiveness of brewka and woltran's abstract dialectical frameworks for two-valued semantics. by expressiveness we mean the ability to encode a desired set of two-valued interpretations over a given propositional vocabulary a using only atoms from a. we also compare adfs' expressiveness with that of (the two-valued semantics of) abstract argumentation frameworks, normal logic programs and propositional logic. while the computational complexity of the two-valued model existence problem for all these languages is (almost) the same, we show that the languages form a neat hierarchy with respect to their expressiveness. we then demonstrate that this hierarchy collapses once we allow to introduce a linear number of new vocabulary elements. we finally also analyse and compare the representational succinctness of adfs (for two-valued model semantics), that is, their capability to represent two-valued interpretation sets in a space-efficient manner.",2015
decision making with dynamic uncertain events,"when to make a decision is a key question in decision making problems characterized by uncertainty. in this paper we deal with decision making in environments where information arrives dynamically. we address the tradeoff between waiting and stopping strategies. on the one hand, waiting to obtain more information reduces uncertainty, but it comes with a cost. stopping and making a decision based on an expected utility reduces the cost of waiting, but the decision is based on uncertain information. we propose an optimal algorithm and two approximation algorithms. we prove that one approximation is optimistic - waits at least as long as the optimal algorithm, while the other is pessimistic - stops not later than the optimal algorithm. we evaluate our algorithms theoretically and empirically and show that the quality of the decision in both approximations is near-optimal and much faster than the optimal algorithm. also, we can conclude from the experiments that the cost function is a key factor to chose the most effective algorithm.",2015
relations between spatial calculi about directions and orientations,"qualitative spatial descriptions characterize essential properties of spatial objects or configurations by relying on relative comparisons rather than measuring. typically, in qualitative approaches only relatively coarse distinctions between configurations are made. qualitative spatial knowledge can be used to represent incomplete and underdetermined knowledge in a systematic way. this is especially useful if the task is to describe features of classes of configurations rather than individual configurations. although reasoning with them is generally np-hard, relative directions are important because they play a key role in human spatial descriptions and there are several approaches how to represent them using qualitative methods. in these approaches directions between spatial locations can be expressed as constraints over infinite domains, e.g. the euclidean plane. the theory of relation algebras has been successfully applied to this field. viewing relation algebras as universal algebras and applying and modifying standard tools from universal algebra in this work, we (re)define notions of qualitative constraint calculus, of homomorphism between calculi, and of quotient of calculi. based on this method we derive important properties for spatial calculi from corresponding properties of related calculi. from a conceptual point of view these formal mappings between calculi are a means to translate between different granularities.",2015
pagoda: pay-as-you-go ontology query answering using a datalog reasoner,"answering conjunctive queries over ontology-enriched datasets is a core reasoning task for many applications. query answering is, however, computationally very expensive, which has led to the development of query answering procedures that sacrifice either expressive power of the ontology language, or the completeness of query answers in order to improve scalability. in this paper, we describe a hybrid approach to query answering over owl 2 ontologies that combines a datalog reasoner with a fully-fledged owl 2 reasoner in order to provide scalable `pay-as-you-go' performance. the key feature of our approach is that it delegates the bulk of the computation to the datalog reasoner and resorts to expensive owl 2 reasoning only as necessary to fully answer the query. furthermore, although our main goal is to efficiently answer queries over owl 2 ontologies and data, our technical results are very general and our approach is applicable to first-order knowledge representation languages that can be captured by rules allowing for existential quantification and disjunction in the head; our only assumption is the availability of a datalog reasoner and a fully-fledged reasoner for the language of interest, both of which are used as `black boxes'. we have implemented our techniques in the pagoda system, which combines the datalog reasoner rdfox and the owl 2 reasoner hermit. our extensive evaluation shows that pagoda succeeds in providing scalable pay-as-you-go query answering for a wide range of owl 2 ontologies, datasets and queries.",2015
continuing plan quality optimisation,"finding high quality plans for large planning problems is hard. although some current anytime planners are often able to improve plans quickly, they tend to reach a limit at which the plans produced are still very far from the best possible, but these planners fail to find any further improvement, even when given several hours of runtime. we present an approach to continuing plan quality optimisation at larger time scales, and its implementation in a system called bdpo2. key to this approach is a decomposition into subproblems of improving parts of the current best plan. the decomposition is based on block deordering, a form of plan deordering which identifies hierarchical plan structure. bdpo2 can be seen as an application of the large neighbourhood search (lns) local search strategy to planning, where the neighbourhood of a plan is defined by replacing one or more subplans with improved subplans. on-line learning is also used to adapt the strategy for selecting subplans and subplanners over the course of plan optimisation. even starting from the best plans found by other means, bdpo2 is able to continue improving plan quality, often producing better plans than other anytime planners when all are given enough runtime. the best results, however, are achieved by a combination of different techniques working together.",2015
constraining information sharing to improve cooperative information gathering,"this paper considers the problem of cooperation between self-interested agents in acquiring better information regarding the nature of the different options and opportunities available to them. by sharing individual findings with others, the agents can potentially achieve a substantial improvement in overall and individual expected benefits. unfortunately, it is well known that with self-interested agents equilibrium considerations often dictate solutions that are far from the fully cooperative ones, hence the agents do not manage to fully exploit the potential benefits encapsulated in such cooperation. in this paper we introduce, analyze and demonstrate the benefit of five methods aiming to improve cooperative information gathering. common to all five that they constrain and limit the information sharing process. nevertheless, the decrease in benefit due to the limited sharing is outweighed by the resulting substantial improvement in the equilibrium individual information gathering strategies. the equilibrium analysis given in the paper, which, in itself is an important contribution to the study of cooperation between self-interested agents, enables demonstrating that for a wide range of settings an improved individual expected benefit is achieved for all agents when applying each of the five methods.",2015
weighted regret-based likelihood: a new approach to describing uncertainty,"recently, halpern and leung suggested representing uncertainty by a set of weighted probability measures, and suggested a way of making decisions based on this representation of uncertainty: maximizing weighted regret. their paper does not answer an apparently simpler question: what it means, according to this representation of uncertainty, for an event e to be more likely than an event e'. in this paper, a notion of comparative likelihood when uncertainty is represented by a set of weighted probability measures is defined. it generalizes the ordering defined by probability (and by lower probability) in a natural way; a generalization of upper probability can also be defined. a complete axiomatic characterization of this notion of regret-based likelihood is given.",2015
possible and necessary winners of partial tournaments,"we study the problem of computing possible and necessary winners for partially specified weighted and unweighted tournaments. this problem arises naturally in elections with incompletely specified votes, partially completed sports competitions, and more generally in any scenario where the outcome of some pairwise comparisons is not yet fully known. we specifically consider a number of well-known solution concepts---including the uncovered set, borda, ranked pairs, and maximin---and show that for most of them, possible and necessary winners can be identified in polynomial time. these positive algorithmic results stand in sharp contrast to earlier results concerning possible and necessary winners given partially specified preference profiles.",2015
pay-as-you-go description logic reasoning by coupling tableau and saturation procedures,"nowadays, saturation-based reasoners for the owl el profile of the web ontology language are able to handle large ontologies such as snomed very efficiently. however, it is currently unclear how saturation-based reasoning procedures can be extended to very expressive description logics such as sroiq--the logical underpinning of the current and second iteration of the web ontology language. tableau-based procedures, on the other hand, are not limited to specific description logic languages or owl profiles, but even highly optimised tableau-based reasoners might not be efficient enough to handle large ontologies such as snomed. in this paper, we present an approach for tightly coupling tableau- and saturation-based procedures that we implement in the owl dl reasoner konclude. our detailed evaluation shows that this combination significantly improves the reasoning performance for a wide range of ontologies.",2015
compressing optimal paths with run length encoding,"we introduce a novel approach to compressed path databases, space efficient oracles used to very quickly identify the first edge on a shortest path. our algorithm achieves query running times on the 100 nanosecond scale, being significantly faster than state-of-the-art first-move oracles from the literature. space consumption is competitive, due to a compression approach that rearranges rows and columns in a first-move matrix and then performs run length encoding (rle) on the contents of the matrix. one variant of our implemented system was, by a convincing margin, the fastest entry in the 2014 grid-based path planning competition. we give a first tractability analysis for the compression scheme used by our algorithm. we study the complexity of computing a database of minimum size for general directed and undirected graphs. we find that in both cases the problem is np-complete. we also show that, for graphs which can be decomposed along articulation points, the problem can be decomposed into independent parts, with a corresponding reduction in its level of difficulty. in particular, this leads to simple and tractable algorithms with linear running time which yield optimal compression results for trees.",2015
"on a practical, integer-linear programming model for delete-free tasks and its use as a heuristic for cost-optimal planning","we propose a new integer-linear programming model for the delete relaxation in cost-optimal planning. while a straightforward ip for the delete relaxation is impractical, our enhanced model incorporates variable reduction techniques based on landmarks, relevance-based constraints, dominated action elimination, immediate action application, and inverse action constraints, resulting in an ip that can be used to directly solve delete-free planning problems. we show that our ip model is competitive with previous state-of-the-art solvers for delete-free problems. the lp-relaxation of the ip model is often a very good approximation to the ip, providing an approach to approximating the optimal value of the delete-free task that is complementary to the well-known lm-cut heuristic. we also show that constraints that partially consider delete effects can be added to our ip/lp models. we embed the new ip/lp models into a forward-search based planner, and show that the performance of the resulting planner on standard ipc benchmarks is comparable with the state-of-the-art for cost-optimal planning.",2015
coactive learning,"we propose coactive learning as a model of interaction between a learning system and a human user, where both have the common goal of providing results of maximum utility to the user. interactions in the coactive learning model take the following form: at each step, the system (e.g. search engine) receives a context (e.g. query) and predicts an object (e.g. ranking); the user responds by correcting the system if necessary, providing a slightly improved but not necessarily optimal object as feedback. we argue that such preference feedback can be inferred in large quantity from observable user behavior (e.g., clicks in web search), unlike the optimal feedback required in the expert model or the cardinal valuations required for bandit learning. despite the relaxed requirements for the feedback, we show that it is possible to adapt many existing online learning algorithms to the coactive framework. in particular, we provide algorithms that achieve square root regret in terms of cardinal utility, even though the learning algorithm never observes cardinal utility values directly. we also provide an algorithm with logarithmic regret in the case of strongly convex loss functions. an extensive empirical study demonstrates the applicability of our model and algorithms on a movie recommendation task, as well as ranking for web search.",2015
learning relational event models from video,"event models obtained automatically from video can be used in applications ranging from abnormal event detection to content based video retrieval. when multiple agents are involved in the events, characterizing events naturally suggests encoding interactions as relations. learning event models from this kind of relational spatio-temporal data using relational learning techniques such as inductive logic programming (ilp) hold promise, but have not been successfully applied to very large datasets which result from video data. in this paper, we present a novel framework remind (relational event model induction) for supervised relational learning of event models from large video datasets using ilp. efficiency is achieved through the learning from interpretations setting and using a typing system that exploits the type hierarchy of objects in a domain. the use of types also helps prevent over generalization. furthermore, we also present a type-refining operator and prove that it is optimal. the learned models can be used for recognizing events from previously unseen videos. we also present an extension to the framework by integrating an abduction step that improves the learning performance when there is noise in the input data. the experimental results on several hours of video data from two challenging real world domains (an airport domain and a physical action verbs domain) suggest that the techniques are suitable to real world scenarios.",2015
the ceteris paribus structure of logics of game forms,"the article introduces a ceteris paribus modal logic, called cp, interpreted on the equivalence classes induced by finite sets of propositional atoms. this logic is studied and then used to embed three logics of strategic interaction, namely atemporal stit, the coalition logic of propositional control (cl&#8722;pc) and the starless fragment of the dynamic logic of propositional assignments (dl&#8722;pa). the embeddings highlight a common ceteris paribus structure underpinning the key operators of all these apparently very different logics and show, we argue, remarkable similarities behind some of the most influential formalisms for reasoning about strategic interaction",2015
clause elimination for sat and qsat,"the famous archetypical np-complete problem of boolean satisfiability (sat) and its pspace-complete generalization of quantified boolean satisfiability (qsat) have become central declarative programming paradigms through which real-world instances of various computationally hard problems can be efficiently solved. this success has been achieved through several breakthroughs in practical implementations of decision procedures for sat and qsat, that is, in sat and qsat solvers. here, simplification techniques for conjunctive normal form (cnf) for sat and for prenex conjunctive normal form (pcnf) for qsat---the standard input formats of sat and qsat solvers---have recently proven very effective in increasing solver efficiency when applied before (i.e., in preprocessing) or during (i.e., in inprocessing) satisfiability search. in this article, we develop and analyze clause elimination procedures for pre- and inprocessing. clause elimination procedures form a family of (p)cnf formula simplification techniques which remove clauses that have specific (in practice polynomial-time) redundancy properties while maintaining the satisfiability status of the formulas. extending known procedures such as tautology, subsumption, and blocked clause elimination, we introduce novel elimination procedures based on asymmetric variants of these techniques, and also develop a novel family of so-called covered clause elimination procedures, as well as natural liftings of the cnf-level procedures to pcnf. we analyze the considered clause elimination procedures from various perspectives. furthermore, for the variants not preserving logical equivalence under clause elimination, we show how to reconstruct solutions to original cnfs from satisfying assignments to simplified cnfs, which is important for practical applications for the procedures. complementing the more theoretical analysis, we present results on an empirical evaluation on the practical importance of the clause elimination procedures in terms of the effect on solver runtimes on standard real-world application benchmarks. it turns out that the importance of applying the clause elimination procedures developed in this work is empirically emphasized in the context of state-of-the-art qsat solving.",2015
using machine translation to provide target-language edit hints in computer aided translation based on translation memories,"this paper explores the use of general-purpose machine translation (mt) in assisting the users of computer-aided translation (cat) systems based on translation memory (tm) to identify the target words in the translation proposals that need to be changed (either replaced or removed) or kept unedited, a task we term as ""word-keeping recommendation"". mt is used as a black box to align source and target sub-segments on the fly in the translation units (tus) suggested to the user. source-language (sl) and target-language (tl) segments in the matching tus are segmented into overlapping sub-segments of variable length and machine-translated into the tl and the sl, respectively. the bilingual sub-segments obtained and the matching between the sl segment in the tu and the segment to be translated are employed to build the features that are then used by a binary classifier to determine the target words to be changed and those to be kept unedited. in this approach, mt results are never presented to the translator. two approaches are presented in this work: one using a word-keeping recommendation system which can be trained on the tm used with the cat system, and a more basic approach which does not require any training. experiments are conducted by simulating the translation of texts in several language pairs with corpora belonging to different domains and using three different mt systems. we compare the performance obtained to that of previous works that have used statistical word alignment for word-keeping recommendation, and show that the mt-based approaches presented in this paper are more accurate in most scenarios. in particular, our results confirm that the mt-based approaches are better than the alignment-based approach when using models trained on out-of-domain tms. additional experiments were performed to check how dependent the mt-based recommender is on the language pair and mt system used for training. these experiments confirm a high degree of reusability of the recommendation models across various mt systems, but a low level of reusability across language pairs.",2015
probabilistic inference techniques for scalable multiagent decision making,"decentralized pomdps provide an expressive framework for multiagent sequential decision making. however, the complexity of these models---nexp-complete even for two agents---has limited their scalability. we present a promising new class of approximation algorithms by developing novel connections between multiagent planning and machine learning. we show how the multiagent planning problem can be reformulated as inference in a mixture of dynamic bayesian networks (dbns). this planning-as-inference approach paves the way for the application of efficient inference techniques in dbns to multiagent decision making. to further improve scalability, we identify certain conditions that are sufficient to extend the approach to multiagent systems with dozens of agents. specifically, we show that the necessary inference within the expectation-maximization framework can be decomposed into processes that often involve a small subset of agents, thereby facilitating scalability. we further show that a number of existing multiagent planning models satisfy these conditions. experiments on large planning benchmarks confirm the benefits of our approach in terms of runtime and scalability with respect to existing techniques.",2015
model theory of xpath on data trees. part i: bisimulation and characterization,"we investigate model theoretic properties of xpath with data (in)equality tests over the class of data trees, i.e., the class of trees where each node contains a label from a finite alphabet and a data value from an infinite domain. we provide notions of (bi)simulations for xppath logics containing the child, parent, ancestor and descendant axes to navigate the tree. we show that these notions precisely characterize the equivalence relation associated with each logic. we study formula complexity measures consisting of the number of nested axes and nested subformulas in a formula; these notions are akin to the notion of quantifier rank in first-order logic. we show characterization results for fine grained notions of equivalence and (bi)simulation that take into account these complexity measures. we also prove that positive fragments of these logics correspond to the formulas preserved under (non-symmetric) simulations. we show that the logic including the child axis is equivalent to the fragment of first-order logic invariant under the corresponding notion of bisimulation. if upward navigation is allowed the characterization fails but a weaker result can still be established. these results hold both over the class of possibly infinite data trees and over the class of finite data trees. besides their intrinsic theoretical value, we argue that bi-simulations are useful tools to prove (non)expressivity results for the logics studied here, and we substantiate this claim with examples.",2015
regular path queries in lightweight description logics: complexity and algorithms,"conjunctive regular path queries are an expressive extension of the well-known class of conjunctive queries. such queries have been extensively studied in the (graph) database community, since they support a controlled form of recursion and enable sophisticated path navigation. somewhat surprisingly, there has been little work aimed at using such queries in the context of description logic (dl) knowledge bases, particularly for the lightweight dls that are considered best suited for data-intensive applications. this paper aims to bridge this gap by providing algorithms and tight complexity bounds for answering two-way conjunctive regular path queries over dl knowledge bases formulated in lightweight dls of the dl-lite and el families. our results demonstrate that in data complexity, the cost of moving to this richer query language is as low as one could wish for: the problem is nl-complete for dl-lite and p-complete for el. the combined complexity of query answering increases from np- to pspace-complete, but for two-way regular path queries (without conjunction), we show that query answering is tractable even with respect to combined complexity. our results reveal two-way conjunctive regular path queries as a promising language for querying data enriched by ontologies formulated in dls of the dl-lite and el families or the corresponding owl 2 ql and el profiles.",2015
approximate value iteration with temporally extended actions,"temporally extended actions have proven useful for reinforcement learning, but their duration also makes them valuable for efficient planning. the options framework provides a concrete way to implement and reason about temporally extended actions. existing literature has demonstrated the value of planning with options empirically, but there is a lack of theoretical analysis formalizing when planning with options is more efficient than planning with primitive actions. we provide a general analysis of the convergence rate of a popular approximate value iteration (avi) algorithm called fitted value iteration (fvi) with options. our analysis reveals that longer duration options and a pessimistic estimate of the value function both lead to faster convergence. furthermore, options can improve convergence even when they are suboptimal and sparsely distributed throughout the state-space. next we consider the problem of generating useful options for planning based on a subset of landmark states. this suggests a new algorithm, landmark-based avi (lavi), that represents the value function only at the landmark states. we analyze both fvi and lavi using the proposed landmark-based options and compare the two algorithms. our experimental results in three different domains demonstrate the key properties from the analysis. our theoretical and experimental results demonstrate that options can play an important role in avi by decreasing approximation error and inducing fast convergence.",2015
bypassing combinatorial protections: polynomial-time algorithms for single-peaked electorates,"for many election systems, bribery (and related) attacks have been shown np-hard using constructions on combinatorially rich structures such as partitions and covers. this paper shows that for voters who follow the most central political-science model of electorates---single-peaked preferences---those hardness protections vanish. by using single-peaked preferences to simplify combinatorial covering challenges, we for the first time show that np-hard bribery problems---including those for kemeny and llull elections---fall to polynomial time for single-peaked electorates. by using single-peaked preferences to simplify combinatorial partition challenges, we for the first time show that np-hard partition-of-voters problems fall to polynomial time for single-peaked electorates. we show that for single-peaked electorates, the winner problems for dodgson and kemeny elections, though theta-two-complete in the general case, fall to polynomial time. and we completely classify the complexity of weighted coalition manipulation for scoring protocols in single-peaked electorates.",2015
satsisfiability and systematicity,"we introduce a new notion of systematicity for satisfiability algorithms with restarts, saying that an algorithm is strongly systematic if it is systematic independent of restart policy but weakly systematic if it is systematic for some restart policies but not others. we show that existing satisfiability engines are generally only weakly systematic, and describe flex, a strongly systematic algorithm that uses an amount of memory polynomial in the size of the problem. on large number factoring problems, flex appears to outperform weakly systematic approaches.",2015
itsat: an efficient sat-based temporal planner,"planning as satisfiability is known as an efficient approach to deal with many types of planning problems. however, this approach has not been competitive with the state-space based methods in temporal planning. this paper describes itsat as an efficient sat-based (satisfiability based) temporal planner capable of temporally expressive planning. the novelty of itsat lies in the way it handles temporal constraints of given problems without getting involved in the difficulties of introducing continuous variables into the corresponding satisfiability problems. we also show how, as in sat-based classical planning, carefully devised preprocessing and encoding schemata can considerably improve the efficiency of sat-based temporal planning. we present two preprocessing methods for mutex relation extraction and action compression. we also show that the separation of causal and temporal reasoning enables us to employ compact encodings that are based on the concept of parallel execution semantics. although such encodings have been shown to be quite effective in classical planning, itsat is the first temporal planner utilizing this type of encoding. our empirical results show that not only does itsat outperform the state-of-the-art temporally expressive planners, it is also competitive with the fast temporal planners that cannot handle required concurrency.",2015
placement of loading stations for electric vehicles: no detours necessary!,"compared to conventional cars, electric vehicles (evs) still suffer from considerably shorter cruising ranges. combined with the sparsity of battery loading stations, the complete transition to e-mobility still seems a long way to go. in this paper, we consider the problem of placing as few loading stations as possible so that on any shortest path there are sufficiently many not to run out of energy. we show how to model this problem and introduce heuristics which provide close-to-optimal solutions even in large road networks.",2015
evolutionary dynamics of multi-agent learning: a survey,"the interaction of multiple autonomous agents gives rise to highly dynamic and nondeterministic environments, contributing to the complexity in applications such as automated financial markets, smart grids, or robotics. due to the sheer number of situations that may arise, it is not possible to foresee and program the optimal behaviour for all agents beforehand. consequently, it becomes essential for the success of the system that the agents can learn their optimal behaviour and adapt to new situations or circumstances. the past two decades have seen the emergence of reinforcement learning, both in single and multi-agent settings, as a strong, robust and adaptive learning paradigm. progress has been substantial, and a wide range of algorithms are now available. an important challenge in the domain of multi-agent learning is to gain qualitative insights into the resulting system dynamics. in the past decade, tools and methods from evolutionary game theory have been successfully employed to study multi-agent learning dynamics formally in strategic interactions. this article surveys the dynamical models that have been derived for various multi-agent reinforcement learning algorithms, making it possible to study and compare them qualitatively. furthermore, new learning algorithms that have been introduced using these evolutionary game theoretic tools are reviewed. the evolutionary models can be used to study complex strategic interactions. examples of such analysis are given for the domains of automated trading in stock markets and collision avoidance in multi-robot systems. the paper provides a roadmap on the progress that has been achieved in analysing the evolutionary dynamics of multi-agent learning by highlighting the main results and accomplishments.",2015
tree-width and the computational complexity of map approximations in bayesian networks,"the problem of finding the most probable explanation to a designated set of variables given partial evidence (the map problem) is a notoriously intractable problem in bayesian networks, both to compute exactly and to approximate. it is known, both from theoretical considerations and from practical experience, that low tree-width is typically an essential prerequisite to efficient exact computations in bayesian networks. in this paper we investigate whether the same holds for approximating map. we define four notions of approximating map (by value, structure, rank, and expectation) and argue that all of them are intractable in general. we prove that efficient value-approximations, structure-approximations, and rank-approximations of map instances with high tree-width will violate the exponential time hypothesis. in contrast, we show that map can sometimes be efficiently expectation-approximated, even in instances with high tree-width, if the most probable explanation has a high probability. we introduce the complexity class fert, analogous to the class ftp, to capture this notion of fixed-parameter expectation-approximability. we suggest a road-map to future research that yields fixed-parameter tractable results for expectation-approximate map, even in graphs with high tree-width.",2015
mechanisms for multi-unit combinatorial auctions with a few distinct goods,"we design and analyze deterministic truthful approximation mechanisms for multi-unit combinatorial auctions involving only a constant number of distinct goods, each in arbitrary limited supply. prospective buyers (bidders) have preferences over multisets of items, i.e., for more than one unit per distinct good. our objective is to determine allocations of multisets that maximize the social welfare. our main results are for multi-minded and submodular bidders. in the first setting each bidder has a positive value for being allocated one multiset from a prespecified demand set of alternatives. in the second setting each bidder is associated to a submodular valuation function that defines his value for the multiset he is allocated. for multi-minded bidders, we design a truthful fptas that fully optimizes the social welfare, while violating the supply constraints on goods within factor (1+e), for any fixed e>0 (i.e., the approximation applies to the constraints and not to the social welfare). this result is best possible, in that full optimization is impossible without violating the supply constraints. for submodular bidders, we obtain a ptas that approximates the optimum social welfare within factor (1+e), for any fixed e>0, without violating the supply constraints. this result is best possible as well. our allocation algorithms are maximal-in-range and yield truthful mechanisms, when paired with vickrey-clarke-groves payments.",2015
autofolio: an automatically configured algorithm selector,"algorithm selection (as) techniques -- which involve choosing from a set of algorithms the one expected to solve a given problem instance most efficiently -- have substantially improved the state of the art in solving many prominent ai problems, such as sat, csp, asp, maxsat and qbf. although several as procedures have been introduced, not too surprisingly, none of them dominates all others across all as scenarios. furthermore, these procedures have parameters whose optimal values vary across as scenarios. this holds specifically for the machine learning techniques that form the core of current as procedures, and for their hyperparameters. therefore, to successfully apply as to new problems, algorithms and benchmark sets, two questions need to be answered: (i) how to select an as approach and (ii) how to set its parameters effectively. we address both of these problems simultaneously by using automated algorithm configuration. specifically, we demonstrate that we can automatically configure claspfolio 2, which implements a large variety of different as approaches and their respective parameters in a single, highly-parameterized algorithm framework. our approach, dubbed autofolio, allows researchers and practitioners across a broad range of applications to exploit the combined power of many different as methods. we demonstrate autofolio can significantly improve the performance of claspfolio 2 on 8 out of the 13 scenarios from the algorithm selection library, leads to new state-of-the-art algorithm selectors for 7 of these scenarios, and matches state-of-the-art performance (statistically) on all other scenarios. compared to the best single algorithm for each as scenario, autofolio achieves average speedup factors between 1.3 and 15.4.",2015
belief change with uncertain action histories,"we consider the iterated belief change that occurs following an alternating sequence of actions and observations. at each instant, an agent has beliefs about the actions that have occurred as well as beliefs about the resulting state of the world. we represent such problems by a sequence of ranking functions, so an agent assigns a quantitative plausibility value to every action and every state at each point in time. the resulting formalism is able to represent fallible belief, erroneous perception, exogenous actions, and failed actions. we illustrate that our framework is a generalization of several existing approaches to belief change, and it appropriately captures the non-elementary interaction between belief update and belief revision.",2015
coherent predictive inference under exchangeability with imprecise probabilities,"coherent reasoning under uncertainty can be represented in a very general manner by coherent sets of desirable gambles. in a context that does not allow for indecision, this leads to an approach that is mathematically equivalent to working with coherent conditional probabilities. if we do allow for indecision, this leads to a more general foundation for coherent (imprecise-)probabilistic inference. in this framework, and for a given finite category set, coherent predictive inference under exchangeability can be represented using bernstein coherent cones of multivariate polynomials on the simplex generated by this category set. this is a powerful generalisation of de finetti's representation theorem allowing for both imprecision and indecision. we define an inference system as a map that associates a bernstein coherent cone of polynomials with every finite category set. many inference principles encountered in the literature can then be interpreted, and represented mathematically, as restrictions on such maps. we discuss, as particular examples, two important inference principles: representation insensitivitya strengthened version of walley's representation invarianceand specificity. we show that there is an infinity of inference systems that satisfy these two principles, amongst which we discuss in particular the skeptically cautious inference system, the inference systems corresponding to (a modified version of) walley and bernard's imprecise dirichlet multinomial models (idmm), the skeptical idmm inference systems, and the haldane inference system. we also prove that the latter produces the same posterior inferences as would be obtained using haldane's improper prior, implying that there is an infinity of proper priors that produce the same coherent posterior inferences as haldane's improper one. finally, we impose an additional inference principle that allows us to characterise uniquely the immediate predictions for the idmm inference systems.",2015
deterministic oversubscription planning as heuristic search: abstractions and reformulations,"while in classical planning the objective is to achieve one of the equally attractive goal states at as low total action cost as possible, the objective in deterministic oversubscription planning (osp) is to achieve an as valuable as possible subset of goals within a fixed allowance of the total action cost. although numerous applications in various fields share the latter objective, no substantial algorithmic advances have been made in deterministic osp. tracing the key sources of progress in classical planning, we identify a severe lack of effective domain-independent approximations for osp. with our focus here on optimal planning, our goal is to bridge this gap. two classes of approximation techniques have been found especially useful in the context of optimal classical planning: those based on state-space abstractions and those based on logical landmarks for goal reachability. the question we study here is whether some similar-in-spirit, yet possibly mathematically different, approximation techniques can be developed for osp. in the context of abstractions, we define the notion of additive abstractions for osp, study the complexity of deriving effective abstractions from a rich space of hypotheses, and reveal some substantial, empirically relevant islands of tractability. in the context of landmarks, we show how standard goal-reachability landmarks of certain classical planning tasks can be compiled into the osp task of interest, resulting in an equivalent osp task with a lower cost allowance, and thus with a smaller search space. our empirical evaluation confirms the effectiveness of the proposed techniques, and opens a wide gate for further developments in oversubscription planning.",2015
agnostic pointwise-competitive selective classification,"pointwise-competitive classifier from class f is required to classify identically to the best classifier in hindsight from f. for noisy, agnostic settings we present a strategy for learning pointwise-competitive classifiers from a finite training sample provided that the classifier can abstain from prediction at a certain region of its choice. for some interesting hypothesis classes and families of distributions, the measure of this rejected region is shown to be diminishing at a fast rate, with high probability. exact implementation of the proposed learning strategy is dependent on an erm oracle that can be hard to compute in the agnostic case. we thus consider a heuristic approximation procedure that is based on svms, and show empirically that this algorithm consistently outperforms a traditional rejection mechanism based on distance from decision boundary.",2015
on the subexponential-time complexity of csp,"not all np-complete problems share the same practical hardness with respect to exact computation. whereas some np-complete problems are amenable to efficient computational methods, others are yet to show any such sign. it becomes a major challenge to develop a theoretical framework that is more fine-grained than the theory of np-completeness, and that can explain the distinction between the exact complexities of various np-complete problems. this distinction is highly relevant for constraint satisfaction problems under natural restrictions, where various shades of hardness can be observed in practice. acknowledging the np-hardness of such problems, one has to look beyond polynomial time computation. the theory of subexponential-time complexity provides such a framework, and has been enjoying increasing popularity in complexity theory. an instance of the constraint satisfaction problem with n variables over a domain of d values can be solved by brute-force in dn steps (omitting a polynomial factor). in this paper we study the existence of subexponential-time algorithms, that is, algorithms running in do(n) steps, for various natural restrictions of the constraint satisfaction problem. we consider both the constraint satisfaction problem in which all the constraints are given extensionally as tables, and that in which all the constraints are given intensionally in the form of global constraints. we provide tight characterizations of the subexponential-time complexity of the aforementioned problems with respect to several natural structural parameters, which allows us to draw a detailed landscape of the subexponential-time complexity of the constraint satisfaction problem. our analysis provides fundamental results indicating whether and when one can significantly improve on the brute-force search approach for solving the constraint satisfaction problem.",2015
lazy model expansion: interleaving grounding with search,"finding satisfying assignments for the variables involved in a set of constraints can be cast as a (bounded) model generation problem: search for (bounded) models of a theory in some logic. the state-of-the-art approach for bounded model generation for rich knowledge representation languages like asp and fo(.) and a csp modeling language such as zinc, is ground-and-solve: reduce the theory to a ground or propositional one and apply a search algorithm to the resulting theory. an important bottleneck is the blow-up of the size of the theory caused by the grounding phase. lazily grounding the theory during search is a way to overcome this bottleneck. we present a theoretical framework and an implementation in the context of the fo(.) knowledge representation language. instead of grounding all parts of a theory, justifications are derived for some parts of it. given a partial assignment for the grounded part of the theory and valid justifications for the formulas of the non-grounded part, the justifications provide a recipe to construct a complete assignment that satisfies the non-grounded part. when a justification for a particular formula becomes invalid during search, a new one is derived; if that fails, the formula is split in a part to be grounded and a part that can be justified. experimental results illustrate the power and generality of this approach.",2015
revision by history,"this article proposes a solution to the problem of obtaining plausibility information, which is necessary to perform belief revision: given a sequence of revisions, together with their results, derive a possible initial order that has generated them; this is different from the usual assumption of starting from an all-equal initial order and modifying it by a sequence of revisions. four semantics for iterated revision are considered: natural, restrained, lexicographic and reinforcement. for each, a necessary and sufficient condition to the existence of an order generating a given history of revisions and results is proved. complexity is proved conp complete in all cases but one (reinforcement revision with unbounded sequence length).",2015
scheduling conservation designs for maximum flexibility via network cascade optimization,"one approach to conserving endangered species is to purchase and protect a set of land parcels in a way that maximizes the expected future population spread. unfortunately, an ideal set of parcels may have a cost that is beyond the immediate budget constraints and must thus be purchased incrementally. this raises the challenge of deciding how to schedule the parcel purchases in a way that maximizes the flexibility of budget usage while keeping population spread loss in control. in this paper, we introduce a formulation of this scheduling problem that does not rely on knowing the future budgets of an organization. in particular, we consider scheduling purchases in a way that achieves a population spread no less than desired but delays purchases as long as possible. such schedules offer conservation planners maximum flexibility and use available budgets in the most efficient way. we develop the problem formally as a stochastic optimization problem over a network cascade model describing a commonly used model of population spread. our solution approach is based on reducing the stochastic problem to a novel variant of the directed steiner tree problem, which we call the set-weighted directed steiner graph problem. we show that this problem is computationally hard, motivating the development of a primal-dual algorithm for the problem that computes both a feasible solution and a bound on the quality of an optimal solution. we evaluate the approach on both real and synthetic conservation data with a standard population spread model. the algorithm is shown to produce near optimal results and is much more scalable than more generic off-the-shelf optimizers. finally, we evaluate a variant of the algorithm to explore the trade-offs between budget savings and population growth.",2015
inferring team task plans from human meetings: a generative modeling approach with logic-based prior,"we aim to reduce the burden of programming and deploying autonomous systems to work in concert with people in time-critical domains such as military field operations and disaster response. deployment plans for these operations are frequently negotiated on-the-fly by teams of human planners. a human operator then translates the agreed-upon plan into machine instructions for the robots. we present an algorithm that reduces this translation burden by inferring the final plan from a processed form of the human team's planning conversation. our hybrid approach combines probabilistic generative modeling with logical plan validation used to compute a highly structured prior over possible plans, enabling us to overcome the challenge of performing inference over a large solution space with only a small amount of noisy data from the team planning session. we validate the algorithm through human subject experimentations and show that it is able to infer a human team's final plan with 86% accuracy on average. we also describe a robot demonstration in which two people plan and execute a first-response collaborative task with a pr2 robot. to the best of our knowledge, this is the first work to integrate a logical planning technique within a generative model to perform plan inference.",2015
computing convex coverage sets for faster multi-objective coordination,"in this article, we propose new algorithms for multi-objective coordination graphs (mo-cogs). key to the efficiency of these algorithms is that they compute a convex coverage set (ccs) instead of a pareto coverage set (pcs). not only is a ccs a sufficient solution set for a large class of problems, it also has important characteristics that facilitate more efficient solutions. we propose two main algorithms for computing a ccs in mo-cogs. convex multi-objective variable elimination (cmove) computes a ccs by performing a series of agent eliminations, which can be seen as solving a series of local multi-objective subproblems. variable elimination linear support (vels) iteratively identifies the single weight vector, w, that can lead to the maximal possible improvement on a partial ccs and calls variable elimination to solve a scalarized instance of the problem for w. vels is faster than cmove for small and medium numbers of objectives and can compute an &#949;-approximate ccs in a fraction of the runtime. in addition, we propose variants of these methods that employ and/or tree search instead of variable elimination to achieve memory efficiency. we analyze the runtime and space complexities of these methods, prove their correctness, and compare them empirically against a naive baseline and an existing pcs method, both in terms of memory-usage and runtime. our results show that, by focusing on the ccs, these methods achieve much better scalability in the number of agents than the current state of the art.",2015
modeling the lifespan of discourse entities with application to coreference resolution,"a discourse typically involves numerous entities, but few are mentioned more than once. distinguishing those that die out after just one mention (singleton) from those that lead longer lives (coreferent) would dramatically simplify the hypothesis space for coreference resolution models, leading to increased performance. to realize these gains, we build a classifier for predicting the singleton/coreferent distinction. the models feature representations synthesize linguistic insights about the factors affecting discourse entity lifespans (especially negation, modality, and attitude predication) with existing results about the benefits of surface (part-of-speech and n-gram-based) features for coreference resolution. the model is effective in its own right, and the feature representations help to identify the anchor phrases in bridging anaphora as well. furthermore, incorporating the model into two very different state-of-the-art coreference resolution systems, one rule-based and the other learning-based, yields significant performance improvements.",2015
a case-based reasoning framework to choose trust models for different e-marketplace environments,"the performance of trust models highly depend on the characteristics of the environments where they are applied. thus, it becomes challenging to choose a suitable trust model for a given e-marketplace environment, especially when ground truth about the agent (buyer and seller) behavior is unknown (called unknown environment). we propose a case-based reasoning framework to choose suitable trust models for unknown environments, based on the intuition that if a trust model performs well in one environment, it will do so in another similar environment. firstly, we build a case base with a number of simulated environments (with known ground truth) along with the trust models most suitable for each of them. given an unknown environment, case-based retrieval algorithms retrieve the most similar case(s), and the trust model of the most similar case(s) is chosen as the most suitable model for the unknown environment. evaluation results confirm the effectiveness of our framework in choosing suitable trust models for different e-marketplace environments.",2015
weighted electoral control,"although manipulation and bribery have been extensively studied under weighted voting, there has been almost no work done on election control under weighted voting. this is unfortunate, since weighted voting appears in many important natural settings. in this paper, we study the complexity of controlling the outcome of weighted elections through adding and deleting voters. we obtain polynomial-time algorithms, np-completeness results, and for many np-complete cases, approximation algorithms. in particular, for scoring rules we completely characterize the complexity of weighted voter control. our work shows that for quite a few important cases, either polynomial-time exact algorithms or polynomial-time approximation algorithms exist.",2015
distributed evaluation of nonmonotonic multi-context systems,"multi-context systems (mcss) are a formalism for systems consisting of knowledge bases (possibly heterogeneous and non-monotonic) that are interlinked via bridge rules, where the global system semantics emerges from the local semantics of the knowledge bases (also called contexts) in an equilibrium. while mcss and related formalisms are inherently targeted for distributed set- tings, no truly distributed algorithms for their evaluation were available. we address this short- coming and present a suite of such algorithms which includes a basic algorithm dmcs, an ad- vanced version dmcsopt that exploits topology-based optimizations, and a streaming algorithm dmcs-streaming that computes equilibria in packages of bounded size. the algorithms be- have quite differently in several respects, as experienced in thorough experimental evaluation of a system prototype. from the experimental results, we derive a guideline for choosing the appropriate algorithm and running mode in particular situations, determined by the parameter settings.",2015
"a compositional framework for grounding language inference, generation, and acquisition in video","we present an approach to simultaneously reasoning about a video clip and an entire natural-language sentence. the compositional nature of language is exploited to construct models which represent the meanings of entire sentences composed out of the meanings of the words in those sentences mediated by a grammar that encodes the predicate-argument relations. we demonstrate that these models faithfully represent the meanings of sentences and are sensitive to how the roles played by participants (nouns), their characteristics (adjectives), the actions performed (verbs), the manner of such actions (adverbs), and changing spatial relations between participants (prepositions) affect the meaning of a sentence and how it is grounded in video. we exploit this methodology in three ways. in the first, a video clip along with a sentence are taken as input and the participants in the event described by the sentence are highlighted, even when the clip depicts multiple similar simultaneous events. in the second, a video clip is taken as input without a sentence and a sentence is generated that describes an event in that clip. in the third, a corpus of video clips is paired with sentences which describe some of the events in those clips and the meanings of the words in those sentences are learned. we learn these meanings without needing to specify which attribute of the video clips each word in a given sentence refers to. the learned meaning representations are shown to be intelligible to humans.",2015
cooperative monitoring to diagnose multiagent plans,"diagnosing the execution of a multiagent plan (map) means identifying and explaining action failures (i.e., actions that did not reach their expected effects). current approaches to map diagnosis are substantially centralized, and assume that action failures are independent of each other. in this paper, the diagnosis of maps, executed in a dynamic and partially observable environment, is addressed in a fully distributed and asynchronous way; in addition, action failures are no longer assumed as independent of each other. the paper presents a novel methodology, named cooperative weak-committed monitoring (cwcm), enabling agents to cooperate while monitoring their own actions. cooperation helps the agents to cope with very scarcely observable environments: what an agent cannot observe directly can be acquired from other agents. cwcm exploits nondeterministic action models to carry out two main tasks: detecting action failures and building trajectory-sets (i.e., structures representing the knowledge an agent has about the environment in the recent past). relying on trajectory-sets, each agent is able to explain its own action failures in terms of exogenous events that have occurred during the execution of the actions themselves. to cope with dependent failures, cwcm is coupled with a diagnostic engine that distinguishes between primary and secondary action failures. an experimental analysis demonstrates that the cwcm methodology, together with the proposed diagnostic inferences, are effective in identifying and explaining action failures even in scenarios where the system observability is significantly reduced.",2014
on the testability of bdi agent systems,"before deploying a software system we need to assure ourselves (and stakeholders) that the system will behave correctly. this assurance is usually done by testing the system. however, it is intuitively obvious that adaptive systems, including agent-based systems, can exhibit complex behaviour, and are thus harder to test. in this paper we examine this ""obvious intuition"" in the case of belief-desire-intention (bdi) agents. we analyse the size of the behaviour space of bdi agents and show that although the intuition is correct, the factors that influence the size are not what we expected them to be. specifically, we found that the introduction of failure handling had a much larger effect on the size of the behaviour space than we expected. we also discuss the implications of these findings on the testability of bdi agents.",2014
text rewriting improves semantic role labeling,"large-scale annotated corpora are a prerequisite to developing high-performance nlp systems. such corpora are expensive to produce, limited in size, often demanding linguistic expertise. in this paper we use text rewriting as a means of increasing the amount of labeled data available for model training. our method uses automatically extracted rewrite rules from comparable corpora and bitexts to generate multiple versions of sentences annotated with gold standard labels. we apply this idea to semantic role labeling and show that a model trained on rewritten data outperforms the state of the art on the conll-2009 benchmark dataset.",2014
simple regret optimization in online planning for markov decision processes,"we consider online planning in markov decision processes (mdps). in online planning, the agent focuses on its current state only, deliberates about the set of possible policies from that state onwards and, when interrupted, uses the outcome of that exploratory deliberation to choose what action to perform next. formally, the performance of algorithms for online planning is assessed in terms of simple regret, the agent's expected performance loss when the chosen action, rather than an optimal one, is followed. to date, state-of-the-art algorithms for online planning in general mdps are either best effort, or guarantee only polynomial-rate reduction of simple regret over time. here we introduce a new monte-carlo tree search algorithm, brue, that guarantees exponential-rate and smooth reduction of simple regret. at a high level, brue is based on a simple yet non-standard state-space sampling scheme, mcts2e, in which different parts of each sample are dedicated to different exploratory objectives. we further extend brue with a variant of ``learning by forgetting.'' the resulting parametrized algorithm, brue(alpha), exhibits even more attractive formal guarantees than brue. our empirical evaluation shows that both brue and its generalization, brue(alpha), are also very effective in practice and compare favorably to the state-of-the-art.",2014
sensitivity of diffusion dynamics to network uncertainty,"simple diffusion processes on networks have been used to model, analyze and predict diverse phenomena such as spread of diseases, information and memes. more often than not, the underlying network data is noisy and sampled. this prompts the following natural question: how sensitive are the diffusion dynamics and subsequent conclusions to uncertainty in the network structure? in this paper, we consider two popular diffusion models: independent cascade (ic) model and linear threshold (lt) model. we study how the expected number of vertices that are influenced/infected, for particular initial conditions, are affected by network perturbations. through rigorous analysis under the assumption of a reasonable perturbation model we establish the following main results. (1) for the ic model, we characterize the sensitivity to network perturbation in terms of the critical probability for phase transition of the network. we find that the expected number of infections is quite stable, unless the transmission probability is close to the critical probability. (2) we show that the standard lt model with uniform edge weights is relatively stable under network perturbations. (3) we study these sensitivity questions using extensive simulations on diverse real world networks and find that our theoretical predictions for both models match the observations quite closely. (4) experimentally, the transient behavior, i.e., the time series of the number of infections, in both models appears to be more sensitive to network perturbations.",2014
entrenchment-based horn contraction,"the agm framework is the benchmark approach in belief change. since the framework assumes an underlying logic containing classical propositional logic, it can not be applied to systems with a logic weaker than propositional logic. to remedy this limitation, several researchers have studied agm-style contraction and revision under the horn fragment of propositional logic (i.e., horn logic). in this paper, we contribute to this line of research by investigating the horn version of the agm entrenchment-based contraction. the study is challenging as the construction of entrenchment-based contraction refers to arbitrary disjunctions which are not expressible under horn logic. in order to adapt the construction to horn logic, we make use of a horn approximation technique called horn strengthening. we provide a representation theorem for the newly constructed contraction which we refer to as entrenchment-based horn contraction. ideally, contractions defined under horn logic (i.e., horn contractions) should be as rational as agm contraction. we propose the notion of horn equivalence which intuitively captures the equivalence between horn contraction and agm contraction. we show that, under this notion, entrenchment-based horn contraction is equivalent to a restricted form of entrenchment-based contraction.",2014
automaton plans,"macros have long been used in planning to represent subsequences of operators. macros can be used in place of individual operators during search, sometimes reducing the effort required to find a plan to the goal. another use of macros is to compactly represent long plans. in this paper we introduce a novel solution concept called automaton plans in which plans are represented using hierarchies of automata. automaton plans can be viewed as an extension of macros that enables parameterization and branching. we provide several examples that illustrate how automaton plans can be useful, both as a compact representation of exponentially long plans and as an alternative to sequential solutions in benchmark domains such as logistics and grid. we also compare automaton plans to other compact plan representations from the literature, and find that automaton plans are strictly more expressive than macros, but strictly less expressive than htns and certain representations allowing efficient sequential access to the operators of the plan.",2014
distributed heuristic forward search for multi-agent planning,"this paper deals with the problem of classical planning for multiple cooperative agents who have private information about their local state and capabilities they do not want to reveal. two main approaches have recently been proposed to solve this type of problem -- one is based on reduction to distributed constraint satisfaction, and the other on partial-order planning techniques. in classical single-agent planning, constraint-based and partial-order planning techniques are currently dominated by heuristic forward search. the question arises whether it is possible to formulate a distributed heuristic forward search algorithm for privacy-preserving classical multi-agent planning. our work provides a positive answer to this question in the form of a general approach to distributed state-space search in which each agent performs only the part of the state expansion relevant to it. the resulting algorithms are simple and efficient -- outperforming previous algorithms by orders of magnitude -- while offering similar flexibility to that of forward-search algorithms for single-agent planning. furthermore, one particular variant of our general approach yields a distributed version of the a* algorithm that is the first cost-optimal distributed algorithm for privacy-preserving planning.",2014
verification of agent-based artifact systems,"artifact systems are a novel paradigm for specifying and implementing business processes described in terms of interacting modules called artifacts. artifacts consist of data and lifecycles, accounting respectively for the relational structure of the artifacts states and their possible evolutions over time. in this paper we put forward artifact-centric multi-agent systems, a novel formalisation of artifact systems in the context of multi-agent systems operating on them. differently from the usual process-based models of services, we give a semantics that explicitly accounts for the data structures on which artifact systems are defined. we study the model checking problem for artifact-centric multi-agent systems against specifications expressed in a quantified version of temporal-epistemic logic expressing the knowledge of the agents in the exchange. we begin by noting that the problem is undecidable in general. we identify a noteworthy class of systems that admit bisimilar, finite abstractions. it follows that we can verify these systems by investigating their finite abstractions; we also show that the corresponding model checking problem is expspace-complete. we then introduce artifact-centric programs, compact and declarative representations of the programs governing both the artifact system and the agents. we show that, while these in principle generate infinite-state systems, under natural conditions their verification problem can be solved on finite abstractions that can be effectively computed from the programs. we exemplify the theoretical results here pursued through a mainstream procurement scenario from the artifact systems literature.",2014
a novel sat-based approach to model based diagnosis,"this paper introduces a novel encoding of model based diagnosis (mbd) to boolean satisfaction (sat) focusing on minimal cardinality diagnosis. the encoding is based on a combination of sophisticated mbd preprocessing algorithms and the application of a sat compiler which optimizes the encoding to provide more succinct cnf representations than obtained with previous works. experimental evidence indicates that our approach is superior to all published algorithms for minimal cardinality mbd. in particular, we can determine, for the first time, minimal cardinality diagnoses for the entire standard iscas-85 and 74xxx benchmarks. our results open the way to improve the state-of-the-art on a range of similar mbd problems.",2014
scoring functions based on second level score for k-sat with long clauses,"it is widely acknowledged that stochastic local search (sls) algorithms can efficiently find models for satisfiable instances of the satisfiability (sat) problem, especially for random k-sat instances. however, compared to random 3-sat instances where sls algorithms have shown great success, random k-sat instances with long clauses remain very difficult. recently, the notion of second level score, denoted as ""score_2"", was proposed for improving sls algorithms on long-clause sat instances, and was first used in the powerful ccasat solver as a tie breaker. in this paper, we propose three new scoring functions based on score_2. despite their simplicity, these functions are very effective for solving random k-sat with long clauses. the first function combines score and score_2, and the second one additionally integrates the diversification property ""age"". these two functions are used in developing a new sls algorithm called cscoresat. experimental results on large random 5-sat and 7-sat instances near phase transition show that cscoresat significantly outperforms previous sls solvers. however, cscoresat cannot rival its competitors on random k-sat instances at phase transition. we improve cscoresat for such instances by another scoring function which combines score_2 with age. the resulting algorithm hscoresat exhibits state-of-the-art performance on random k-sat (k>3) instances at phase transition. we also study the computation of score_2, including its implementation and computational complexity.",2014
push and rotate: a complete multi-agent pathfinding algorithm,"multi-agent pathfinding is a relevant problem in a wide range of domains, for example in robotics and video games research. formally, the problem considers a graph consisting of vertices and edges, and a set of agents occupying vertices. an agent can only move to an unoccupied, neighbouring vertex, and the problem of finding the minimal sequence of moves to transfer each agent from its start location to its destination is an np-hard problem. we present push and rotate, a new algorithm that is complete for multi-agent pathfinding problems in which there are at least two empty vertices. push and rotate first divides the graph into subgraphs within which it is possible for agents to reach any position of the subgraph, and then uses the simple push, swap, and rotate operations to find a solution; a post-processing algorithm is also presented that eliminates redundant moves. push and rotate can be seen as extending luna and bekris's push and swap algorithm, which we showed to be incomplete in a previous publication. in our experiments we compare our approach with the push and swap, mapp, and bibox algorithms. the latter algorithm is restricted to a smaller class of instances as it requires biconnected graphs, but can nevertheless be considered state of the art due to its strong performance. our experiments show that push and swap suffers from incompleteness, mapp is generally not competitive with push and rotate, and bibox is better than push and rotate on randomly generated biconnected instances, while push and rotate performs better on grids.",2014
reasoning about topological and cardinal direction relations between 2-dimensional spatial objects,"increasing the expressiveness of qualitative spatial calculi is an essential step towards meeting the requirements of applications. this can be achieved by combining existing calculi in a way that we can express spatial information using relations from multiple calculi. the great challenge is to develop reasoning algorithms that are correct and complete when reasoning over the combined information. previous work has mainly studied cases where the interaction between the combined calculi was small, or where one of the two calculi was very simple. in this paper we tackle the important combination of topological and directional information for extended spatial objects. we combine some of the best known calculi in qualitative spatial reasoning, the rcc8 algebra for representing topological information, and the rectangle algebra (ra) and the cardinal direction calculus (cdc) for directional information. we consider two different interpretations of the rcc8 algebra, one uses a weak connectedness relation, the other uses a strong connectedness relation. in both interpretations, we show that reasoning with topological and directional information is decidable and remains in np. our computational complexity results unveil the significant differences between ra and cdc, and that between weak and strong rcc8 models. take the combination of basic rcc8 and basic cdc constraints as an example: we show that the consistency problem is in p only when we use the strong rcc8 algebra and explicitly know the corresponding basic ra constraints.",2014
optimal scheduling of contract algorithms for anytime problem-solving,"a contract algorithm is an algorithm which is given, as part of the input, a specified amount of allowable computation time. the algorithm must then complete its execution within the allotted time. an interruptible algorithm, in contrast, can be interrupted at an arbitrary point in time, at which point it must report its currently best solution. it is known that contract algorithms can simulate interruptible algorithms using iterative deepening techniques. this simulation is done at a penalty in the performance of the solution, as measured by the so-called acceleration ratio. in this paper we give matching (i.e., optimal) upper and lower bounds for the acceleration ratio under such a simulation. we assume the most general setting in which n problem instances must be solved by means of scheduling executions of contract algorithms in $m$ identical parallel processors. this resolves an open conjecture of bernstein, filkenstein, and zilberstein who gave an optimal schedule under the restricted setting of round robin and length-increasing schedules, but whose optimality in the general unrestricted case remained open. lastly, we show how to evaluate the average acceleration ratio of the class of exponential strategies in the setting of n problem instances and m parallel processors. this is a broad class of schedules that tend to be either optimal or near-optimal, for several variants of the basic problem.",2014
iterative plan construction for the workflow satisfiability problem,"the workflow satisfiability problem (wsp) is a problem of practical interest that arises whenever tasks need to be performed by authorized users, subject to constraints defined by business rules. we are required to decide whether there exists a plan - an assignment of tasks to authorized users - such that all constraints are satisfied. it is natural to see the wsp as a subclass of the constraint satisfaction problem (csp) in which the variables are tasks and the domain is the set of users. what makes the wsp distinctive is that the number of tasks is usually very small compared to the number of users, so it is appropriate to ask for which constraint languages the wsp is fixed-parameter tractable (fpt), parameterized by the number of tasks. this novel approach to the wsp, using techniques from csp, has enabled us to design a generic algorithm which is fpt for several families of workflow constraints considered in the literature. furthermore, we prove that the union of fpt languages remains fpt if they satisfy a simple compatibility condition. lastly, we identify a new fpt constraint language, user-independent constraints, that includes many of the constraints of interest in business processing systems. we demonstrate that our generic algorithm has provably optimal running time o*(2^(klog k)), for this language, where k is the number of tasks.",2014
no agent left behind: dynamic fair division of multiple resources,"recently fair division theory has emerged as a promising approach for allocation of multiple computational resources among agents. while in reality agents are not all present in the system simultaneously, previous work has studied static settings where all relevant information is known upfront. our goal is to better understand the dynamic setting. on the conceptual level, we develop a dynamic model of fair division, and propose desirable axiomatic properties for dynamic resource allocation mechanisms. on the technical level, we construct two novel mechanisms that provably satisfy some of these properties, and analyze their performance using real data. we believe that our work informs the design of superior multiagent systems, and at the same time expands the scope of fair division theory by initiating the study of dynamic and fair resource allocation mechanisms.",2014
using meta-mining to support data mining workflow planning and optimization,"knowledge discovery in databases is a complex process that involves many different data processing and learning operators. today's knowledge discovery support systems can contain several hundred operators. a major challenge is to assist the user in designing workflows which are not only valid but also -- ideally -- optimize some performance measure associated with the user goal. in this paper we present such a system. the system relies on a meta-mining module which analyses past data mining experiments and extracts meta-mining models which associate dataset characteristics with workflow descriptors in view of workflow performance optimization. the meta-mining model is used within a data mining workflow planner, to guide the planner during the workflow planning. we learn the meta-mining models using a similarity learning approach, and extract the workflow descriptors by mining the workflows for generalized relational patterns accounting also for domain knowledge provided by a data mining ontology. we evaluate the quality of the data mining workflows that the system produces on a collection of real world datasets coming from biology and show that it produces workflows that are significantly better than alternative methods that can only do workflow selection and not planning.",2014
the complexity of answering conjunctive and navigational queries over owl 2 el knowledge bases,"owl 2 el is a popular ontology language that supports role inclusions---that is, axioms that capture compositional properties of roles. role inclusions closely correspond to context-free grammars, which was used to show that answering conjunctive queries (cqs) over owl 2 el knowledge bases with unrestricted role inclusions is undecidable. however, owl 2 el inherits from owl 2 dl the syntactic regularity restriction on role inclusions, which ensures that role chains implying a particular role can be described using a finite automaton (fa). this is sufficient to ensure decidability of cq answering; however, the fas can be worst-case exponential in size so the known approaches do not provide a tight upper complexity bound. in this paper, we solve this open problem and show that answering cqs over owl 2 el knowledge bases is pspace-complete in combined complexity (i.e., the complexity measured in the total size of the input). to this end, we use a novel encoding of regular role inclusions using bounded-stack pushdown automata---that is, fas extended with a stack of bounded size. apart from theoretical interest, our encoding can be used in practical tableau algorithms to avoid the exponential blowup due to role inclusions. in addition, we sharpen the lower complexity bound and show that the problem is pspace-hard even if we consider only role inclusions as part of the input (i.e., the query and all other parts of the knowledge base are fixed). finally, we turn our attention to navigational queries over owl 2 el knowledge bases, and we show that answering positive, converse-free conjunctive graph xpath queries is pspace-complete as well; this is interesting since allowing the converse operator in queries is known to make the problem exptime-hard. thus, in this paper we present several important contributions to the landscape of the complexity of answering expressive queries over description logic knowledge bases.",2014
on minimum representations of matched formulas,"a boolean formula in conjunctive normal form (cnf) is called matched if the system of sets of variables which appear in individual clauses has a system of distinct representatives. each matched cnf is trivially satisfiable (each clause can be satisfied by its representative variable). another property which is easy to see, is that the class of matched cnfs is not closed under partial assignment of truth values to variables. this latter property leads to a fact (proved here) that given two matched cnfs it is co-np complete to decide whether they are logically equivalent. the construction in this proof leads to another result: a much shorter and simpler proof of the fact that the boolean minimization problem for matched cnfs is a complete problem for the second level of the polynomial hierarchy. the main result of this paper deals with the structure of clause minimum cnfs. we prove here that if a boolean function f admits a representation by a matched cnf then every clause minimum cnf representation of f is matched.",2014
tutorial on structured continuous-time markov processes,"a continuous-time markov process (ctmp) is a collection of variables indexed by a continuous quantity, time. it obeys the markov property that the distribution over a future variable is independent of past variables given the state at the present time. we introduce continuous-time markov process representations and algorithms for filtering, smoothing, expected sufficient statistics calculations, and model estimation, assuming no prior knowledge of continuous-time processes but some basic knowledge of probability and statistics. we begin by describing ""flat"" or unstructured markov processes and then move to structured markov processes (those arising from state spaces consisting of assignments to variables) including kronecker, decision-diagram, and continuous-time bayesian network representations. we provide the first connection between decision-diagrams and continuous-time bayesian networks.",2014
bdd ordering heuristics for classical planning,"symbolic search using binary decision diagrams (bdds) can often save large amounts of memory due to its concise representation of state sets. a decisive factor for this method's success is the chosen variable ordering. generally speaking, it is plausible that dependent variables should be brought close together in order to reduce bdd sizes. in planning, variable dependencies are typically captured by means of causal graphs, and in preceding work these were taken as the basis for finding bdd variable orderings. starting from the observation that the two concepts of ""dependency"" are actually quite different, we introduce a framework for assessing the strength of variable ordering heuristics in sub-classes of planning. it turns out that, even for extremely simple planning tasks, causal graph based variable orders may be exponentially worse than optimal. experimental results on a wide range of variable ordering variants corroborate our theoretical findings. furthermore, we show that dynamic reordering is much more effective at reducing bdd size, but it is not cost-effective due to a prohibitive runtime overhead. we exhibit the potential of middle-ground techniques, running dynamic reordering until simple stopping criteria hold.",2014
a hidden markov model-based acoustic cicada detector for crowdsourced smartphone biodiversity monitoring,"in recent years, the field of computational sustainability has striven to apply artificial intelligence techniques to solve ecological and environmental problems. in ecology, a key issue for the safeguarding of our planet is the monitoring of biodiversity. automated acoustic recognition of species aims to provide a cost-effective method for biodiversity monitoring. this is particularly appealing for detecting endangered animals with a distinctive call, such as the new forest cicada. to this end, we pursue a crowdsourcing approach, whereby the millions of visitors to the new forest, where this insect was historically found, will help to monitor its presence by means of a smartphone app that can detect its mating call. existing research in the field of acoustic insect detection has typically focused upon the classification of recordings collected from fixed field microphones. such approaches segment a lengthy audio recording into individual segments of insect activity, which are independently classified using cepstral coefficients extracted from the recording as features. this paper reports on a contrasting approach, whereby we use crowdsourcing to collect recordings via a smartphone app, and present an immediate feedback to the users as to whether an insect has been found. our classification approach does not remove silent parts of the recording via segmentation, but instead uses the temporal patterns throughout each recording to classify the insects present. we show that our approach can successfully discriminate between the call of the new forest cicada and similar insects found in the new forest, and is robust to common types of environment noise. a large scale trial deployment of our smartphone app collected over 6000 reports of insect activity from over 1000 users. despite the cicada not having been rediscovered in the new forest, the effectiveness of this approach was confirmed for both the detection algorithm, which successfully identified the same cicada through the app in countries where the same species is still present, and of the crowdsourcing methodology, which collected a vast number of recordings and involved thousands of contributors.",2014
an exact double-oracle algorithm for zero-sum extensive-form games with imperfect information,"developing scalable solution algorithms is one of the central problems in computational game theory. we present an iterative algorithm for computing an exact nash equilibrium for two-player zero-sum extensive-form games with imperfect information. our approach combines two key elements: (1) the compact sequence-form representation of extensive-form games and (2) the algorithmic framework of double-oracle methods. the main idea of our algorithm is to restrict the game by allowing the players to play only selected sequences of available actions. after solving the restricted game, new sequences are added by finding best responses to the current solution using fast algorithms. we experimentally evaluate our algorithm on a set of games inspired by patrolling scenarios, board, and card games. the results show significant runtime improvements in games admitting an equilibrium with small support, and substantial improvement in memory use even on games with large support. the improvement in memory use is particularly important because it allows our algorithm to solve much larger game instances than existing linear programming methods. our main contributions include (1) a generic sequence-form double-oracle algorithm for solving zero-sum extensive-form games; (2) fast methods for maintaining a valid restricted game model when adding new sequences; (3) a search algorithm and pruning methods for computing best-response sequences; (4) theoretical guarantees about the convergence of the algorithm to a nash equilibrium; (5) experimental analysis of our algorithm on several games, including an approximate version of the algorithm.",2014
topic-based dissimilarity and sensitivity models for translation rule selection,"translation rule selection is a task of selecting appropriate translation rules for an ambiguous source-language segment. as translation ambiguities are pervasive in statistical machine translation, we introduce two topic-based models for translation rule selection which incorporates global topic information into translation disambiguation. we associate each synchronous translation rule with source- and target-side topic distributions.with these topic distributions, we propose a topic dissimilarity model to select desirable (less dissimilar) rules by imposing penalties for rules with a large value of dissimilarity of their topic distributions to those of given documents. in order to encourage the use of non-topic specific translation rules, we also present a topic sensitivity model to balance translation rule selection between generic rules and topic-specific rules. furthermore, we project target-side topic distributions onto the source-side topic model space so that we can benefit from topic information of both the source and target language. we integrate the proposed topic dissimilarity and sensitivity model into hierarchical phrase-based machine translation for synchronous translation rule selection. experiments show that our topic-based translation rule selection model can substantially improve translation quality.",2014
knowledge forgetting in answer set programming,"the ability of discarding or hiding irrelevant information has been recognized as an important feature for knowledge based systems, including answer set programming. the notion of strong equivalence in answer set programming plays an important role for different problems as it gives rise to a substitution principle and amounts to knowledge equivalence of logic programs. in this paper, we uniformly propose a semantic knowledge forgetting, called ht- and flp-forgetting, for logic programs under stable model and flp-stable model semantics, respectively. our proposed knowledge forgetting discards exactly the knowledge of a logic program which is relevant to forgotten variables. thus it preserves strong equivalence in the sense that strongly equivalent logic programs will remain strongly equivalent after forgetting the same variables. we show that this semantic forgetting result is always expressible; and we prove a representation theorem stating that the ht- and flp-forgetting can be precisely characterized by zhang-zhou's four forgetting postulates under the ht- and flp-model semantics, respectively. we also reveal underlying connections between the proposed forgetting and the forgetting of propositional logic, and provide complexity results for decision problems in relation to the forgetting. an application of the proposed forgetting is also considered in a conflict solving scenario.",2014
a decision-theoretic model of assistance,"there is a growing interest in intelligent assistants for a variety of applications from sorting email to helping people with disabilities to do their daily chores. in this paper, we formulate the problem of intelligent assistance in a decision-theoretic framework, and present both theoretical and empirical results. we first introduce a class of pomdps called hidden-goal mdps (hgmdps), which formalizes the problem of interactively assisting an agent whose goal is hidden and whose actions are observable. in spite of its restricted nature, we show that optimal action selection for hgmdps is pspace-complete even for deterministic dynamics. we then introduce a more restricted model called helper action mdps (hamdps), which are sufficient for modeling many real-world problems. we show classes of hamdps for which efficient algorithms are possible. more interestingly, for general hamdps we show that a simple myopic policy achieves a near optimal regret, compared to an oracle assistant that knows the agent's goal. we then introduce more sophisticated versions of this policy for the general case of hgmdps that we combine with a novel approach for quickly learning about the agent being assisted. we evaluate our approach in two game-like computer environments where human subjects perform tasks, and in a real-world domain of providing assistance during folder navigation in a computer desktop environment. the results show that in all three domains the framework results in an assistant that substantially reduces user effort with only modest computation.",2014
finding optimal solutions for voting game design problems,"in many circumstances where multiple agents need to make a joint decision, voting is used to aggregate the agents' preferences. each agent's vote carries a weight, and if the sum of the weights of the agents in favor of some outcome is larger than or equal to a given quota, then this outcome is decided upon. the distribution of weights leads to a certain distribution of power. several `power indices' have been proposed to measure such power. in the so-called inverse problem, we are given a target distribution of power, and are asked to come up with a game in the form of a quota, plus an assignment of weights to the players whose power distribution is as close as possible to the target distribution (according to some specied distance measure). here we study solution approaches for the larger class of voting game design (vgd) problems, one of which is the inverse problem. in the general vgd problem, the goal is to find a voting game (with a given number of players) that optimizes some function over these games. in the inverse problem, for example, we look for a weighted voting game that minimizes the distance between the distribution of power among the players and a given target distribution of power (according to a given distance measure). our goal is to find algorithms that solve voting game design problems exactly, and we approach this goal by enumerating all games in the class of games of interest. we first present a doubly exponential algorithm for enumerating the set of simple games. we then improve on this algorithm for the class of weighted voting games and obtain a quadratic exponential (i.e., 2^o(n^2)) algorithm for enumerating them. we show that this improved algorithm runs in output-polynomial time, making it the fastest possible enumeration algorithm up to a polynomial factor. finally, we propose an exact anytime-algorithm that runs in exponential time for the power index weighted voting game design problem (the `inverse problem'). we implement this algorithm to find a weighted voting game with a normalized banzhaf power distribution closest to a target power index, and perform experiments to obtain some insights about the set of weighted voting games. we remark that our algorithm is applicable to optimizing any exponential-time computable function, the distance of the normalized banzhaf index to a target power index is merely taken as an example.",2014
enhanced partial expansion a*,"when solving instances of problem domains that feature a large branching factor, a* may generate a large number of nodes whose cost is greater than the cost of the optimal solution. we designate such nodes as surplus. generating surplus nodes and adding them to the open list may dominate both time and memory of the search. a recently introduced variant of a* called partial expansion a* (pea*) deals with the memory aspect of this problem. when expanding a node n, pea* generates all of its children and puts into open only the children with f = f (n). n is re-inserted in the open list with the f -cost of the best discarded child. this guarantees that surplus nodes are not inserted into open. in this paper, we present a novel variant of a* called enhanced partial expansion a* (epea*) that advances the idea of pea* to address the time aspect. given a priori domain- and heuristic- specific knowledge, epea* generates only the nodes with f = f(n). although epea* is not always applicable or practical, we study several variants of epea*, which make it applicable to a large number of domains and heuristics. in particular, the ideas of epea* are applicable to ida* and to the domains where pattern databases are traditionally used. experimental studies show significant improvements in run-time and memory performance for several standard benchmark applications. we provide several theoretical studies to facilitate an understanding of the new algorithm.",2014
an efficient algorithm for estimating state sequences in imprecise hidden markov models,"we present an efficient exact algorithm for estimating state sequences from outputs or observations in imprecise hidden markov models (ihmms). the uncertainty linking one state to the next, and that linking a state to its output, is represented by a set of probability mass functions instead of a single such mass function. we consider as best estimates for state sequences the maximal sequences for the posterior joint state model conditioned on the observed output sequence, associated with a gain function that is the indicator of the state sequence. this corresponds to and generalises finding the state sequence with the highest posterior probability in (precise-probabilistic) hmms, thereby making our algorithm a generalisation of the one by viterbi. we argue that the computational complexity of our algorithm is at worst quadratic in the length of the ihmm, cubic in the number of states, and essentially linear in the number of maximal state sequences. an important feature of our imprecise approach is that there may be more than one maximal sequence, typically in those instances where its precise-probabilistic counterpart is sensitive to the choice of prior. for binary ihmms, we investigate experimentally how the number of maximal state sequences depends on the model parameters. we also present an application in optical character recognition, demonstrating that our algorithm can be usefully applied to robustify the inferences made by its precise-probabilistic counterpart.",2014
reconnection with the ideal tree: a new approach to real-time search,"many applications, ranging from video games to dynamic robotics, require solving single-agent, deterministic search problems in partially known environments under very tight time constraints. real-time heuristic search (rths) algorithms are specifically designed for those applications. as a subroutine, most of them invoke a standard, but bounded, search algorithm that searches for the goal. in this paper we present frit, a simple approach for single-agent deterministic search problems under tight constraints and partially known environments that unlike traditional rths does not search for the goal but rather searches for a path that connects the current state with a so-called ideal tree t . when the agent observes that an arc in the tree cannot be traversed in the actual environment, it removes such an arc from t and then carries out a reconnection search whose objective is to find a path between the current state and any node in t . the reconnection search is done using an algorithm that is passed as a parameter to frit. if such a parameter is an rths algorithm, then the resulting algorithm can be an rths algorithm. we show, in addition, that frit may be fed with a (bounded) complete blind-search algorithm. we evaluate our approach over grid pathfinding benchmarks including game maps and mazes. our results show that frit, used with rtaa*, a standard rths algorithm, outperforms rtaa* significantly; by one order of magnitude under tight time constraints. in addition, frit(dartaa*) substantially outperforms dartaa*, a state-of-the-art rths algorithm, usually obtaining solutions 50% cheaper on average when performing the same search effort. finally, frit(bfs), i.e., frit using breadth-first-search, obtains best-quality solutions when time is limited compared to adaptive a* and repeated a*. finally we show that bug2, a pathfinding-specific navigation algorithm, outperforms frit(bfs) when planning time is extremely limited, but when given more time, the situation reverses.",2014
property directed reachability for automated planning,"property directed reachability (pdr) is a very promising recent method for deciding reachability in symbolically represented transition systems. while originally conceived as a model checking algorithm for hardware circuits, it has already been successfully applied in several other areas. this paper is the first investigation of pdr from the perspective of automated planning. similarly to the planning as satisfiability paradigm, pdr draws its strength from internally employing an efficient sat-solver. we show that most standard encoding schemes of planning into sat can be directly used to turn pdr into a planning algorithm. as a non-obvious alternative, we propose to replace the sat-solver inside pdr by a planning-specific procedure implementing the same interface. this sat-solver free variant is not only more efficient, but offers additional insights and opportunities for further improvements. an experimental comparison to the state of the art planners finds it highly competitive, solving most problems on several domains.",2014
game-theoretic patrolling with dynamic execution uncertainty and a case study on a real transit system,"attacker-defender stackelberg security games (ssgs) have emerged as an important research area in multi-agent systems. however, existing ssgs models yield fixed, static, schedules which fail in dynamic domains where defenders face execution uncertainty, i.e., in domains where defenders may face unanticipated disruptions of their schedules. a concrete example is an application involving checking fares on trains, where a defender's schedule is frequently interrupted by fare evaders, making static schedules useless. to address this shortcoming, this paper provides four main contributions. first, we present a novel general bayesian stackelberg game model for security resource allocation in dynamic uncertain domains. in this new model, execution uncertainty is handled by using a markov decision process (mdp) for generating defender policies. second, we study the problem of computing a stackelberg equilibrium for this game and exploit problem structure to reduce it to a polynomial-sized optimization problem. shifting to evaluation, our third contribution shows, in simulation, that our mdp-based policies overcome the failures of previous ssg algorithms. in so doing, we can now build a complete system, that enables handling of schedule interruptions and, consequently, to conduct some of the first controlled experiments on ssgs in the field. hence, as our final contribution, we present results from a real-world experiment on metro trains in los angeles validating our mdp-based model, and most importantly, concretely measuring the benefits of ssgs for security resource allocation.",2014
hc-search: a learning framework for search-based structured prediction,"structured prediction is the problem of learning a function that maps structured inputs to structured outputs. prototypical examples of structured prediction include part-of-speech tagging and semantic segmentation of images. inspired by the recent successes of search-based structured prediction, we introduce a new framework for structured prediction called hc-search. given a structured input, the framework uses a search procedure guided by a learned heuristic h to uncover high quality candidate outputs and then employs a separate learned cost function c to select a final prediction among those outputs. the overall loss of this prediction architecture decomposes into the loss due to h not leading to high quality outputs, and the loss due to c not selecting the best among the generated outputs. guided by this decomposition, we minimize the overall loss in a greedy stage-wise manner by first training h to quickly uncover high quality outputs via imitation learning, and then training c to correctly rank the outputs generated via h according to their true losses. importantly, this training procedure is sensitive to the particular loss function of interest and the time-bound allowed for predictions. experiments on several benchmark domains show that our approach significantly outperforms several state-of-the-art methods.",2014
a multivariate complexity analysis of lobbying in multiple referenda,"assume that each of n voters may or may not approve each of m issues. if an agent (the lobby) may influence up to k voters, then the central question of the np-hard lobbying problem is whether the lobby can choose the voters to be influenced so that as a result each issue gets a majority of approvals. this problem can be modeled as a simple matrix modification problem: can one replace k rows of a binary n x m-matrix by k all-1 rows such that each column in the resulting matrix has a majority of 1s? significantly extending on previous work that showed parameterized intractability (w[2]-completeness) with respect to the number k of modified rows, we study how natural parameters such as n, m, k, or the ""maximum number of 1s missing for any column to have a majority of 1s"" (referred to as ""gap value g"") govern the computational complexity of lobbying. among other results, we prove that lobbying is fixed-parameter tractable for parameter m and provide a greedy logarithmic-factor approximation algorithm which solves lobbying even optimally if m < 5. we also show empirically that this greedy algorithm performs well on general instances. as a further key result, we prove that lobbying is logsnp-complete for constant values g>0, thus providing a first natural complete problem from voting for this complexity class of limited nondeterminism.",2014
"monotone temporal planning: tractability, extensions and applications","this paper describes a polynomially-solvable class of temporal planning problems. polynomiality follows from two assumptions. firstly, by supposing that each sub-goal fluent can be established by at most one action, we can quickly determine which actions are necessary in any plan. secondly, the monotonicity of sub-goal fluents allows us to express planning as an instance of stp&#8800; (simple temporal problem with difference constraints). this class includes temporally-expressive problems requiring the concurrent execution of actions, with potential applications in the chemical, pharmaceutical and construction industries. we also show that any (temporal) planning problem has a monotone relaxation which can lead to the polynomial-time detection of its unsolvability in certain cases. indeed we show that our relaxation is orthogonal to relaxations based on the ignore-deletes approach used in classical planning since it preserves deletes and can also exploit temporal information.",2014
improving delete relaxation heuristics through explicitly represented conjunctions,"heuristic functions based on the delete relaxation compute upper and lower bounds on the optimal delete-relaxation heuristic h+, and are of paramount importance in both optimal and satisficing planning. here we introduce a principled and flexible technique for improving h+, by augmenting delete-relaxed planning tasks with a limited amount of delete information. this is done by introducing special fluents that explicitly represent conjunctions of fluents in the original planning task, rendering h+ the perfect heuristic h* in the limit. previous work has introduced a method in which the growth of the task is potentially exponential in the number of conjunctions introduced. we formulate an alternative technique relying on conditional effects, limiting the growth of the task to be linear in this number. we show that this method still renders h+ the perfect heuristic h* in the limit. we propose techniques to find an informative set of conjunctions to be introduced in different settings, and analyze and extend existing methods for lower-bounding and upper-bounding h+ in the presence of conditional effects. we evaluate the resulting heuristic functions empirically on a set of ipc benchmarks, and show that they are sometimes much more informative than standard delete-relaxation heuristics.",2014
integrating queueing theory and scheduling for dynamic scheduling problems,"dynamic scheduling problems consist of both challenging combinatorics, as found in classical scheduling problems, and stochastics due to uncertainty about the arrival times, resource requirements, and processing times of jobs. to address these two challenges, we investigate the integration of queueing theory and scheduling. the former reasons about long-run stochastic system characteristics, whereas the latter typically deals with short-term combinatorics. we investigate two simple problems to isolate the core differences and potential synergies between the two approaches: a two-machine dynamic flowshop and a flexible queueing network. we show for the first time that stability, a fundamental characteristic in queueing theory, can be applied to approaches that periodically solve combinatorial scheduling problems. we empirically demonstrate that for a dynamic flowshop, the use of combinatorial reasoning has little impact on schedule quality beyond queueing approaches. in contrast, for the more complicated flexible queueing network, a novel algorithm that combines long-term guidance from queueing theory with short-term combinatorial decision making outperforms all other tested approaches. to our knowledge, this is the first time that such a hybrid of queueing theory and scheduling techniques has been proposed and evaluated.",2014
false-name manipulation in weighted voting games is hard for probabilistic polynomial time,"false-name manipulation refers to the question of whether a player in a weighted voting game can increase her power by splitting into several players and distributing her weight among these false identities. relatedly, the beneficial merging problem asks whether a coalition of players can increase their power in a weighted voting game by merging their weights. for the problems of whether merging or splitting players in weighted voting games is beneficial in terms of the shapley--shubik and the normalized banzhaf index, merely np-hardness lower bounds are known, leaving the question about their exact complexity open. for the shapley--shubik and the probabilistic banzhaf index, we raise these lower bounds to hardness for pp, ""probabilistic polynomial time,"" a class considered to be by far a larger class than np. for both power indices, we provide matching upper bounds for beneficial merging and, whenever the new players' weights are given, also for beneficial splitting, thus resolving previous conjectures in the affirmative. relatedly, we consider the beneficial annexation problem, asking whether a single player can increase her power by taking over other players' weights. it is known that annexation is never disadvantageous for the shapley--shubik index, and that beneficial annexation is np-hard for the normalized banzhaf index. we show that annexation is never disadvantageous for the probabilistic banzhaf index either, and for both the shapley--shubik index and the probabilistic banzhaf index we show that it is np-complete to decide whether annexing another player is advantageous. moreover, we propose a general framework for merging and splitting that can be applied to different classes and representations of games.",2014
probabilistic inference in credal networks: new complexity results,"credal networks are graph-based statistical models whose parameters take values in a set, instead of being sharply specified as in traditional statistical models (e.g., bayesian networks). the computational complexity of inferences on such models depends on the irrelevance/independence concept adopted. in this paper, we study inferential complexity under the concepts of epistemic irrelevance and strong independence. we show that inferences under strong independence are np-hard even in trees with binary variables except for a single ternary one. we prove that under epistemic irrelevance the polynomial-time complexity of inferences in credal trees is not likely to extend to more general models (e.g., singly connected topologies). these results clearly distinguish networks that admit efficient inferences and those where inferences are most likely hard, and settle several open questions regarding their computational complexity. we show that these results remain valid even if we disallow the use of zero probabilities. we also show that the computation of bounds on the probability of the future state in a hidden markov model is the same whether we assume epistemic irrelevance or strong independence, and we prove a similar result for inference in naive bayes structures. these inferential equivalences are important for practitioners, as hidden markov models and naive bayes structures are used in real applications of imprecise probability.",2014
planning through automatic portfolio configuration: the pbp approach,"in the field of domain-independent planning, several powerful planners implementing different techniques have been developed. however, no one of these systems outperforms all others in every known benchmark domain. in this work, we propose a multi-planner approach that automatically configures a portfolio of planning techniques for each given domain. the configuration process for a given domain uses a set of training instances to: (i) compute and analyze some alternative sets of macro-actions for each planner in the portfolio identifying a (possibly empty) useful set, (ii) select a cluster of planners, each one with the identified useful set of macro-actions, that is expected to perform best, and (iii) derive some additional information for configuring the execution scheduling of the selected planners at planning time. the resulting planning system, called pbp (portfolio- based planner), has two variants focusing on speed and plan quality. different versions of pbp entered and won the learning track of the sixth and seventh international planning competitions. in this paper, we experimentally analyze pbp considering planning speed and plan quality in depth. we provide a collection of results that help to understand pbps behavior, and demonstrate the effectiveness of our approach to configuring a portfolio of planners with macro-actions.",2014
mdd propagation for sequence constraints,"we study propagation for the sequence constraint in the context of constraint programming based on limited-width mdds. our first contribution is proving that establishing mdd-consistency for sequence is np-hard. yet, we also show that this task is fixed parameter tractable with respect to the length of the sub-sequences. in addition, we propose a partial filtering algorithm that relies on a specific decomposition of the constraint and a novel extension of mdd filtering to node domains. we experimentally evaluate the performance of our proposed filtering algorithm, and demonstrate that the strength of the mdd propagation increases as the maximum width is increased. in particular, mdd propagation can outperform conventional domain propagation for sequence by reducing the search tree size and solving time by several orders of magnitude. similar improvements are observed with respect to the current best mdd approach that applies the decomposition of sequence into among constraints.",2014
sentiment analysis of short informal texts,"we describe a state-of-the-art sentiment analysis system that detects (a) the sentiment of short informal textual messages such as tweets and sms (message-level task) and (b) the sentiment of a word or a phrase within a message (term-level task). the system is based on a supervised statistical text classification approach leveraging a variety of surface-form, semantic, and sentiment features. the sentiment features are primarily derived from novel high-coverage tweet-specific sentiment lexicons. these lexicons are automatically generated from tweets with sentiment-word hashtags and from tweets with emoticons. to adequately capture the sentiment of words in negated contexts, a separate sentiment lexicon is generated for negated words. the system ranked first in the semeval-2013 shared task `sentiment analysis in twitter' (task 2), obtaining an f-score of 69.02 in the message-level task and 88.93 in the term-level task. post-competition improvements boost the performance to an f-score of 70.45 (message-level task) and 89.50 (term-level task). the system also obtains state-of-the-art performance on two additional datasets: the semeval-2013 sms test set and a corpus of movie review excerpts. the ablation experiments demonstrate that the use of the automatically generated lexicons results in performance gains of up to 6.5 absolute percentage points.",2014
policy iteration based on stochastic factorization,"when a transition probability matrix is represented as the product of two stochastic matrices, one can swap the factors of the multiplication to obtain another transition matrix that retains some fundamental characteristics of the original. since the derived matrix can be much smaller than its precursor, this property can be exploited to create a compact version of a markov decision process (mdp), and hence to reduce the computational cost of dynamic programming. building on this idea, this paper presents an approximate policy iteration algorithm called policy iteration based on stochastic factorization, or pisf for short. in terms of computational complexity, pisf replaces standard policy iteration's cubic dependence on the size of the mdp with a function that grows only linearly with the number of states in the model. the proposed algorithm also enjoys nice theoretical properties: it always terminates after a finite number of iterations and returns a decision policy whose performance only depends on the quality of the stochastic factorization. in particular, if the approximation error in the factorization is sufficiently small, pisf computes the optimal value function of the mdp. the paper also discusses practical ways of factoring an mdp and illustrates the usefulness of the proposed algorithm with an application involving a large-scale decision problem of real economical interest.",2014
speeding up iterative ontology alignment using block-coordinate descent,"in domains such as biomedicine, ontologies are prominently utilized for annotating data. consequently, aligning ontologies facilitates integrating data. several algorithms exist for automatically aligning ontologies with diverse levels of performance. as alignment applications evolve and exhibit online run time constraints, performing the alignment in a reasonable amount of time without compromising the quality of the alignment is a crucial challenge. a large class of alignment algorithms is iterative and often consumes more time than others in delivering solutions of high quality. we present a novel and general approach for speeding up the multivariable optimization process utilized by these algorithms. specifically, we use the technique of block-coordinate descent (bcd), which exploits the subdimensions of the alignment problem identified using a partitioning scheme. we integrate this approach into multiple well-known alignment algorithms and show that the enhanced algorithms generate similar or improved alignments in significantly less time on a comprehensive testbed of ontology pairs. because bcd does not overly constrain how we partition or order the parts, we vary the partitioning and ordering schemes in order to empirically determine the best schemes for each of the selected algorithms. as biomedicine represents a key application domain for ontologies, we introduce a comprehensive biomedical ontology testbed for the community in order to evaluate alignment algorithms. because biomedical ontologies tend to be large, default iterative techniques find it difficult to produce a good quality alignment within a reasonable amount of time. we align a significant number of ontology pairs from this testbed using bcd-enhanced algorithms. our contributions represent an important step toward making a significant class of alignment techniques computationally feasible.",2014
arbitration and stability in cooperative games with overlapping coalitions,"overlapping coalition formation (ocf) games, introduced by chalkiadakis, elkind, markakis, polukarov and jennings in 2010, are cooperative games where players can simultaneously participate in several coalitions. capturing the notion of stability in ocf games is a difficult task:deviating players may continue to contribute resources to joint projects with non-deviators, and the crucial question is what payoffs the deviators expect to receive from such projects. chalkiadakis et al. introduce three stability concepts for ocf games---the conservative core, the refined core, and the optimistic core---that are based on different answers to this question. in this paper, we propose a unified framework for the study of stability in the ocf setting, which encompasses the stability concepts considered by chalkiadakis et al. as well as a wide variety of alternative stability concepts. our approach is based on the notion of arbitration functions, which determine the payoff obtained by the deviators, given their deviation and the current allocation of resources. we provide a characterization of stable outcomes under arbitration. we then conduct an in-depth study of four types of arbitration functions, which correspond to four notions of the core; these include the three notions of the core considered by chalkiadakis et al. our results complement those of chalkiadakis et al. and answer questions left open by their work. in particular, we show that ocf games with the conservative arbitration function are essentially equivalent to non-ocf games, by relating the conservative core of an ocf game to the core of a non-overlapping cooperative game, and use this result to obtain a strictly weaker sufficient condition for conservative core non-emptiness than the one given by chalkiadakis et al.",2014
demand side energy management via multiagent coordination in consumer cooperatives,"a key challenge in creating a sustainable and energy-efficient society is to make consumer demand adaptive to the supply of energy, especially to the renewable supply. in this article, we propose a partially-centralized organization of consumers (or agents), namely, a consumer cooperative that purchases electricity from the market. in the cooperative, a central coordinator buys the electricity for the whole group. the technical challenge is that consumers make their own demand decisions, based on their private demand constraints and preferences, which they do not share with the coordinator or other agents. we propose a novel multiagent coordination algorithm, to shape the energy demand of the cooperative. to coordinate individual consumers under incomplete information, the coordinator determines virtual price signals that it sends to the consumers to induce them to shift their demands when required. we prove that this algorithm converges to the central optimal solution and minimizes the electric energy cost of the cooperative. additionally, we present results on the time complexity of the iterative algorithm and its implications for agents' incentive compatibility. furthermore, we perform simulations based on real world consumption data to (a) characterize the convergence properties of our algorithm and (b) understand the effect of differing demand characteristics of participants as well as of different price functions on the cost reduction. the results show that the convergence time scales linearly with the agent population size and length of the optimization horizon. finally, we observe that as participants' flexibility of shifting their demands increases, cost reduction increases and that the cost reduction is not sensitive to variation in consumption patterns of the consumers.",2014
"belief tracking for planning with sensing: width, complexity and approximations","we consider the problem of belief tracking in a planning setting where states are valuations over a set of variables that are partially observable, and beliefs stand for the sets of states that are possible. while the problem is intractable in the worst case, it has been recently shown that in deterministic conformant and contingent problems, belief tracking is exponential in a width parameter that is often bounded and small. in this work, we extend these results in two ways. first, we introduce a width notion that applies to non-deterministic problems as well, develop a factored belief tracking algorithm that is exponential in the problem width, and show how it applies to existing benchmarks. second, we introduce a meaningful, powerful, and sound approximation scheme, beam tracking, that is exponential in a smaller parameter, the problem causal width, and has much broader applicability. we illustrate the value of this algorithm over large instances of problems such as battleship, minesweeper, and wumpus, where it yields state-of-the-art performance in real-time.",2014
multimodal distributional semantics,"distributional semantic models derive computational representations of word meaning from the patterns of co-occurrence of words in text. such models have been a success story of computational linguistics, being able to provide reliable estimates of semantic relatedness for the many semantic tasks requiring them. however, distributional models extract meaning information exclusively from text, which is an extremely impoverished basis compared to the rich perceptual sources that ground human semantic knowledge. we address the lack of perceptual grounding of distributional models by exploiting computer vision techniques that automatically identify discrete visual words in images, so that the distributional representation of a word can be extended to also encompass its co-occurrence with the visual words of images it is associated with. we propose a flexible architecture to integrate text- and image-based distributional information, and we show in a set of empirical tests that our integrated model is superior to the purely text-based approach, and it provides somewhat complementary semantic information with respect to the latter.",2014
robustness and stability in constraint programming under dynamism and uncertainty,"many real life problems that can be solved by constraint programming, come from uncertain and dynamic environments. because of the dynamism, the original problem may change over time, and thus the solution found for the original problem may become invalid. for this reason, dealing with such problems has become an important issue in the fields of constraint programming. in some cases, there exist extant knowledge about the uncertain and dynamic environment. in other cases, this information is fragmentary or unknown. in this paper, we extend the concept of robustness and stability for constraint satisfaction problems (csps) with ordered domains, where only limited assumptions need to be made as to possible changes. we present a search algorithm that searches for both robust and stable solutions for csps of this nature. it is well-known that meeting both criteria simultaneously is a desirable objective for constraint solving in uncertain and dynamic environments. we also present compelling evidence that our search algorithm outperforms other general-purpose algorithms for dynamic csps using random instances and benchmarks derived from real life problems.",2014
closure and consistency in logic-associated argumentation,"properties like logical closure and consistency are important properties in any logical reasoning system. caminada and amgoud showed that not every logic-based argument system satisfies these relevant properties. but under conditions like closure under contraposition or transposition of the monotonic part of the underlying logic, aspic-like systems satisfy these properties. in contrast, the logical closure and consistency properties are not well-understood for other well-known and widely applied systems like logic programming or assumption based argumentation. though conditions like closure under contraposition or transposition seem intuitive in aspic-like systems, they rule out many sensible aspic-like systems that satisfy both properties of closure and consistency. we present a new condition referred to as the self-contradiction axiom that guarantees the consistency property in both aspic-like and assumption-based systems and is implied by both properties of closure under contraposition or transposition. we develop a logic-associated abstract argumentation framework, by associating abstract argumentation with abstract logics to represent the conclusions of arguments. we show that logic-associated abstract argumentation frameworks capture aspic-like systems (without preferences) and assumption-based argumentation. we present two simple and natural properties of compactness and cohesion in logic-associated abstract argumentation frameworks and show that they capture the logical closure and consistency properties. we demonstrate that in both assumption-based argumentation and aspic-like systems, cohesion follows naturally from the self-contradiction axiom. we further give a translation from aspic-like systems (without preferences) into equivalent assumption-based systems that keeps the self-contradiction axiom invariant.",2014
towards minimizing disappointment in repeated games,"we consider the problem of learning in repeated games against arbitrary associates. specifically, we study the ability of expert algorithms to quickly learn effective strategies in repeated games, towards the ultimate goal of learning near-optimal behavior against any arbitrary associate within only a handful of interactions. our contribution is three-fold. first, we advocate a new metric, called disappointment, for evaluating expert algorithms in repeated games. unlike minimizing traditional notions of regret, minimizing disappointment in repeated games is equivalent to maximizing payoffs. unfortunately, eliminating disappointment is impossible to guarantee in general. however, it is possible for an expert algorithm to quickly achieve low disappointment against many known classes of algorithms in many games. second, we show that popular existing expert algorithms often fail to achieve low disappointment against a variety of associates, particularly in early rounds of the game. finally, we describe a new meta-algorithm that can be applied to existing expert algorithms to substantially reduce disappointment in many two-player repeated games when associates follow various static, reinforcement learning, and expert algorithms.",2014
a procedural characterization of solution concepts in games,"we show how game-theoretic solution concepts such as nash equilibrium, correlated equilibrium, rationalizability, and sequential equilibrium can be given a uniform definition in terms of a knowledge-based program with counterfactual semantics. in a precise sense, this program can be viewed as providing a procedural characterization of rationality.",2014
representing and reasoning about the rules of general games with imperfect information,"a general game player is a system that can play previously unknown games just by being given their rules. for this purpose, the game description language (gdl) has been developed as a high-level knowledge representation formalism to communicate game rules to players. in this paper, we address a fundamental limitation of state-of-the-art methods and systems for general game playing, namely, their being confined to deterministic games with complete information about the game state. we develop a simple yet expressive extension of standard gdl that allows for formalising the rules of arbitrary finite, n-player games with randomness and incomplete state knowledge. in the second part of the paper, we address the intricate reasoning challenge for general game-playing systems that comes with the new description language. we develop a full embedding of extended gdl into the situation calculus augmented by scherl and levesque's knowledge fluent. we formally prove that this provides a sound and complete reasoning method for players' knowledge about game states as well as about the knowledge of the other players.",2014
selfishness level of strategic games,"we introduce a new measure of the discrepancy in strategic games between the social welfare in a nash equilibrium and in a social optimum, that we call selfishness level. it is the smallest fraction of the social welfare that needs to be offered to each player to achieve that a social optimum is realized in a pure nash equilibrium. the selfishness level is unrelated to the price of stability and the price of anarchy and is invariant under positive linear transformations of the payoff functions. also, it naturally applies to other solution concepts and other forms of games. we study the selfishness level of several well-known strategic games. this allows us to quantify the implicit tension within a game between players' individual interests and the impact of their decisions on the society as a whole. our analyses reveal that the selfishness level often provides a deeper understanding of the characteristics of the underlying game that influence the players' willingness to cooperate. in particular, the selfishness level of finite ordinal potential games is finite, while that of weakly acyclic games can be infinite. we derive explicit bounds on the selfishness level of fair cost sharing games and linear congestion games, which depend on specific parameters of the underlying game but are independent of the number of players. further, we show that the selfishness level of the $n$-players prisoner's dilemma is c/(b(n-1)-c), where b and c are the benefit and cost for cooperation, respectively, that of the n-players public goods game is (1-c/n)/(c-1), where c is the public good multiplier, and that of the traveler's dilemma game is (b-1)/2, where b is the bonus. finally, the selfishness level of cournot competition (an example of an infinite ordinal potential game), tragedy of the commons, and bertrand competition is infinite.",2014
an empirical evaluation of ranking measures with respect to robustness to noise,"ranking measures play an important role in model evaluation and selection. using both synthetic and real-world data sets, we investigate how different types and levels of noise affect the area under the roc curve (auc), the area under the roc convex hull, the scored auc, the kolmogorov-smirnov statistic, and the h-measure. in our experiments, the auc was, overall, the most robust among these measures, thereby reinvigorating it as a reliable metric despite its well-known deficiencies. this paper also introduces a novel ranking measure, which is remarkably robust to noise yet conceptually simple.",2014
efficient hex-program evaluation based on unfounded sets,"hex-programs extend logic programs under the answer set semantics with external computations through external atoms. as reasoning from ground horn programs with nonmonotonic external atoms of polynomial complexity is already on the second level of the polynomial hierarchy, minimality checking of answer set candidates needs special attention. to this end, we present an approach based on unfounded sets as a generalization of related techniques for asp programs. the unfounded set detection is expressed as a propositional sat problem, for which we provide two different encodings and optimizations to them. we then integrate our approach into a previously developed evaluation framework for hex-programs, which is enriched by additional learning techniques that aim at avoiding the reconstruction of the same or related unfounded sets. furthermore, we provide a syntactic criterion that allows one to skip the minimality check in many cases. an experimental evaluation shows that the new approach significantly decreases runtime.",2014
symmetric subgame-perfect equilibria in resource allocation,"we analyze symmetric protocols to rationally coordinate on an asymmetric, efficient allocation in an infinitely repeated n-agent, c-resource allocation problems, where the resources are all homogeneous. bhaskar proposed one way to achieve this in 2-agent, 1-resource games: agents start by symmetrically randomizing their actions, and as soon as they each choose different actions, they start to follow a potentially asymmetric ""convention"" that prescribes their actions from then on. we extend the concept of convention to the general case of infinitely repeated resource allocation games with n agents and c resources. we show that for any convention, there exists a symmetric subgame-perfect equilibrium which implements it. we present two conventions: bourgeois, where agents stick to the first allocation; and market, where agents pay for the use of resources, and observe a global coordination signal which allows them to alternate between different allocations. we define price of anonymity of a convention as a ratio between the maximum social payoff of any (asymmetric) strategy profile and the expected social payoff of the subgame-perfect equilibrium which implements the convention. we show that while the price of anonymity of the bourgeois convention is infinite, the market convention decreases this price by reducing the conflict between the agents.",2014
multiagent only knowing in dynamic systems,"the idea of ""only knowing"" a collection of sentences, as proposed by levesque, has been previously shown to be very useful in characterizing knowledge-based agents: in terms of a specification, a precise and perspicuous account of the beliefs and non-beliefs is obtained in a monotonic setting. levesque's logic is based on a first-order modal language with quantifying-in, thus allowing for de re versus de dicto distinctions, among other things. however, the logic and its recent dynamic extension only deal with the case of a single agent. in this work, we propose a first-order multiagent framework with knowledge, actions, sensing and only knowing, that is shown to inherit all the features of the single agent version. most significantly, we prove reduction theorems by means of which reasoning about knowledge and actions in the framework simplifies to non-epistemic, non-dynamic reasoning about the initial situation.",2014
mechanisms for fair allocation problems: no-punishment payment rules in verifiable settings,"mechanism design is considered in the context of fair allocations of indivisible goods with monetary compensation, by focusing on problems where agents' declarations on allocated goods can be verified before payments are performed. a setting is considered where verification might be subject to errors, so that payments have to be awarded under the presumption of innocence, as incorrect declared values do not necessarily mean manipulation attempts by the agents. within this setting, a mechanism is designed that is shown to be truthful, efficient, and budget-balanced. moreover, agents' utilities are fairly determined by the shapley value of suitable coalitional games, and enjoy highly desirable properties such as equal treatment of equals, envy-freeness, and a stronger one called individual-optimality. in particular, the latter property guarantees that, for every agent, her/his utility is the maximum possible one over any alternative optimal allocation. the computational complexity of the proposed mechanism is also studied. it turns out that it is #p-complete so that, to deal with applications with many agents involved, two polynomial-time randomized variants are also proposed: one that is still truthful and efficient, and which is approximately budget-balanced with high probability, and another one that is truthful in expectation, while still budget-balanced and efficient.",2014
text-based twitter user geolocation prediction,"geographical location is vital to geospatial applications like local search and event detection. in this paper, we investigate and improve on the task of text-based geolocation prediction of twitter users. previous studies on this topic have typically assumed that geographical references (e.g., gazetteer terms, dialectal words) in a text are indicative of its authors location. however, these references are often buried in informal, ungrammatical, and multilingual data, and are therefore non-trivial to identify and exploit. we present an integrated geolocation prediction framework and investigate what factors impact on prediction accuracy. first, we evaluate a range of feature selection methods to obtain location indicative words. we then evaluate the impact of non-geotagged tweets, language, and user-declared metadata on geolocation prediction. in addition, we evaluate the impact of temporal variance on model generalisation, and discuss how users differ in terms of their geolocatability. we achieve state-of-the-art results for the text-based twitter user geolocation task, and also provide the most extensive exploration of the task to date. our findings provide valuable insights into the design of robust, practical text-based geolocation prediction systems.",2014
information-theoretic multi-view domain adaptation: a theoretical and empirical study,"multi-view learning aims to improve classification performance by leveraging the consistency among different views of data. the incorporation of multiple views was paid little attention in the studies of domain adaptation, where the view consistency based on source data is largely violated in the target domain due to the distribution gap between different domain data. in this paper, we leverage multiple views for cross-domain document classification. the central idea is to strengthen the views' consistency on target data by identifying the associations of domain-specific features from different domains. we present an information-theoretic multi-view adaptation model (imam) using a multi-way clustering scheme, where word and link clusters can draw together seemingly unrelated features across domains, which boosts the consistency between document clusterings that are based on the respective word and link views. moreover, we demonstrate that imam can always find the document clustering with the minimal disagreement rate to the overlap of view-based clusterings. we provide both theoretical and empirical justifications of the proposed method. our experiments show that imam significantly outperforms traditional multi-view algorithm co-training, the co-training-based adaptation algorithm coda, the single-view transfer model cocc and the large-margin-based multi-view transfer model mvtl-lm.",2014
large-scale optimization for evaluation functions with minimax search,"this paper presents a new method, minimax tree optimization (mmto), to learn a heuristic evaluation function of a practical alpha-beta search program. the evaluation function may be a linear or non-linear combination of weighted features, and the weights are the parameters to be optimized. to control the search results so that the move decisions agree with the game records of human experts, a well-modeled objective function to be minimized is designed. moreover, a numerical iterative method is used to nd local minima of the objective function, and more than forty million parameters are adjusted by using a small number of hyper parameters. this method was applied to shogi, a major variant of chess in which the evaluation function must handle a larger state space than in chess. experimental results show that the large-scale optimization of the evaluation function improves the playing strength of shogi programs, and the new method performs signicantly better than other methods. implementation of the new method in our shogi program bonanza made substantial contributions to the program's rst-place nish in the 2013 world computer shogi championship. additionally, we present preliminary evidence of broader applicability of our method to other two-player games such as chess.",2014
inapproximability of treewidth and related problems,"graphical models, such as bayesian networks and markov networks play an important role in artificial intelligence and machine learning. inference is a central problem to be solved on these networks. this, and other problems on these graph models are often known to be hard to solve in general, but tractable on graphs with bounded treewidth. therefore, finding or approximating the treewidth of a graph is a fundamental problem related to inference in graphical models. in this paper, we study the approximability of a number of graph problems: treewidth and pathwidth of graphs, minimum fill-in, one-shot black (and black-white) pebbling costs of directed acyclic graphs, and a variety of different graph layout problems such as minimum cut linear arrangement and interval graph completion. we show that, assuming the recently introduced small set expansion conjecture, all of these problems are np-hard to approximate to within any constant factor in polynomial time.",2014
algorithms and applications for the same-decision probability,"when making decisions under uncertainty, the optimal choices are often difficult to discern, especially if not enough information has been gathered. two key questions in this regard relate to whether one should stop the information gathering process and commit to a decision (stopping criterion), and if not, what information to gather next (selection criterion). in this paper, we show that the recently introduced notion, same-decision probability (sdp), can be useful as both a stopping and a selection criterion, as it can provide additional insight and allow for robust decision making in a variety of scenarios. this query has been shown to be highly intractable, being pp^pp-complete, and is exemplary of a class of queries which correspond to the computation of certain expectations. we propose the first exact algorithm for computing the sdp, and demonstrate its effectiveness on several real and synthetic networks. finally, we present new complexity results, such as the complexity of computing the sdp on models with a naive bayes structure. additionally, we prove that computing the non-myopic value of information is complete for the same complexity class as computing the sdp.",2014
algorithms for argumentation semantics: labeling attacks as a generalization of labeling arguments,"a dung argumentation framework (af) is a pair (a,r): a is a set of abstract arguments and r &#8838; a&#215;a is a binary relation, so-called the attack relation, for capturing the conflicting arguments. labeling based algorithms for enumerating extensions (i.e. sets of acceptable arguments) have been set out such that arguments (i.e. elements of a) are the only subject for labeling. in this paper we present implemented algorithms for listing extensions by labeling attacks (i.e. elements of r) along with arguments. specifically, these algorithms are concerned with enumerating all extensions of an af under a number of argumentation semantics: preferred, stable, complete, semi stable, stage, ideal and grounded. our algorithms have impact, in particular, on enumerating extensions of af-extended models that allow attacks on attacks. to demonstrate this impact, we instantiate our algorithms for an example of such models: namely argumentation frameworks with recursive attacks (afra), thereby we end up with unified algorithms that enumerate extensions of any af/afra.",2014
improved separations of regular resolution from clause learning proof systems,"this paper studies the relationship between resolution and conflict driven clause learning (cdcl) without restarts, and refutes some conjectured possible separations. we prove that the guarded, xor-ified pebbling tautology clauses, which urquhart proved are hard for regular resolution, as well as the guarded graph tautology clauses of alekhnovich, johannsen, pitassi, and urquhart have polynomial size pool resolution refutations that use only input lemmas as learned clauses. for the latter set of clauses, we extend this to prove that a cdcl search without restarts can refute these clauses in polynomial time, provided it makes the right choices for decision literals and clause learning. this holds even if the cdcl search is required to greedily process conflicts arising from unit propagation. this refutes the conjecture that the guarded graph tautology clauses or the guarded xor-ified pebbling tautology clauses can be used to separate cdcl without restarts from general resolution. together with subsequent results by buss and kolodziejczyk, this means we lack any good conjectures about how to establish the exact logical strength of conflict-driven clause learning without restarts.",2014
convergence of a q-learning variant for continuous states and actions,"this paper presents a reinforcement learning algorithm for solving infinite horizon markov decision processes under the expected total discounted reward criterion when both the state and action spaces are continuous. this algorithm is based on watkins' q-learning, but uses nadaraya-watson kernel smoothing to generalize knowledge to unvisited states. as expected, continuity conditions must be imposed on the mean rewards and transition probabilities. using results from kernel regression theory, this algorithm is proven capable of producing a q-value function estimate that is uniformly within an arbitrary tolerance of the true q-value function with probability one. the algorithm is then applied to an example problem to empirically show convergence as well.",2014
comparative evaluation of link-based approaches for candidate ranking in link-to-wikipedia systems,"in recent years, the task of automatically linking pieces of text (anchors) mentioned in a document to wikipedia articles that represent the meaning of these anchors has received extensive research attention. typically, link-to-wikipedia systems try to find a set of wikipedia articles that are candidates to represent the meaning of the anchor and, later, rank these candidates to select the most appropriate one. in this ranking process the systems rely on context information obtained from the document where the anchor is mentioned and/or from wikipedia. in this paper we center our attention in the use of wikipedia links as context information. in particular, we offer a review of several candidate ranking approaches in the state-of-the-art that rely on wikipedia link information. in addition, we provide a comparative empirical evaluation of the different approaches on five different corpora: the tac 2010 corpus and four corpora built from actual wikipedia articles and news items.",2014
natural language inference for arabic using extended tree edit distance with subtrees,"many natural language processing (nlp) applications require the computation of similarities between pairs of syntactic or semantic trees. many researchers have used tree edit distance for this task, but this technique suffers from the drawback that it deals with single node operations only. we have extended the standard tree edit distance algorithm to deal with subtree transformation operations as well as single nodes. the extended algorithm with subtree operations, ted+st, is more effective and flexible than the standard algorithm, especially for applications that pay attention to relations among nodes (e.g. in linguistic trees, deleting a modifier subtree should be cheaper than the sum of deleting its components individually). we describe the use of ted+st for checking entailment between two arabic text snippets. the preliminary results of using ted+st were encouraging when compared with two string-based approaches and with the standard algorithm.",2013
learning optimal bayesian networks: a shortest path perspective,"in this paper, learning a bayesian network structure that optimizes a scoring function for a given dataset is viewed as a shortest path problem in an implicit state-space search graph. this perspective highlights the importance of two research issues: the development of search strategies for solving the shortest path problem, and the design of heuristic functions for guiding the search. this paper introduces several techniques for addressing the issues. one is an a* search algorithm that learns an optimal bayesian network structure by only searching the most promising part of the solution space. the others are mainly two heuristic functions. the first heuristic function represents a simple relaxation of the acyclicity constraint of a bayesian network. although admissible and consistent, the heuristic may introduce too much relaxation and result in a loose bound. the second heuristic function reduces the amount of relaxation by avoiding directed cycles within some groups of variables. empirical results show that these methods constitute a promising approach to learning optimal bayesian network structures.",2013
a survey of multi-objective sequential decision-making,"sequential decision-making problems with multiple objectives arise naturally in practice and pose unique challenges for research in decision-theoretic planning and learning, which has largely focused on single-objective settings. this article surveys algorithms designed for sequential decision-making problems with multiple objectives. though there is a growing body of literature on this subject, little of it makes explicit under what circumstances special methods are needed to solve multi-objective problems. therefore, we identify three distinct scenarios in which converting such a problem to a single-objective one is impossible, infeasible, or undesirable. furthermore, we propose a taxonomy that classifies multi-objective methods according to the applicable scenario, the nature of the scalarization function (which projects multi-objective values to scalar ones), and the type of policies considered. we show how these factors determine the nature of an optimal solution, which can be a single policy, a convex hull, or a pareto front. using this taxonomy, we survey the literature on multi-objective methods for planning and learning. finally, we discuss key applications of such methods and outline opportunities for future work.",2013
taming the infinite chase: query answering under expressive relational constraints,"the chase algorithm is a fundamental tool for query evaluation and for testing query containment under tuple-generating dependencies (tgds) and equality-generating dependencies (egds). so far, most of the research on this topic has focused on cases where the chase procedure terminates. this paper introduces expressive classes of tgds defined via syntactic restrictions: guarded tgds (gtgds) and weakly guarded sets of tgds (wgtgds). for these classes, the chase procedure is not guaranteed to terminate and thus may have an infinite outcome. nevertheless, we prove that the problems of conjunctive-query answering and query containment under such tgds are decidable. we provide decision procedures and tight complexity bounds for these problems. then we show how egds can be incorporated into our results by providing conditions under which egds do not harmfully interact with tgds and do not affect the decidability and complexity of query answering. we show applications of the aforesaid classes of constraints to the problem of answering conjunctive queries in f-logic lite, an object-oriented ontology language, and in some tractable description logics.",2013
an online mechanism for multi-unit demand and its application to plug-in hybrid electric vehicle charging,"we develop an online mechanism for the allocation of an expiring resource to a dynamic agent population. each agent has a non-increasing marginal valuation function for the resource, and an upper limit on the number of units that can be allocated in any period. we propose two versions on a truthful allocation mechanism. each modifies the decisions of a greedy online assignment algorithm by sometimes cancelling an allocation of resources. one version makes this modification immediately upon an allocation decision while a second waits until the point at which an agent departs the market. adopting a prior-free framework, we show that the second approach has better worst-case allocative efficiency and is more scalable. on the other hand, the first approach (with immediate cancellation) may be easier in practice because it does not need to reclaim units previously allocated. we consider an application to recharging plug-in hybrid electric vehicles (phevs). using data from a real-world trial of phevs in the uk, we demonstrate higher system performance than a fixed price system, performance comparable with a standard, but non-truthful scheduling heuristic, and the ability to support 50% more vehicles at the same fuel cost than a simple randomized policy.",2013
optimal implementation of watched literals and more general techniques,"i prove that an implementation technique for scanning lists in backtracking search algorithms is optimal. the result applies to a simple general framework, which i present: applications include watched literal unit propagation in sat and a number of examples in constraint satisfaction. techniques like watched literals are known to be highly space efficient and effective in practice. when implemented in the `circular' approach described here, these techniques also have optimal run time per branch in big-o terms when amortized across a search tree. this also applies when multiple list elements must be found. the constant factor overhead of the worst case is only 2. replacing the existing non-optimal implementation of unit propagation in minisat speeds up propagation by 29%, though this is not enough to improve overall run time significantly.",2013
optimizing sparql query answering over owl ontologies,"the sparql query language is currently being extended by the world wide web consortium (w3c) with so-called entailment regimes. an entailment regime defines how queries are evaluated under more expressive semantics than sparql's standard simple entailment, which is based on subgraph matching. the queries are very expressive since variables can occur within complex concepts and can also bind to concept or role names. in this paper, we describe a sound and complete algorithm for the owl direct semantics entailment regime. we further propose several novel optimizations such as strategies for determining a good query execution order, query rewriting techniques, and show how specialized owl reasoning tasks and the concept and role hierarchy can be used to reduce the query execution time. for determining a good execution order, we propose a cost-based model, where the costs are based on information about the instances of concepts and roles that are extracted from a model abstraction built by an owl reasoner. we present two ordering strategies: a static and a dynamic one. for the dynamic case, we improve the performance by exploiting an individual clustering approach that allows for computing the cost functions based on one individual sample from a cluster. we provide a prototypical implementation and evaluate the efficiency of the proposed optimizations. our experimental study shows that the static ordering usually outperforms the dynamic one when accurate statistics are available. this changes, however, when the statistics are less accurate, e.g., due to nondeterministic reasoning decisions. for queries that go beyond conjunctive instance queries we observe an improvement of up to three orders of magnitude due to the proposed optimizations.",2013
a global model for concept-to-text generation,"concept-to-text generation refers to the task of automatically producing textual output from non-linguistic input. we present a joint model that captures content selection (""what to say"") and surface realization (""how to say"") in an unsupervised domain-independent fashion. rather than breaking up the generation process into a sequence of local decisions, we define a probabilistic context-free grammar that globally describes the inherent structure of the input (a corpus of database records and text describing some of them). we recast generation as the task of finding the best derivation tree for a set of database records and describe an algorithm for decoding in this framework that allows to intersect the grammar with additional information capturing fluency and syntactic well-formedness constraints. experimental evaluation on several domains achieves results competitive with state-of-the-art systems that use domain specific constraints, explicit feature engineering or labeled data.",2013
beth definability in expressive description logics,"the beth definability property, a well-known property from classical logic, is investigated in the context of description logics: if a general l-tbox implicitly defines an l-concept in terms of a given signature, where l is a description logic, then does there always exist over this signature an explicit definition in l for the concept? this property has been studied before and used to optimize reasoning in description logics. in this paper a complete classification of beth definability is provided for extensions of the basic description logic alc with transitive roles, inverse roles, role hierarchies, and/or functionality restrictions, both on arbitrary and on finite structures. moreover, we present a tableau-based algorithm which computes explicit definitions of at most double exponential size. this algorithm is optimal because it is also shown that the smallest explicit definition of an implicitly defined concept may be double exponentially long in the size of the input tbox. finally, if explicit definitions are allowed to be expressed in first-order logic, then we show how to compute them in single exponential time.",2013
defeasible inheritance-based description logics,"defeasible inheritance networks are a non-monotonic framework that deals with hierarchical knowledge. on the other hand, rational closure is acknowledged as a landmark of the preferential approach to non-monotonic reasoning. we will combine these two approaches and define a new non-monotonic closure operation for propositional knowledge bases that combines the advantages of both. then we redefine such a procedure for description logics (dls), a family of logics well-suited to model structured information. in both cases we will provide a simple reasoning method that is built on top of the classical entailment relation and, thus, is amenable of an implementation based on existing reasoners. eventually, we evaluate our approach on well-known landmark test examples.",2013
horn clause contraction functions,"in classical, agm-style belief change, it is assumed that the underlying logic contains classical propositional logic. this is clearly a limiting assumption, particularly in artificial intelligence. consequently there has been recent interest in studying belief change in approaches where the full expressivity of classical propositional logic is not obtained. in this paper we investigate belief contraction in horn knowledge bases. we point out that the obvious extension to the horn case, involving horn remainder sets as a starting point, is problematic. not only do horn remainder sets have undesirable properties, but also some desirable horn contraction functions are not captured by this approach. for horn belief set contraction, we develop an account in terms of a model-theoretic characterisation involving weak remainder sets. maxichoice and partial meet horn contraction is specified, and we show that the problems arising with earlier work are resolved by these approaches. as well, constructions of the specific operators and sets of postulates are provided, and representation results are obtained. we also examine horn package contraction, or contraction by a set of formulas. again, we give a construction and postulate set, linking them via a representation result. last, we investigate the closely-related notion of forgetting in horn clauses. this work is arguably interesting since horn clauses have found widespread use in ai; as well, the results given here may potentially be extended to other areas which make use of horn-like reasoning, such as logic programming, rule-based systems, and description logics. finally, since horn reasoning is weaker than classical reasoning, this work sheds light on the foundations of belief change",2013
ai methods in algorithmic composition: a comprehensive survey,"algorithmic composition is the partial or total automation of the process of music composition by using computers. since the 1950s, different computational techniques related to artificial intelligence have been used for algorithmic composition, including grammatical representations, probabilistic methods, neural networks, symbolic rule-based systems, constraint programming and evolutionary algorithms. this survey aims to be a comprehensive account of research on algorithmic composition, presenting a thorough view of the field for researchers in artificial intelligence.",2013
protecting moving targets with multiple mobile resources,"in recent years, stackelberg security games have been successfully applied to solve resource allocation and scheduling problems in several security domains. however, previous work has mostly assumed that the targets are stationary relative to the defender and the attacker, leading to discrete game models with finite numbers of pure strategies. this paper in contrast focuses on protecting mobile targets that leads to a continuous set of strategies for the players. the problem is motivated by several real-world domains including protecting ferries with escort boats and protecting refugee supply lines. our contributions include: (i) a new game model for multiple mobile defender resources and moving targets with a discretized strategy space for the defender and a continuous strategy space for the attacker. (ii) an efficient linear-programming-based solution that uses a compact representation for the defender's mixed strategy, while accurately modeling the attacker's continuous strategy using a novel sub-interval analysis method. (iii) discussion and analysis of multiple heuristic methods for equilibrium refinement to improve robustness of defender's mixed strategy. (iv) discussion of approaches to sample actual defender schedules from the defender's mixed strategy. (iv) detailed experimental analysis of our algorithms in the ferry protection domain.",2013
reasoning about explanations for negative query answers in dl-lite,"in order to meet usability requirements, most logic-based applications provide explanation facilities for reasoning services. this holds also for description logics, where research has focused on the explanation of both tbox reasoning and, more recently, query answering. besides explaining the presence of a tuple in a query answer, it is important to explain also why a given tuple is missing. we address the latter problem for instance and conjunctive query answering over dl-lite ontologies by adopting abductive reasoning; that is, we look for additions to the abox that force a given tuple to be in the result. as reasoning tasks we consider existence and recognition of an explanation, and relevance and necessity of a given assertion for an explanation. we characterize the computational complexity of these problems for arbitrary, subset minimal, and cardinality minimal explanations.",2013
generating natural language descriptions from owl ontologies: the naturalowl system,"we present naturalowl, a natural language generation system that produces texts describing individuals or classes of owl ontologies. unlike simpler owl verbalizers, which typically express a single axiom at a time in controlled, often not entirely fluent natural language primarily for the benefit of domain experts, we aim to generate fluent and coherent multi-sentence texts for end-users. with a system like naturalowl, one can publish information in owl on the web, along with automatically produced corresponding texts in multiple languages, making the information accessible not only to computer programs and domain experts, but also end-users. we discuss the processing stages of naturalowl, the optional domain-dependent linguistic resources that the system can use at each stage, and why they are useful. we also present trials showing that when the domain-dependent llinguistic resources are available, naturalowl produces significantly better texts compared to a simpler verbalizer, and that the resources can be created with relatively light effort.",2013
a case of pathology in multiobjective heuristic search,"this article considers the performance of the moa* multiobjective search algorithm with heuristic information. it is shown that in certain cases blind search can be more efficient than perfectly informed search, in terms of both node and label expansions. a class of simple graph search problems is defined for which the number of nodes grows linearly with problem size and the number of nondominated labels grows quadratically. it is proved that for these problems the number of node expansions performed by blind moa* grows linearly with problem size, while the number of such expansions performed with a perfectly informed heuristic grows quadratically. it is also proved that the number of label expansions grows quadratically in the blind case and cubically in the informed case.",2013
unsupervised sub-tree alignment for tree-to-tree translation,"this article presents a probabilistic sub-tree alignment model and its application to tree-to-tree machine translation. unlike previous work, we do not resort to surface heuristics or expensive annotated data, but instead derive an unsupervised model to infer the syntactic correspondence between two languages. more importantly, the developed model is syntactically-motivated and does not rely on word alignments. as a by-product, our model outputs a sub-tree alignment matrix encoding a large number of diverse alignments between syntactic structures, from which machine translation systems can efficiently extract translation rules that are often filtered out due to the errors in 1-best alignment. experimental results show that the proposed approach outperforms three state-of-the-art baseline approaches in both alignment accuracy and grammar quality. when applied to machine translation, our approach yields a +1.0 bleu improvement and a -0.9 ter reduction on the nist machine translation evaluation corpora. with tree binarization and fuzzy decoding, it even outperforms a state-of-the-art hierarchical phrase-based system.",2013
"the complexity of optimal monotonic planning: the bad, the good, and the causal graph","for almost two decades, monotonic, or ``delete free,'' relaxation has been one of the key auxiliary tools in the practice of domain-independent deterministic planning. in the particular contexts of both satisficing and optimal planning, it underlies most state-of-the-art heuristic functions. while satisficing planning for monotonic tasks is polynomial-time, optimal planning for monotonic tasks is np-equivalent. here we establish both negative and positive results on the complexity of some wide fragments of optimal monotonic planning, with the fragments being defined around the causal graph topology. our results shed some light on the link between the complexity of general optimal planning and the complexity of optimal planning for the respective monotonic relaxations.",2013
single network relational transductive learning,"relational classification on a single connected network has been of particular interest in the machine learning and data mining communities in the last decade or so. this is mainly due to the explosion in popularity of social networking sites such as facebook, linkedin and google+ amongst others. in statistical relational learning, many techniques have been developed to address this problem, where we have a connected unweighted homogeneous/heterogeneous graph that is partially labeled and the goal is to propagate the labels to the unlabeled nodes. in this paper, we provide a different perspective by enabling the effective use of graph transduction techniques for this problem. we thus exploit the strengths of this class of methods for relational learning problems. we accomplish this by providing a simple procedure for constructing a weight matrix that serves as input to a rich class of graph transduction techniques. our procedure has multiple desirable properties. for example, the weights it assigns to edges between unlabeled nodes naturally relate to a measure of association commonly used in statistics, namely the gamma test statistic. we further portray the efficacy of our approach on synthetic as well as real data, by comparing it with state-of-the-art relational learning algorithms, and graph transduction techniques with an adjacency matrix or a real valued weight matrix computed using available attributes as input. in these experiments we see that our approach consistently outperforms other approaches when the graph is sparsely labeled, and remains competitive with the best when the proportion of known labels increases.",2013
scalable and efficient bayes-adaptive reinforcement learning based on monte-carlo tree search,"bayesian planning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. unfortunately, planning optimally in the face of uncertainty is notoriously taxing, since the search space is enormous. in this paper we introduce a tractable, sample-based method for approximate bayes-optimal planning which exploits monte-carlo tree search. our approach avoids expensive applications of bayes rule within the search tree by sampling models from current beliefs, and furthermore performs this sampling in a lazy manner. this enables it to outperform previous bayesian model-based reinforcement learning algorithms by a significant margin on several well-known benchmark problems. as we show, our approach can even work in problems with an infinite state space that lie qualitatively out of reach of almost all previous work in bayesian exploration.",2013
exact query reformulation over databases with first-order and description logics ontologies,"we study a general framework for query rewriting in the presence of an arbitrary first-order logic ontology over a database signature. the framework supports deciding the existence of a safe-range first-order equivalent reformulation of a query in terms of the database signature, and if so, it provides an effective approach to construct the reformulation based on interpolation using standard theorem proving techniques (e.g., tableau). since the reformulation is a safe-range formula, it is effectively executable as an sql query. at the end, we present a non-trivial application of the framework with ontologies in the very expressive alchoiq description logic, by providing effective means to compute safe-range first-order exact reformulations of queries.",2013
a smooth transition from powerlessness to absolute power,"we study the phase transition of the coalitional manipulation problem for generalized scoring rules. previously it has been shown that, under some conditions on the distribution of votes, if the number of manipulators is o(sqrt{n}), where n is the number of voters, then the probability that a random profile is manipulable by the coalition goes to zero as the number of voters goes to infinity, whereas if the number of manipulators is omega(sqrt{n}), then the probability that a random profile is manipulable goes to one. here we consider the critical window, where a coalition has size c*sqrt{n}, and we show that as c goes from zero to infinity, the limiting probability that a random profile is manipulable goes from zero to one in a smooth fashion, i.e., there is a smooth phase transition between the two regimes. this result analytically validates recent empirical results, and suggests that deciding the coalitional manipulation problem may be of limited computational hardness in practice.",2013
a constraint solver for flexible protein model,"this paper proposes the formalization and implementation of a novel class of constraints aimed at modeling problems related to placement of multi-body systems in the 3-dimensional space. each multi-body is a system composed of body elements, connected by joint relationships and constrained by geometric properties. the emphasis of this investigation is the use of multi-body systems to model native conformations of protein structures---where each body represents an entity of the protein (e.g., an amino acid, a small peptide) and the geometric constraints are related to the spatial properties of the composing atoms. the paper explores the use of the proposed class of constraints to support a variety of different structural analysis of proteins, such as loop modeling and structure prediction. the declarative nature of a constraint-based encoding provides elaboration tolerance and the ability to make use of any additional knowledge in the analysis studies. the filtering capabilities of the proposed constraints also allow to control the number of representative solutions that are withdrawn from the conformational space of the protein, by means of criteria driven by uniform distribution sampling principles. in this scenario it is possible to select the desired degree of precision and/or number of solutions. the filtering component automatically excludes configurations that violate the spatial and geometric properties of the composing multi-body system. the paper illustrates the implementation of a constraint solver based on the multi-body perspective and its empirical evaluation on protein structure analysis problems.",2013
a feature subset selection algorithm automatic recommendation method,"many feature subset selection (fss) algorithms have been proposed, but not all of them are appropriate for a given feature selection problem. at the same time, so far there is rarely a good way to choose appropriate fss algorithms for the problem at hand. thus, fss algorithm automatic recommendation is very important and practically useful. in this paper, a meta learning based fss algorithm automatic recommendation method is presented. the proposed method first identifies the data sets that are most similar to the one at hand by the k-nearest neighbor classification algorithm, and the distances among these data sets are calculated based on the commonly-used data set characteristics. then, it ranks all the candidate fss algorithms according to their performance on these similar data sets, and chooses the algorithms with best performance as the appropriate ones. the performance of the candidate fss algorithms is evaluated by a multi-criteria metric that takes into account not only the classification accuracy over the selected features, but also the runtime of feature selection and the number of selected features. the proposed recommendation method is extensively tested on 115 real world data sets with 22 well-known and frequently-used different fss algorithms for five representative classifiers. the results show the effectiveness of our proposed fss algorithm recommendation method.",2013
scheduling a dynamic aircraft repair shop with limited repair resources,"we address a dynamic repair shop scheduling problem in the context of military aircraft fleet management where the goal is to maintain a full complement of aircraft over the long-term. a number of flights, each with a requirement for a specific number and type of aircraft, are already scheduled over a long horizon. we need to assign aircraft to flights and schedule repair activities while considering the flights requirements, repair capacity, and aircraft failures. the number of aircraft awaiting repair dynamically changes over time due to failures and it is therefore necessary to rebuild the repair schedule online. to solve the problem, we view the dynamic repair shop as successive static repair scheduling sub-problems over shorter time periods. we propose a complete approach based on the logic-based benders decomposition to solve the static sub-problems, and design different rescheduling policies to schedule the dynamic repair shop. computational experiments demonstrate that the benders model is able to find and prove optimal solutions on average four times faster than a mixed integer programming model. the rescheduling approach having both aspects of scheduling over a longer horizon and quickly adjusting the schedule increases aircraft available in the long term by 10% compared to the approaches having either one of the aspects alone.",2013
identifying the class of maxi-consistent operators in argumentation,"dungs abstract argumentation theory can be seen as a general framework for non-monotonic reasoning. an important question is then: what is the class of logics that can be subsumed as instantiations of this theory? the goal of this paper is to identify and study the large class of logic-based instantiations of dungs theory which correspond to the maxi-consistent operator, i.e. to the function which returns maximal consistent subsets of an inconsistent knowledge base. in other words, we study the class of instantiations where very extension of the argumentation system corresponds to exactly one maximal consistent subset of the knowledge base. we show that an attack relation belonging to this class must be conflict-dependent, must not be valid, must not be conflict-complete, must not be symmetric etc. then, we show that some attack relations serve as lower or upper bounds of the class (e.g. if an attack relation contains canonical undercut then it is not a member of this class). by using our results, we show for all existing attack relations whether or not they belong to this class. we also define new attack relations which are members of this class. finally, we interpret our results and discuss more general questions, like: what is the added value of argumentation in such a setting? we believe that this work is a first step towards achieving our long-term goal, which is to better understand the role of argumentation and, particularly, the expressivity of logic-based instantiations of dung-style argumentation frameworks.",2013
distributed reasoning for multiagent simple temporal problems,"this research focuses on building foundational algorithms for scheduling agents that assist people in managing their activities in environments where tempo and complex activity interdependencies outstrip people's cognitive capacity. we address the critical challenge of reasoning over individuals' interacting schedules to efficiently answer queries about how to meet scheduling goals while respecting individual privacy and autonomy to the extent possible. we formally define the multiagent simple temporal problem for naturally capturing and reasoning over the distributed but interconnected scheduling problems of multiple individuals. our hypothesis is that combining bottom-up and top-down approaches will lead to effective solution techniques. in our bottom-up phase, an agent externalizes constraints that compactly summarize how its local subproblem affects other agents' subproblems, whereas in our top-down phase an agent proactively constructs and internalizes new local constraints that decouple its subproblem from others'. we confirm this hypothesis by devising distributed algorithms that calculate summaries of the joint solution space for multiagent scheduling problems, without centralizing or otherwise redistributing the problems. the distributed algorithms permit concurrent execution to achieve significant speedup over the current art and also increase the level of privacy and independence in individual agent reasoning. these algorithms are most advantageous for problems where interactions between the agents are sparse compared to the complexity of agents' individual problems.",2013
a survey on latent tree models and applications,"in data analysis, latent variables play a central role because they help provide powerful insights into a wide variety of phenomena, ranging from biological to human sciences. the latent tree model, a particular type of probabilistic graphical models, deserves attention. its simple structure - a tree - allows simple and efficient inference, while its latent variables capture complex relationships. in the past decade, the latent tree model has been subject to significant theoretical and methodological developments. in this review, we propose a comprehensive study of this model. first we summarize key ideas underlying the model. second we explain how it can be efficiently learned from data. third we illustrate its use within three types of applications: latent structure discovery, multidimensional clustering, and probabilistic inference. finally, we conclude and give promising directions for future researches in this field.",2013
analysis of watson's strategies for playing jeopardy!,"major advances in question answering technology were needed for ibm watson to play jeopardy! at championship level -- the show requires rapid-fire answers to challenging natural language questions, broad general knowledge, high precision, and accurate confidence estimates. in addition, jeopardy! features four types of decision making carrying great strategic importance: (1) daily double wagering; (2) final jeopardy wagering; (3) selecting the next square when in control of the board; (4) deciding whether to attempt to answer, i.e., ""buzz in."" using sophisticated strategies for these decisions, that properly account for the game state and future event probabilities, can significantly boost a player's overall chances to win, when compared with simple ""rule of thumb"" strategies. this article presents our approach to developing watson's game-playing strategies, comprising development of a faithful simulation model, and then using learning and monte-carlo methods within the simulator to optimize watson's strategic decision-making. after giving a detailed description of each of our game-strategy algorithms, we then focus in particular on validating the accuracy of the simulator's predictions, and documenting performance improvements using our methods. quantitative performance benefits are shown with respect to both simple heuristic strategies, and actual human contestant performance in historical episodes. we further extend our analysis of human play to derive a number of valuable and counterintuitive examples illustrating how human contestants may improve their performance on the show.",2013
the arcade learning environment: an evaluation platform for general agents,"in this article we introduce the arcade learning environment (ale): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent ai technology. ale provides an interface to hundreds of atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ale presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. we illustrate the promise of ale by developing and benchmarking domain-independent agents designed using well-established ai techniques for both reinforcement learning and planning. in doing so, we also propose an evaluation methodology made possible by ale, reporting empirical results on over 55 different games. all of the software, including the benchmark agents, is publicly available.",2013
sharing rewards in cooperative connectivity games,"we consider how selfish agents are likely to share revenues derived from maintaining connectivity between important network servers. we model a network where a failure of one node may disrupt communication between other nodes as a cooperative game called the vertex connectivity game (cg). in this game, each agent owns a vertex, and controls all the edges going to and from that vertex. a coalition of agents wins if it fully connects a certain subset of vertices in the graph, called the primary vertices. power indices measure an agent's ability to affect the outcome of the game. we show that in our domain, such indices can be used to both determine the fair share of the revenues an agent is entitled to, and identify significant possible points of failure affecting the reliability of communication in the network. we show that in general graphs, calculating the shapley and banzhaf power indices is #p-complete, but suggest a polynomial algorithm for calculating them in trees. we also investigate finding stable payoff divisions of the revenues in cgs, captured by the game theoretic solution of the core, and its relaxations, the epsilon-core and least core. we show a polynomial algorithm for computing the core of a cg, but show that testing whether an imputation is in the epsilon-core is conp-complete. finally, we show that for trees, it is possible to test for epsilon-core imputations in polynomial time.",2013
learning by observation of agent software images,"learning by observation can be of key importance whenever agents sharing similar features want to learn from each other. this paper presents an agent architecture that enables software agents to learn by direct observation of the actions executed by expert agents while they are performing a task. this is possible because the proposed architecture displays information that is essential for observation, making it possible for software agents to observe each other. the agent architecture supports a learning process that covers all aspects of learning by observation, such as discovering and observing experts, learning from the observed data, applying the acquired knowledge and evaluating the agent's progress. the evaluation provides control over the decision to obtain new knowledge or apply the acquired knowledge to new problems. we combine two methods for learning from the observed information. the first one, the recall method, uses the sequence on which the actions were observed to solve new problems. the second one, the classification method, categorizes the information in the observed data and determines to which set of categories the new problems belong. results show that agents are able to learn in conditions where common supervised learning algorithms fail, such as when agents do not know the results of their actions a priori or when not all the effects of the actions are visible. the results also show that our approach provides better results than other learning methods since it requires shorter learning periods.",2013
strong equivalence of qualitative optimization problems,"we introduce the framework of qualitative optimization problems (or, simply, optimization problems) to represent preference theories. the formalism uses separate modules to describe the space of outcomes to be compared (the generator) and the preferences on outcomes (the selector). we consider two types of optimization problems. they differ in the way the generator, which we model by a propositional theory, is interpreted: by the standard propositional logic semantics, and by the equilibrium-model (answer-set) semantics. under the latter interpretation of generators, optimization problems directly generalize answer-set optimization programs proposed previously. we study strong equivalence of optimization problems, which guarantees their interchangeability within any larger context. we characterize several versions of strong equivalence obtained by restricting the class of optimization problems that can be used as extensions and establish the complexity of associated reasoning tasks. understanding strong equivalence is essential for modular representation of optimization problems and rewriting techniques to simplify them without changing their inherent properties.",2013
lifted variable elimination: decoupling the operators from the constraint language,"lifted probabilistic inference algorithms exploit regularities in the structure of graphical models to perform inference more efficiently. more specifically, they identify groups of interchangeable variables and perform inference once per group, as opposed to once per variable. the groups are defined by means of constraints, so the flexibility of the grouping is determined by the expressivity of the constraint language. existing approaches for exact lifted inference use specific languages for (in)equality constraints, which often have limited expressivity. in this article, we decouple lifted inference from the constraint language. we define operators for lifted inference in terms of relational algebra operators, so that they operate on the semantic level (the constraints' extension) rather than on the syntactic level, making them language-independent. as a result, lifted inference can be performed using more powerful constraint languages, which provide more opportunities for lifting. we empirically demonstrate that this can improve inference efficiency by orders of magnitude, allowing exact inference where until now only approximate inference was feasible.",2013
decentralized anti-coordination through multi-agent learning,"to achieve an optimal outcome in many situations, agents need to choose distinct actions from one another. this is the case notably in many resource allocation problems, where a single resource can only be used by one agent at a time. how shall a designer of a multi-agent system program its identical agents to behave each in a different way? from a game theoretic perspective, such situations lead to undesirable nash equilibria. for example consider a resource allocation game in that two players compete for an exclusive access to a single resource. it has three nash equilibria. the two pure-strategy ne are efficient, but not fair. the one mixed-strategy ne is fair, but not efficient. aumann's notion of correlated equilibrium fixes this problem: it assumes a correlation device that suggests each agent an action to take. however, such a ""smart"" coordination device might not be available. we propose using a randomly chosen, ""stupid"" integer coordination signal. ""smart"" agents learn which action they should use for each value of the coordination signal. we present a multi-agent learning algorithm that converges in polynomial number of steps to a correlated equilibrium of a channel allocation game, a variant of the resource allocation game. we show that the agents learn to play for each coordination signal value a randomly chosen pure-strategy nash equilibrium of the game. therefore, the outcome is an efficient correlated equilibrium. this ce becomes more fair as the number of the available coordination signal values increases.",2013
on the computation of fully proportional representation,"we investigate two systems of fully proportional representation suggested by chamberlin courant and monroe. both systems assign a representative to each voter so that the ""sum of misrepresentations"" is minimized. the winner determination problem for both systems is known to be np-hard, hence this work aims at investigating whether there are variants of the proposed rules and/or specific electorates for which these problems can be solved efficiently. as a variation of these rules, instead of minimizing the sum of misrepresentations, we considered minimizing the maximal misrepresentation introducing effectively two new rules. in the general case these ""minimax"" versions of classical rules appeared to be still np-hard. we investigated the parameterized complexity of winner determination of the two classical and two new rules with respect to several parameters. here we have a mixture of positive and negative results: e.g., we proved fixed-parameter tractability for the parameter the number of candidates but fixed-parameter intractability for the number of winners. for single-peaked electorates our results are overwhelmingly positive: we provide polynomial-time algorithms for most of the considered problems. the only rule that remains np-hard for single-peaked electorates is the classical monroe rule.",2013
topic segmentation and labeling in asynchronous conversations,"topic segmentation and labeling is often considered a prerequisite for higher-level conversation analysis and has been shown to be useful in many natural language processing (nlp) applications. we present two new corpora of email and blog conversations annotated with topics, and evaluate annotator reliability for the segmentation and labeling tasks in these asynchronous conversations. we propose a complete computational framework for topic segmentation and labeling in asynchronous conversations. our approach extends state-of-the-art methods by considering a fine-grained structure of an asynchronous conversation, along with other conversational features by applying recent graph-based methods for nlp. for topic segmentation, we propose two novel unsupervised models that exploit the fine-grained conversational structure, and a novel graph-theoretic supervised model that combines lexical, conversational and topic features. for topic labeling, we propose two novel (unsupervised) random walk models that respectively capture conversation specific clues from two different sources: the leading sentences and the fine-grained conversational structure. empirical evaluation shows that the segmentation and the labeling performed by our best models beat the state-of-the-art, and are highly correlated with human annotations.",2013
a refined view of causal graphs and component sizes: sp-closed graph classes and beyond,"the causal graph of a planning instance is an important tool for planning both in practice and in theory. the theoretical studies of causal graphs have largely analysed the computational complexity of planning for instances where the causal graph has a certain structure, often in combination with other parameters like the domain size of the variables. chen and gim&#233;nez ignored even the structure and considered only the size of the weakly connected components. they proved that planning is tractable if the components are bounded by a constant and otherwise intractable. their intractability result was, however, conditioned by an assumption from parameterised complexity theory that has no known useful relationship with the standard complexity classes. we approach the same problem from the perspective of standard complexity classes, and prove that planning is np-hard for classes with unbounded components under an additional restriction we refer to as sp-closed. we then argue that most np-hardness theorems for causal graphs are difficult to apply and, thus, prove a more general result; even if the component sizes grow slowly and the class is not densely populated with graphs, planning still cannot be tractable unless the polynomial hierachy collapses. both these results still hold when restricted to the class of acyclic causal graphs. we finally give a partial characterization of the borderline between np-hard and np-intermediate classes, giving further insight into the problem.",2013
asymmetric distributed constraint optimization problems,"distributed constraint optimization (dcop) is a powerful framework for representing and solving distributed combinatorial problems, where the variables of the problem are owned by different agents. many multi-agent problems include constraints that produce different gains (or costs) for the participating agents. asymmetric gains of constrained agents cannot be naturally represented by the standard dcop model. the present paper proposes a general framework for asymmetric dcops (adcops). in adcops different agents may have different valuations for constraints that they are involved in. the new framework bridges the gap between multi-agent problems which tend to have asymmetric structure and the standard symmetric dcop model. the benefits of the proposed model over previous attempts to generalize the dcop model are discussed and evaluated. innovative algorithms that apply to the special properties of the proposed adcop model are presented in detail. these include complete algorithms that have a substantial advantage in terms of runtime and network load over existing algorithms (for standard dcops) which use alternative representations. moreover, standard incomplete algorithms (i.e., local search algorithms) are inapplicable to the existing dcop representations of asymmetric constraints and when they are applied to the new adcop framework they often fail to converge to a local optimum and yield poor results. the local search algorithms proposed in the present paper converge to high quality solutions. the experimental evidence that is presented reveals that the proposed local search algorithms for adcops achieve high quality solutions while preserving a high level of privacy.",2013
protecting privacy through distributed computation in multi-agent decision making,"as large-scale theft of data from corporate servers is becoming increasingly common, it becomes interesting to examine alternatives to the paradigm of centralizing sensitive data into large databases. instead, one could use cryptography and distributed computation so that sensitive data can be supplied and processed in encrypted form, and only the final result is made known. in this paper, we examine how such a paradigm can be used to implement constraint satisfaction, a technique that can solve a broad class of ai problems such as resource allocation, planning, scheduling, and diagnosis. most previous work on privacy in constraint satisfaction only attempted to protect specific types of information, in particular the feasibility of particular combinations of decisions. we formalize and extend these restricted notions of privacy by introducing four types of private information, including the feasibility of decisions and the final decisions made, but also the identities of the participants and the topology of the problem. we present distributed algorithms that allow computing solutions to constraint satisfaction problems while maintaining these four types of privacy. we formally prove the privacy properties of these algorithms, and show experiments that compare their respective performance on benchmark problems.",2013
heuristic search when time matters,"in many applications of shortest-path algorithms, it is impractical to find a provably optimal solution; one can only hope to achieve an appropriate balance between search time and solution cost that respects the user's preferences. preferences come in many forms; we consider utility functions that linearly trade-off search time and solution cost. many natural utility functions can be expressed in this form. for example, when solution cost represents the makespan of a plan, equally weighting search time and plan makespan minimizes the time from the arrival of a goal until it is achieved. current state-of-the-art approaches to optimizing utility functions rely on anytime algorithms, and the use of extensive training data to compute a termination policy. we propose a more direct approach, called bugsy, that incorporates the utility function directly into the search, obviating the need for a separate termination policy. we describe a new method based on off-line parameter tuning and a novel benchmark domain for planning under time pressure based on platform-style video games. we then present what we believe to be the first empirical study of applying anytime monitoring to heuristic search, and we compare it with our proposals. our results suggest that the parameter tuning technique can give the best performance if a representative set of training instances is available. if not, then bugsy is the algorithm of choice, as it performs well and does not require any off-line training. this work extends the tradition of research on metareasoning for search by illustrating the benefits of embedding lightweight reasoning about time into the search algorithm itself.",2013
acyclicity notions for existential rules and their application to query answering in ontologies,"answering conjunctive queries (cqs) over a set of facts extended with existential rules is a prominent problem in knowledge representation and databases. this problem can be solved using the chase algorithm, which extends the given set of facts with fresh facts in order to satisfy the rules. if the chase terminates, then cqs can be evaluated directly in the resulting set of facts. the chase, however, does not terminate necessarily, and checking whether the chase terminates on a given set of rules and facts is undecidable. numerous acyclicity notions were proposed as sufficient conditions for chase termination. in this paper, we present two new acyclicity notions called model-faithful acyclicity (mfa) and model-summarising acyclicity (msa). furthermore, we investigate the landscape of the known acyclicity notions and establish a complete taxonomy of all notions known to us. finally, we show that mfa and msa generalise most of these notions. existential rules are closely related to the horn fragments of the owl 2 ontology language; furthermore, several prominent owl 2 reasoners implement cq answering by using the chase to materialise all relevant facts. in order to avoid termination problems, many of these systems handle only the owl 2 rl profile of owl 2; furthermore, some systems go beyond owl 2 rl, but without any termination guarantees. in this paper we also investigate whether various acyclicity notions can provide a principled and practical solution to these problems. on the theoretical side, we show that query answering for acyclic ontologies is of lower complexity than for general ontologies. on the practical side, we show that many of the commonly used owl 2 ontologies are msa, and that the number of facts obtained by materialisation is not too large. our results thus suggest that principled development of materialisation-based owl 2 reasoners is practically feasible.",2013
a decidable extension of sroiq with complex role chains and unions,"we design a decidable extension of the description logic sroiq underlying the web ontology language owl 2. the new logic, called sr+oiq, supports a controlled use of role axioms whose right-hand side may contain role chains or role unions. we give a tableau algorithm for checking concept satisfiability with respect to sr+oiq ontologies and prove its soundness, completeness and termination.",2013
"framing image description as a ranking task: data, models and evaluation metrics","the ability to associate images with natural language sentences that describe what is depicted in them is a hallmark of image understanding, and a prerequisite for applications such as sentence-based image search. in analogy to image search, we propose to frame sentence-based image annotation as the task of ranking a given pool of captions. we introduce a new benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. we introduce a number of systems that perform quite well on this task, even though they are only based on features that can be obtained with minimal supervision. our results clearly indicate the importance of training on multiple captions per image, and of capturing syntactic (word order-based) and semantic features of these captions. we also perform an in-depth comparison of human and automatic evaluation metrics for this task, and propose strategies for collecting human judgments cheaply and on a very large scale, allowing us to augment our collection with additional relevance judgments of which captions describe which image. our analysis shows that metrics that consider the ranked list of results for each query image or sentence are significantly more robust than metrics that are based on a single response per query. moreover, our study suggests that the evaluation of ranking-based image description systems may be fully automated.",2013
short and long supports for constraint propagation,"special-purpose constraint propagation algorithms frequently make implicit use of short supports -- by examining a subset of the variables, they can infer support (a justification that a variable-value pair may still form part of an assignment that satisfies the constraint) for all other variables and values and save substantial work -- but short supports have not been studied in their own right. the two main contributions of this paper are the identification of short supports as important for constraint propagation, and the introduction of haggisgac, an efficient and effective general purpose propagation algorithm for exploiting short supports. given the complexity of haggisgac, we present it as an optimised version of a simpler algorithm shortgac. although experiments demonstrate the efficiency of shortgac compared with other general-purpose propagation algorithms where a compact set of short supports is available, we show theoretically and experimentally that haggisgac is even better. we also find that haggisgac performs better than gac-schema on full-length supports. we also introduce a variant algorithm haggisgac-stable, which is adapted to avoid work on backtracking and in some cases can be faster and have significant reductions in memory use. all the proposed algorithms are excellent for propagating disjunctions of constraints. in all experiments with disjunctions we found our algorithms to be faster than constructive or and gac-schema by at least an order of magnitude, and up to three orders of magnitude.",2013
optimal rectangle packing: an absolute placement approach,"we consider the problem of finding all enclosing rectangles of minimum area that can contain a given set of rectangles without overlap. our rectangle packer chooses the x-coordinates of all the rectangles before any of the y-coordinates. we then transform the problem into a perfect-packing problem with no empty space by adding additional rectangles. to determine the y-coordinates, we branch on the different rectangles that can be placed in each empty position. our packer allows us to extend the known solutions for a consecutive-square benchmark from 27 to 32 squares. we also introduce three new benchmarks, avoiding properties that make a benchmark easy, such as rectangles with shared dimensions. our third benchmark consists of rectangles of increasingly high precision. to pack them efficiently, we limit the rectangles' coordinates and the bounding box dimensions to the set of subset sums of the rectangles' dimensions. overall, our algorithms represent the current state-of-the-art for this problem, outperforming other algorithms by orders of magnitude, depending on the benchmark.",2013
automatic aggregation by joint modeling of aspects and values,"we present a model for aggregation of product review snippets by joint aspect identification and sentiment analysis. our model simultaneously identifies an underlying set of ratable aspects presented in the reviews of a product (e.g., sushi and miso for a japanese restaurant) and determines the corresponding sentiment of each aspect. this approach directly enables discovery of highly-rated or inconsistent aspects of a product. our generative model admits an efficient variational mean-field inference algorithm. it is also easily extensible, and we describe several modifications and their effects on model structure and inference. we test our model on two tasks, joint aspect identification and sentiment analysis on a set of yelp reviews and aspect identification alone on a set of medical summaries. we evaluate the performance of the model on aspect identification, sentiment analysis, and per-word labeling accuracy. we demonstrate that our model outperforms applicable baselines by a considerable margin, yielding up to 32% relative error reduction on aspect identification and up to 20% relative error reduction on sentiment analysis.",2013
undominated groves mechanisms,"the family of groves mechanisms, which includes the well-known vcg mechanism (also known as the clarke mechanism), is a family of efficient and strategy-proof mechanisms. unfortunately, the groves mechanisms are generally not budget balanced. that is, under such mechanisms, payments may flow into or out of the system of the agents, resulting in deficits or reduced utilities for the agents. we consider the following problem: within the family of groves mechanisms, we want to identify mechanisms that give the agents the highest utilities, under the constraint that these mechanisms must never incur deficits. we adopt a prior-free approach. we introduce two general measures for comparing mechanisms in prior-free settings. we say that a non-deficit groves mechanism m individually dominates another non-deficit groves mechanism m' if for every type profile, every agent's utility under m is no less than that under m', and this holds with strict inequality for at least one type profile and one agent. we say that a non-deficit groves mechanism m collectively dominates another non-deficit groves mechanism m' if for every type profile, the agents' total utility under m is no less than that under m', and this holds with strict inequality for at least one type profile. the above definitions induce two partial orders on non-deficit groves mechanisms. we study the maximal elements corresponding to these two partial orders, which we call the individually undominated mechanisms and the collectively undominated mechanisms, respectively.",2013
generating extractive summaries of scientific paradigms,"researchers and scientists increasingly find themselves in the position of having to quickly understand large amounts of technical material. our goal is to effectively serve this need by using bibliometric text mining and summarization techniques to generate summaries of scientific literature. we show how we can use citations to produce automatically generated, readily consumable, technical extractive summaries. we first propose c-lexrank, a model for summarizing single scientific articles based on citations, which employs community detection and extracts salient information-rich sentences. next, we further extend our experiments to summarize a set of papers, which cover the same scientific topic. we generate extractive summaries of a set of question answering (qa) and dependency parsing (dp) papers, their abstracts, and their citation sentences and show that citations have unique information amenable to creating a summary.",2013
integrative semantic dependency parsing via efficient large-scale feature selection,"semantic parsing, i.e., the automatic derivation of meaning representation such as an instantiated predicate-argument structure for a sentence, plays a critical role in deep processing of natural language. unlike all other top systems of semantic dependency parsing that have to rely on a pipeline framework to chain up a series of submodels each specialized for a specific subtask, the one presented in this article integrates everything into one model, in hopes of achieving desirable integrity and practicality for real applications while maintaining a competitive performance. this integrative approach tackles semantic parsing as a word pair classification problem using a maximum entropy classifier. we leverage adaptive pruning of argument candidates and large-scale feature selection engineering to allow the largest feature space ever in use so far in this field, it achieves a state-of-the-art performance on the evaluation data set for conll-2008 shared task, on top of all but one top pipeline system, confirming its feasibility and effectiveness.",2013
toward supervised anomaly detection,"anomaly detection is being regarded as an unsupervised learning task as anomalies stem from adversarial or unlikely events with unknown distributions. however, the predictive performance of purely unsupervised anomaly detection often fails to match the required detection rates in many tasks and there exists a need for labeled data to guide the model generation. our first contribution shows that classical semi-supervised approaches, originating from a supervised classifier, are inappropriate and hardly detect new and unknown anomalies. we argue that semi-supervised anomaly detection needs to ground on the unsupervised learning paradigm and devise a novel algorithm that meets this requirement. although being intrinsically non-convex, we further show that the optimization problem has a convex equivalent under relatively mild assumptions. additionally, we propose an active learning strategy to automatically filter candidates for labeling. in an empirical study on network intrusion detection data, we observe that the proposed learning methodology requires much less labeled data than the state-of-the-art, while achieving higher detection accuracies.",2013
parameterized complexity results for exact bayesian network structure learning,"bayesian network structure learning is the notoriously difficult problem of discovering a bayesian network that optimally represents a given set of training data. in this paper we study the computational worst-case complexity of exact bayesian network structure learning under graph theoretic restrictions on the (directed) super-structure. the super-structure is an undirected graph that contains as subgraphs the skeletons of solution networks. we introduce the directed super-structure as a natural generalization of its undirected counterpart. our results apply to several variants of score-based bayesian network structure learning where the score of a network decomposes into local scores of its nodes. results: we show that exact bayesian network structure learning can be carried out in non-uniform polynomial time if the super-structure has bounded treewidth, and in linear time if in addition the super-structure has bounded maximum degree. furthermore, we show that if the directed super-structure is acyclic, then exact bayesian network structure learning can be carried out in quadratic time. we complement these positive results with a number of hardness results. we show that both restrictions (treewidth and degree) are essential and cannot be dropped without loosing uniform polynomial time tractability (subject to a complexity-theoretic assumption). similarly, exact bayesian network structure learning remains np-hard for ""almost acyclic"" directed super-structures. furthermore, we show that the restrictions remain essential if we do not search for a globally optimal network but aim to improve a given network by means of at most k arc additions, arc deletions, or arc reversals (k-neighborhood local search).",2013
boolean equi-propagation for concise and efficient sat encodings of combinatorial problems,"we present an approach to propagation-based sat encoding of combinatorial problems, boolean equi-propagation, where constraints are modeled as boolean functions which propagate information about equalities between boolean literals. this information is then applied to simplify the cnf encoding of the constraints. a key factor is that considering only a small fragment of a constraint model at one time enables us to apply stronger, and even complete, reasoning to detect equivalent literals in that fragment. once detected, equivalences apply to simplify the entire constraint model and facilitate further reasoning on other fragments. equi-propagation in combination with partial evaluation and constraint simplification provide the foundation for a powerful approach to sat-based finite domain constraint solving. we introduce a tool called bee (ben-gurion equi-propagation encoder) based on these ideas and demonstrate for a variety of benchmarks that our approach leads to a considerable reduction in the size of cnf encodings and subsequent speed-ups in sat solving times.",2013
a hybrid lp-rpg heuristic for modelling numeric resource flows in planning,"although the use of metric fluents is fundamental to many practical planning problems, the study of heuristics to support fully automated planners working with these fluents remains relatively unexplored. the most widely used heuristic is the relaxation of metric fluents into interval-valued variables --- an idea first proposed a decade ago. other heuristics depend on domain encodings that supply additional information about fluents, such as capacity constraints or other resource-related annotations. a particular challenge to these approaches is in handling interactions between metric fluents that represent exchange, such as the transformation of quantities of raw materials into quantities of processed goods, or trading of money for materials. the usual relaxation of metric fluents is often very poor in these situations, since it does not recognise that resources, once spent, are no longer available to be spent again. we present a heuristic for numeric planning problems building on the propositional relaxed planning graph, but using a mathematical program for numeric reasoning. we define a class of producer--consumer planning problems and demonstrate how the numeric constraints in these can be modelled in a mixed integer program (mip). this mip is then combined with a metric relaxed planning graph (rpg) heuristic to produce an integrated hybrid heuristic. the mip tracks resource use more accurately than the usual relaxation, but relaxes the ordering of actions, while the rpg captures the causal propositional aspects of the problem. we discuss how these two components interact to produce a single unified heuristic and go on to explore how further numeric features of planning problems can be integrated into the mip. we show that encoding a limited subset of the propositional problem to augment the mip can yield more accurate guidance, partly by exploiting structure such as propositional landmarks and propositional resources. our results show that the use of this heuristic enhances scalability on problems where numeric resource interaction is key in finding a solution.",2013
qualitative order of magnitude energy-flow-based failure modes and effects analysis,"this paper presents a structured power and energy-flow-based qualitative modelling approach that is applicable to a variety of system types including electrical and fluid flow. the modelling is split into two parts. power flow is a global phenomenon and is therefore naturally represented and analysed by a network comprised of the relevant structural elements from the components of a system. the power flow analysis is a platform for higher-level behaviour prediction of energy related aspects using local component behaviour models to capture a state-based representation with a global time. the primary application is failure modes and effects analysis (fmea) and a form of exaggeration reasoning is used, combined with an order of magnitude representation to derive the worst case failure modes. the novel aspects of the work are an order of magnitude(om) qualitative network analyser to represent any power domain and topology, including multiple power sources, a feature that was not required for earlier specialised electrical versions of the approach. secondly, the representation of generalised energy related behaviour as state-based local models is presented as a modelling strategy that can be more vivid and intuitive for a range of topologically complex applications than qualitative equation-based representations.the two-level modelling strategy allows the broad system behaviour coverage of qualitative simulation to be exploited for the fmea task, while limiting the difficulties of qualitative ambiguity explanation that can arise from abstracted numerical models. we have used the method to support an automated fmea system with examples of an aircraft fuel system and domestic a heating system discussed in this paper.",2013
incremental clustering and expansion for faster optimal planning in dec-pomdps,"this article presents the state-of-the-art in optimal solution methods for decentralized partially observable markov decision processes (dec-pomdps), which are general models for collaborative multiagent planning under uncertainty. building off the generalized multiagent a* (gmaa*) algorithm, which reduces the problem to a tree of one-shot collaborative bayesian games (cbgs), we describe several advances that greatly expand the range of dec-pomdps that can be solved optimally. first, we introduce lossless incremental clustering of the cbgs solved by gmaa*, which achieves exponential speedups without sacrificing optimality. second, we introduce incremental expansion of nodes in the gmaa* search tree, which avoids the need to expand all children, the number of which is in the worst case doubly exponential in the node's depth. this is particularly beneficial when little clustering is possible. in addition, we introduce new hybrid heuristic representations that are more compact and thereby enable the solution of larger dec-pomdps. we provide theoretical guarantees that, when a suitable heuristic is used, both incremental clustering and incremental expansion yield algorithms that are both complete and search equivalent. finally, we present extensive empirical results demonstrating that gmaa*-ice, an algorithm that synthesizes these advances, can optimally solve dec-pomdps of unprecedented size.",2013
probabilistic planning for continuous dynamic systems under bounded risk,"this paper presents a model-based planner called the probabilistic sulu planner or the p-sulu planner, which controls stochastic systems in a goal directed manner within user-specified risk bounds. the objective of the p-sulu planner is to allow users to command continuous, stochastic systems, such as unmanned aerial and space vehicles, in a manner that is both intuitive and safe. to this end, we first develop a new plan representation called a chance-constrained qualitative state plan (ccqsp), through which users can specify the desired evolution of the plant state as well as the acceptable level of risk. an example of a ccqsp statement is ``go to a through b within 30 minutes, with less than 0.001% probability of failure."" we then develop the p-sulu planner, which can tractably solve a ccqsp planning problem. in order to enable ccqsp planning, we develop the following two capabilities in this paper: 1) risk-sensitive planning with risk bounds, and 2) goal-directed planning in a continuous domain with temporal constraints. the first capability is to ensures that the probability of failure is bounded. the second capability is essential for the planner to solve problems with a continuous state space such as vehicle path planning. we demonstrate the capabilities of the p-sulu planner by simulations on two real-world scenarios: the path planning and scheduling of a personal aerial vehicle as well as the space rendezvous of an autonomous cargo spacecraft.",2013
predicting behavior in unstructured bargaining with a probability distribution,"in experimental tests of human behavior in unstructured bargaining games, typically many joint utility outcomes are found to occur, not just one. this suggests we predict the outcome of such a game as a probability distribution. this is in contrast to what is conventionally done (e.g, in the nash bargaining solution), which is predict a single outcome. we show how to translate nash's bargaining axioms to provide a distribution over outcomes rather than a single outcome. we then prove that a subset of those axioms forces the distribution over utility outcomes to be a power-law distribution. unlike nash's original result, our result holds even if the feasible set is finite. when the feasible set is convex and comprehensive, the mode of the power law distribution is the harsanyi bargaining solution, and if we require symmetry it is the nash bargaining solution. however, in general these modes of the joint utility distribution are not the experimentalist's bayes-optimal predictions for the joint utility. nor are the bargains corresponding to the modes of those joint utility distributions the modes of the distribution over bargains in general, since more than one bargain may result in the same joint utility. after introducing distributional bargaining solution concepts, we show how an external regulator can use them to optimally design an unstructured bargaining scenario. throughout we demonstrate our analysis in computational experiments involving flight rerouting negotiations in the national airspace system. we emphasize that while our results are formulated for unstructured bargaining, they can also be used to make predictions for noncooperative games where the modeler knows the utility functions of the players over possible outcomes of the game, but does not know the move spaces the players use to determine those outcomes.",2013
efficient computation of the shapley value for game-theoretic network centrality,"the shapley value---probably the most important normative payoff division scheme in coalitional games---has recently been advocated as a useful measure of centrality in networks. however, although this approach has a variety of real-world applications (including social and organisational networks, biological networks and communication networks), its computational properties have not been widely studied. to date, the only practicable approach to compute shapley value-based centrality has been via monte carlo simulations which are computationally expensive and not guaranteed to give an exact answer. against this background, this paper presents the first study of the computational aspects of the shapley value for network centralities. specifically, we develop exact analytical formulae for shapley value-based centrality in both weighted and unweighted networks and develop efficient (polynomial time) and exact algorithms based on them. we empirically evaluate these algorithms on two real-life examples (an infrastructure network representing the topology of the western states power grid and a collaboration network from the field of astrophysics) and demonstrate that they deliver significant speedups over the monte carlo approach. for instance, in the case of unweighted networks our algorithms are able to return the exact solution about 1600 times faster than the monte carlo approximation, even if we allow for a generous 10% error margin for the latter method.",2013
description logic knowledge and action bases,"description logic knowledge and action bases (kab) are a mechanism for providing both a semantically rich representation of the information on the domain of interest in terms of a description logic knowledge base and actions to change such information over time, possibly introducing new objects. we resort to a variant of dl-lite where the unique name assumption is not enforced and where equality between objects may be asserted and inferred. actions are specified as sets of conditional effects, where conditions are based on epistemic queries over the knowledge base (tbox and abox), and effects are expressed in terms of new aboxes. in this setting, we address verification of temporal properties expressed in a variant of first-order mu-calculus with quantification across states. notably, we show decidability of verification, under a suitable restriction inspired by the notion of weak acyclicity in data exchange.",2013
numvc: an efficient local search algorithm for minimum vertex cover,"the minimum vertex cover (mvc) problem is a prominent np-hard combinatorial optimization problem of great importance in both theory and application. local search has proved successful for this problem. however, there are two main drawbacks in state-of-the-art mvc local search algorithms. first, they select a pair of vertices to exchange simultaneously, which is time-consuming. secondly, although using edge weighting techniques to diversify the search, these algorithms lack mechanisms for decreasing the weights. to address these issues, we propose two new strategies: two-stage exchange and edge weighting with forgetting. the two-stage exchange strategy selects two vertices to exchange separately and performs the exchange in two stages. the strategy of edge weighting with forgetting not only increases weights of uncovered edges, but also decreases some weights for each edge periodically. these two strategies are used in designing a new mvc local search algorithm, which is referred to as numvc. we conduct extensive experimental studies on the standard benchmarks, namely dimacs and bhoslib. the experiment comparing numvc with state-of-the-art heuristic algorithms show that numvc is at least competitive with the nearest competitor namely pls on the dimacs benchmark, and clearly dominates all competitors on the bhoslib benchmark. also, experimental results indicate that numvc finds an optimal solution much faster than the current best exact algorithm for maximum clique on random instances as well as some structured ones. moreover, we study the effectiveness of the two strategies and the run-time behaviour through experimental analysis.",2013
interactions between knowledge and time in a first-order logic for multi-agent systems: completeness results,"we investigate a class of first-order temporal-epistemic logics for reasoning about multi-agent systems. we encode typical properties of systems including perfect recall, synchronicity, no learning, and having a unique initial state in terms of variants of quantified interpreted systems, a first-order extension of interpreted systems. we identify several monodic fragments of first-order temporal-epistemic logic and show their completeness with respect to their corresponding classes of quantified interpreted systems.",2012
the tractability of csp classes defined by forbidden patterns,"the constraint satisfaction problem (csp) is a general problem central to computer science and artificial intelligence. although the csp is np-hard in general, considerable effort has been spent on identifying tractable subclasses. the main two approaches consider structural properties (restrictions on the hypergraph of constraint scopes) and relational properties (restrictions on the language of constraint relations). recently, some authors have considered hybrid properties that restrict the constraint hypergraph and the relations simultaneously. our key contribution is the novel concept of a csp pattern and classes of problems defined by forbidden patterns (which can be viewed as forbidding generic sub-problems). we describe the theoretical framework which can be used to reason about classes of problems defined by forbidden patterns. we show that this framework generalises certain known hybrid tractable classes. although we are not close to obtaining a complete characterisation concerning the tractability of general forbidden patterns, we prove a dichotomy in a special case: classes of problems that arise when we can only forbid binary negative patterns (generic sub-problems in which only disallowed tuples are specified). in this case we show that all (finite sets of) forbidden patterns define either polynomial-time solvable or np-complete classes of instances.",2012
an approximative inference method for solving so satisfiability problems,"this paper considers the fragment &#8707;&#8704;so of second-order logic. many interesting problems, such as conformant planning, can be naturally expressed as finite domain satisfiability problems of this logic. such satisfiability problems are computationally hard (&#931;p2) and many of these problems are often solved approximately. in this paper, we develop a general approximative method, i.e., a sound but incomplete method, for solving &#8707;&#8704;so satisfiability problems. we use a syntactic representation of a constraint propagation method for first-order logic to transform such an &#8707;&#8704;so satisfiability problem to an &#8707;so(id) satisfiability problem (second-order logic, extended with inductive definitions). the finite domain satisfiability problem for the latter language is in np and can be handled by several existing solvers. inductive definitions are a powerful knowledge representation tool, and this moti- vates us to also approximate &#8707;&#8704;so(id) problems. in order to do this, we first show how to perform propagation on such inductive definitions. next, we use this to approximate &#8707;&#8704;so(id) satisfiability problems. all this provides a general theoretical framework for a number of approximative methods in the literature. moreover, we also show how we can use this framework for solving practical useful problems, such as conformant planning, in an effective way.",2012
towards unsupervised learning of temporal relations between events,"automatic extraction of temporal relations between event pairs is an important task for several natural language processing applications such as question answering, information extraction, and summarization. since most existing methods are supervised and require large corpora, which for many languages do not exist, we have concentrated our efforts to reduce the need for annotated data as much as possible. this paper presents two different algorithms towards this goal. the first algorithm is a weakly supervised machine learning approach for classification of temporal relations between events. in the first stage, the algorithm learns a general classifier from an annotated corpus. then, inspired by the hypothesis of ""one type of temporal relation per discourse'', it extracts useful information from a cluster of topically related documents. we show that by combining the global information of such a cluster with local decisions of a general classifier, a bootstrapping cross-document classifier can be built to extract temporal relations between events. our experiments show that without any additional annotated data, the accuracy of the proposed algorithm is higher than that of several previous successful systems. the second proposed method for temporal relation extraction is based on the expectation maximization (em) algorithm. within em, we used different techniques such as a greedy best-first search and integer linear programming for temporal inconsistency removal. we think that the experimental results of our em based algorithm, as a first step toward a fully unsupervised temporal relation extraction method, is encouraging.",2012
coalition structure generation over graphs,"we give the analysis of the computational complexity of coalition structure generation over graphs. given an undirected graph g = (n,e) and a valuation function v : p(n) &#8594; r over the subsets of nodes, the problem is to find a partition of n into connected subsets, that maximises the sum of the components values. this problem is generally npcomplete; in particular, it is hard for a defined class of valuation functions which are independent of disconnected membersthat is, two nodes have no effect on each others marginal con- tribution to their vertex separator. nonetheless, for all such functions we provide bounds on the complexity of coalition structure generation over general and minorfree graphs. our proof is constructive and yields algorithms for solving corresponding instances of the problem. furthermore, we derive linear time bounds for graphs of bounded treewidth. however, as we show, the problem remains npcomplete for planar graphs, and hence, for any k_k minorfree graphs where k &#8805; 5. moreover, a 3-sat problem with m clauses can be represented by a coalition structure generation problem over a planar graph with o(m^2) nodes. importantly, our hardness result holds for a particular subclass of valuation functions, termed edge sum, where the value of each subset of nodes is simply determined by the sum of given weights of the edges in the induced subgraph.",2012
reasoning over ontologies with hidden content: the import-by-query approach,"there is currently a growing interest in techniques for hiding parts of the signature of an ontology kh that is being reused by another ontology kv. towards this goal, in this paper we propose the import-by-query framework, which makes the content of kh accessible through a limited query interface. if kv reuses the symbols from kh in a certain restricted way, one can reason over kv u kh by accessing only kv and the query interface. we map out the landscape of the import-by-query problem. in particular, we outline the limitations of our framework and prove that certain restrictions on the expressivity of kh and the way in which kv reuses symbols from kh are strictly necessary to enable reasoning in our setting. we also identify cases in which reasoning is possible and we present suitable import-by-query reasoning algorithms.",2012
generating approximate solutions to the ttp using a linear distance relaxation,"in some domestic professional sports leagues, the home stadiums are located in cities connected by a common train line running in one direction. for these instances, we can incorporate this geographical information to determine optimal or nearly-optimal solutions to the n-team traveling tournament problem (ttp), an np-hard sports scheduling problem whose solution is a double round-robin tournament schedule that minimizes the sum total of distances traveled by all n teams. we introduce the linear distance traveling tournament problem (ld-ttp), and solve it for n=4 and n=6, generating the complete set of possible solutions through elementary combinatorial techniques. for larger n, we propose a novel ""expander construction"" that generates an approximate solution to the ld-ttp. for n congruent to 4 modulo 6, we show that our expander construction produces a feasible double round-robin tournament schedule whose total distance is guaranteed to be no worse than 4/3 times the optimal solution, regardless of where the n teams are located. this 4/3-approximation for the ld-ttp is stronger than the currently best-known ratio of 5/3 + epsilon for the general ttp. we conclude the paper by applying this linear distance relaxation to general (non-linear) n-team ttp instances, where we develop fast approximate solutions by simply ""assuming"" the n teams lie on a straight line and solving the modified problem. we show that this technique surprisingly generates the distance-optimal tournament on all benchmark sets on 6 teams, as well as close-to-optimal schedules for larger n, even when the teams are located around a circle or positioned in three-dimensional space.",2012
removing redundant messages in n-ary bnb-adopt,"this note considers how to modify bnb-adopt, a well-known algorithm for optimally solving distributed constraint optimization problems, with a double aim: (i) to avoid sending most of the redundant messages and (ii) to handle cost functions of any arity. some of the messages exchanged by bnb-adopt turned out to be redundant. removing most of the redundant messages increases substantially communication efficiency: the number of exchanged messages is - in most cases - at least three times fewer (keeping the other measures almost unchanged), and termination and optimality are maintained. on the other hand, handling n-ary cost functions was addressed in the original work, but the presence of thresholds makes their practical usage more complex. both issues - removing most of the redundant messages and efficiently handling n-ary cost functions - can be combined, producing the new version bnb-adopt+. experimentally, we show the benefits of this version over the original one.",2012
a tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing,"dual decomposition, and more generally lagrangian relaxation, is a classical method for combinatorial optimization; it has recently been applied to several inference problems in natural language processing (nlp). this tutorial gives an overview of the technique. we describe example algorithms, describe formal guarantees for the method, and describe practical issues in implementing the algorithms. while our examples are predominantly drawn from the nlp literature, the material should be of general relevance to inference problems in machine learning. a central theme of this tutorial is that lagrangian relaxation is naturally applied in conjunction with a broad class of combinatorial algorithms, allowing inference in models that go significantly beyond previous work on lagrangian relaxation for inference in graphical models.",2012
transforming graph data for statistical relational learning,"relational data representations have become an increasingly important topic due to the recent proliferation of network datasets (e.g., social, biological, information networks) and a corresponding increase in the application of statistical relational learning (srl) algorithms to these domains. in this article, we examine and categorize techniques for transforming graph-based relational data to improve srl algorithms. in particular, appropriate transformations of the nodes, links, and/or features of the data can dramatically affect the capabilities and results of srl algorithms. we introduce an intuitive taxonomy for data representation transformations in relational domains that incorporates link transformation and node transformation as symmetric representation tasks. more specifically, the transformation tasks for both nodes and links include (i) predicting their existence, (ii) predicting their label or type, (iii) estimating their weight or importance, and (iv) system- atically constructing their relevant features. we motivate our taxonomy through detailed examples and use it to survey competing approaches for each of these tasks. we also dis- cuss general conditions for transforming links, nodes, and features. finally, we highlight challenges that remain to be addressed.",2012
a new look at bdds for pseudo-boolean constraints,"pseudo-boolean constraints are omnipresent in practical applications, and thus a significant effort has been devoted to the development of good sat encoding techniques for them. some of these encodings first construct a binary decision diagram (bdd) for the constraint, and then encode the bdd into a propositional formula. these bdd-based approaches have some important advantages, such as not being dependent on the size of the coefficients, or being able to share the same bdd for representing many constraints. we first focus on the size of the resulting bdds, which was considered to be an open problem in our research community. we report on previous work where it was proved that there are pseudo-boolean constraints for which no polynomial bdd exists. we also give an alternative and simpler proof assuming that np is different from co-np. more interestingly, here we also show how to overcome the possible exponential blowup of bdds by \emph{coefficient decomposition}. this allows us to give the first polynomial generalized arc-consistent robdd-based encoding for pseudo-boolean constraints. finally, we focus on practical issues: we show how to efficiently construct such robdds, how to encode them into sat with only 2 clauses per node, and present experimental results that confirm that our approach is competitive with other encodings and state-of-the-art pseudo-boolean solvers.",2012
complexity of judgment aggregation,"we analyse the computational complexity of three problems in judgment aggregation: (1) computing a collective judgment from a profile of individual judgments (the winner determination problem); (2) deciding whether a given agent can influence the outcome of a judgment aggregation procedure in her favour by reporting insincere judgments (the strategic manipulation problem); and (3) deciding whether a given judgment aggregation scenario is guaranteed to result in a logically consistent outcome, independently from what the judgments supplied by the individuals are (the problem of the safety of the agenda). we provide results both for specific aggregation procedures (the quota rules, the premise-based procedure, and a distance-based procedure) and for classes of aggregation procedures characterised in terms of fundamental axioms.",2012
safe exploration of state and action spaces in reinforcement learning,"in this paper, we consider the important problem of safe exploration in reinforcement learning. while reinforcement learning is well-suited to domains with complex transition dynamics and high-dimensional state-action spaces, an additional challenge is posed by the need for safe and efficient exploration. traditional exploration techniques are not particularly useful for solving dangerous tasks, where the trial and error process may lead to the selection of actions whose execution in some states may result in damage to the learning system (or any other system). consequently, when an agent begins an interaction with a dangerous and high-dimensional state-action space, an important question arises; namely, that of how to avoid (or at least minimize) damage caused by the exploration of the state-action space. we introduce the pi-srl algorithm which safely improves suboptimal albeit robust behaviors for continuous state and action control tasks and which efficiently learns from the experience gained from the environment. we evaluate the proposed method in four complex tasks: automatic car parking, pole-balancing, helicopter hovering, and business management.",2012
replanning in domains with partial information and sensing actions,"replanning via determinization is a recent, popular approach for online planning in mdps. in this paper we adapt this idea to classical, non-stochastic domains with partial information and sensing actions, presenting a new planner: sdr (sample, determinize, replan). at each step we generate a solution plan to a classical planning problem induced by the original problem. we execute this plan as long as it is safe to do so. when this is no longer the case, we replan. the classical planning problem we generate is based on the translation-based approach for conformant planning introduced by palacios and geffner. the state of the classical planning problem generated in this approach captures the belief state of the agent in the original problem. unfortunately, when this method is applied to planning problems with sensing, it yields a non-deterministic planning problem that is typically very large. our main contribution is the introduction of state sampling techniques for overcoming these two problems. in addition, we introduce a novel, lazy, regression-based method for querying the agent's belief state during run-time. we provide a comprehensive experimental evaluation of the planner, showing that it scales better than the state-of-the-art clg planner on existing benchmark problems, but also highlighting its weaknesses with new domains. we also discuss its theoretical guarantees.",2012
irrelevant and independent natural extension for sets of desirable gambles,"the results in this paper add useful tools to the theory of sets of desirable gambles, a growing toolbox for reasoning with partial probability assessments. we investigate how to combine a number of marginal coherent sets of desirable gambles into a joint set using the properties of epistemic irrelevance and independence. we provide formulas for the smallest such joint, called their independent natural extension, and study its main properties. the independent natural extension of maximal coherent sets of desirable gambles allows us to define the strong product of sets of desirable gambles. finally, we explore an easy way to generalise these results to also apply for the conditional versions of epistemic irrelevance and independence. having such a set of tools that are easily implemented in computer programs is clearly beneficial to fields, like ai, with a clear interest in coherent reasoning under uncertainty using general and robust uncertainty models that require no full specification.",2012
learning to predict from textual data,"given a current news event, we tackle the problem of generating plausible predictions of future events it might cause. we present a new methodology for modeling and predicting such future news events using machine learning and data mining techniques. our pundit algorithm generalizes examples of causality pairs to infer a causality predictor. to obtain precisely labeled causality examples, we mine 150 years of news articles and apply semantic natural language modeling techniques to headlines containing certain predefined causality patterns. for generalization, the model uses a vast number of world knowledge ontologies. empirical evaluation on real news articles shows that our pundit algorithm performs as well as non-expert humans.",2012
the time complexity of a* with approximate heuristics on multiple-solution search spaces,we study the behavior of the a* search algorithm when coupled with a heuristic h satisfying (1-epsilon1)h*,2012
tractable set constraints,"many fundamental problems in artificial intelligence, knowledge representation, and verification involve reasoning about sets and relations between sets and can be modeled as set constraint satisfaction problems (set csps). such problems are frequently intractable, but there are several important set csps that are known to be polynomial-time tractable. we introduce a large class of set csps that can be solved in quadratic time. our class, which we call ei, contains all previously known tractable set csps, but also some new ones that are of crucial importance for example in description logics. the class of ei set constraints has an elegant universal-algebraic characterization, which we use to show that every set constraint language that properly contains all ei set constraints already has a finite sublanguage with an np-hard constraint satisfaction problem.",2012
evaluating indirect strategies for chinese-spanish statistical machine translation,"although, chinese and spanish are two of the most spoken languages in the world, not much research has been done in machine translation for this language pair. this paper focuses on investigating the state-of-the-art of chinese-to-spanish statistical machine translation (smt), which nowadays is one of the most popular approaches to machine translation. for this purpose, we report details of the available parallel corpus which are basic traveller expressions corpus (btec), holy bible and united nations (un). additionally, we conduct experimental work with the largest of these three corpora to explore alternative smt strategies by means of using a pivot language. three alternatives are considered for pivoting: cascading, pseudo-corpus and triangulation. as pivot language, we use either english, arabic or french. results show that, for a phrase-based smt system, english is the best pivot language between chinese and spanish. we propose a system output combination using the pivot strategies which is capable of outperforming the direct translation strategy. the main objective of this work is motivating and involving the research community to work in this important pair of languages given their demographic impact.",2012
colin: planning with continuous linear numeric change,"in this paper we describe colin, a forward-chaining heuristic search planner, capable of reasoning with continuous linear numeric change, in addition to the full temporal semantics of pddl. through this work we make two advances to the state-of-the-art in terms of expressive reasoning capabilities of planners: the handling of continuous linear change, and the handling of duration-dependent effects in combination with duration inequalities, both of which require tightly coupled temporal and numeric reasoning during planning. colin combines ff-style forward chaining search, with the use of a linear program (lp) to check the consistency of the interacting temporal and numeric constraints at each state. the lp is used to compute bounds on the values of variables in each state, reducing the range of actions that need to be considered for application. in addition, we develop an extension of the temporal relaxed planning graph heuristic of crikey3, to support reasoning directly with continuous change. we extend the range of task variables considered to be suitable candidates for specifying the gradient of the continuous numeric change effected by an action. finally, we explore the potential for employing mixed integer programming as a tool for optimising the timestamps of the actions in the plan, once a solution has been found. to support this, we further contribute a selection of extended benchmark domains that include continuous numeric effects. we present results for colin that demonstrate its scalability on a range of benchmarks, and compare to existing state-of-the-art planners.",2012
solving limited memory influence diagrams,"we present a new algorithm for exactly solving decision making problems represented as influence diagrams. we do not require the usual assumptions of no forgetting and regularity; this allows us to solve problems with simultaneous decisions and limited information. the algorithm is empirically shown to outperform a state-of-the-art algorithm on randomly generated problems of up to 150 variables and 10^64 solutions. we show that these problems are np-hard even if the underlying graph structure of the problem has low treewidth and the variables take on a bounded number of states, and that they admit no provably good approximation if variables can take on an arbitrary number of states.",2012
algorithms and limits for compact plan representations,"compact representations of objects is a common concept in computer science. automated planning can be viewed as a case of this concept: a planning instance is a compact implicit representation of a graph and the problem is to find a path (a plan) in this graph. while the graphs themselves are represented compactly as planning instances, the paths are usually represented explicitly as sequences of actions. some cases are known where the plans always have compact representations, for example, using macros. we show that these results do not extend to the general case, by proving a number of bounds for compact representations of plans under various criteria, like efficient sequential or random access of actions. in addition to this, we show that our results have consequences for what can be gained from reformulating planning into some other problem. as a contrast to this we also prove a number of positive results, demonstrating restricted cases where plans do have useful compact representations, as well as proving that macro plans have favourable access properties. our results are finally discussed in relation to other relevant contexts.",2012
improving statistical machine translation for a resource-poor language using related resource-rich languages,"we propose a novel language-independent approach for improving machine translation for resource-poor languages by exploiting their similarity to resource-rich ones. more precisely, we improve the translation from a resource-poor source language x_1 into a resource-rich language y given a bi-text containing a limited number of parallel sentences for x_1-y and a larger bi-text for x_2-y for some resource-rich language x_2 that is closely related to x_1. this is achieved by taking advantage of the opportunities that vocabulary overlap and similarities between the languages x_1 and x_2 in spelling, word order, and syntax offer: (1) we improve the word alignments for the resource-poor language, (2) we further augment it with additional translation options, and (3) we take care of potential spelling differences through appropriate transliteration. the evaluation for indonesian- >english using malay and for spanish -> english using portuguese and pretending spanish is resource-poor shows an absolute gain of up to 1.35 and 3.37 bleu points, respectively, which is an improvement over the best rivaling approaches, while using much less additional data. overall, our method cuts the amount of necessary ""real'' training data by a factor of 2--5.",2012
modeling social causality and responsibility judgment in multi-agent interactions,"social causality is the inference an entity makes about the social behavior of other entities and self. besides physical cause and effect, social causality involves reasoning about epistemic states of agents and coercive circumstances. based on such inference, responsibility judgment is the process whereby one singles out individuals to assign responsibility, credit or blame for multi-agent activities. social causality and responsibility judgment are a key aspect of social intelligence, and a model for them facilitates the design and development of a variety of multi-agent interactive systems. based on psychological attribution theory, this paper presents a domain-independent computational model to automate social inference and judgment process according to an agents causal knowledge and observations of interaction. we conduct experimental studies to empirically validate the computational model. the experimental results show that our model predicts human judgments of social attributions and makes inferences consistent with what most people do in their judgments. therefore, the proposed model can be generically incorporated into an intelligent system to augment its social and cognitive functionality.",2012
algorithms for generating ordered solutions for explicit and/or structures,"we present algorithms for generating alternative solutions for explicit acyclic and/or structures in non-decreasing order of cost. the proposed algorithms use a best first search technique and report the solutions using an implicit representation ordered by cost. in this paper, we present two versions of the search algorithm -- (a) an initial version of the best first search algorithm, asg, which may present one solution more than once while generating the ordered solutions, and (b) another version, lasg, which avoids the construction of the duplicate solutions. the actual solutions can be reconstructed quickly from the implicit compact representation used. we have applied the methods on a few test domains, some of them are synthetic while the others are based on well known problems including the search space of the 5-peg tower of hanoi problem, the matrix-chain multiplication problem and the problem of finding secondary structure of rna. experimental results show the efficacy of the proposed algorithms over the existing approach. our proposed algorithms have potential use in various domains ranging from knowledge based frameworks to service composition, where the and/or structure is widely used for representing problems.",2012
plan-based policies for efficient multiple battery load management,"efficient use of multiple batteries is a practical problem with wide and growing application. the problem can be cast as a planning problem under uncertainty. we describe the approach we have adopted to modelling and solving this problem, seen as a markov decision problem, building effective policies for battery switching in the face of stochastic load profiles. our solution exploits and adapts several existing techniques: planning for deterministic mixed discrete-continuous problems and monte carlo sampling for policy learning. the paper describes the development of planning techniques to allow solution of the non-linear continuous dynamic models capturing the battery behaviours. this approach depends on carefully handled discretisation of the temporal dimension. the construction of policies is performed using a classification approach and this idea offers opportunities for wider exploitation in other problems. the approach and its generality are described in the paper. application of the approach leads to construction of policies that, in simulation, significantly outperform those that are currently in use and the best published solutions to the battery management problem. we achieve solutions that achieve more than 99% efficiency in simulation compared with the theoretical limit and do so with far fewer battery switches than existing policies. behaviour of physical batteries does not exactly match the simulated models for many reasons, so to confirm that our theoretical results can lead to real measured improvements in performance we also conduct and report experiments using a physical test system. these results demonstrate that we can obtain 5%-15% improvement in lifetimes in the case of a two battery system.",2012
narrative planning: compilations to classical planning,"a model of story generation recently proposed by riedl and young casts it as planning, with the additional condition that story characters behave intentionally. this means that characters have perceivable motivation for the actions they take. i show that this condition can be compiled away (in more ways than one) to produce a classical planning problem that can be solved by an off-the-shelf classical planner, more efficiently than by riedl and young's specialised planner.",2012
semantic similarity measures applied to an ontology for human-like interaction,"the focus of this paper is the calculation of similarity between two concepts from an ontology for a human-like interaction system. in order to facilitate this calculation, a similarity function is proposed based on five dimensions (sort, compositional, essential, restrictive and descriptive) constituting the structure of ontological knowledge. the paper includes a proposal for computing a similarity function for each dimension of knowledge. later on, the similarity values obtained are weighted and aggregated to obtain a global similarity measure. in order to calculate those weights associated to each dimension, four training methods have been proposed. the training methods differ in the element to fit: the user, concepts or pairs of concepts, and a hybrid approach. for evaluating the proposal, the knowledge base was fed from wordnet and extended by using a knowledge editing toolkit (cognos). the evaluation of the proposal is carried out through the comparison of system responses with those given by human test subjects, both providing a measure of the soundness of the procedure and revealing ways in which the proposal may be improved.",2012
modelling observation correlations for active exploration and robust object detection,"today, mobile robots are expected to carry out increasingly complex tasks in multifarious, real-world environments. often, the tasks require a certain semantic understanding of the workspace. consider, for example, spoken instructions from a human collaborator referring to objects of interest; the robot must be able to accurately detect these objects to correctly understand the instructions. however, existing object detection, while competent, is not perfect. in particular, the performance of detection algorithms is commonly sensitive to the position of the sensor relative to the objects in the scene. this paper presents an online planning algorithm which learns an explicit model of the spatial dependence of object detection and generates plans which maximize the expected performance of the detection, and by extension the overall plan performance. crucially, the learned sensor model incorporates spatial correlations between measurements, capturing the fact that successive measurements taken at the same or nearby locations are not independent. we show how this sensor model can be incorporated into an efficient forward search algorithm in the information space of detected objects, allowing the robot to generate motion plans efficiently. we investigate the performance of our approach by addressing the tasks of door and text detection in indoor environments and demonstrate significant improvement in detection performance during task execution over alternative methods in simulated and real robot experiments.",2012
tractable triangles and cross-free convexity in discrete optimisation,"the minimisation problem of a sum of unary and pairwise functions of discrete variables is a general np-hard problem with wide applications such as computing map configurations in markov random fields (mrf), minimising gibbs energy, or solving binary valued constraint satisfaction problems (vcsps). we study the computational complexity of classes of discrete optimisation problems given by allowing only certain types of costs in every triangle of variable-value assignments to three distinct variables. we show that for several computational problems, the only non- trivial tractable classes are the well known maximum matching problem and the recently discovered joint-winner property. our results, apart from giving complete classifications in the studied cases, provide guidance in the search for hybrid tractable classes; that is, classes of problems that are not captured by restrictions on the functions (such as submodularity) or the structure of the problem graph (such as bounded treewidth). furthermore, we introduce a class of problems with convex cardinality functions on cross-free sets of assignments. we prove that while imposing only one of the two conditions renders the problem np-hard, the conjunction of the two gives rise to a novel tractable class satisfying the cross-free convexity property, which generalises the joint-winner property to problems of unbounded arity.",2012
riffled independence for efficient inference with partial rankings,"distributions over rankings are used to model data in a multitude of real world settings such as preference analysis and political elections. modeling such distributions presents several computational challenges, however, due to the factorial size of the set of rankings over an item set. some of these challenges are quite familiar to the artificial intelligence community, such as how to compactly represent a distribution over a combinatorially large space, and how to efficiently perform probabilistic inference with these representations. with respect to ranking, however, there is the additional challenge of what we refer to as human task complexity  users are rarely willing to provide a full ranking over a long list of candidates, instead often preferring to provide partial ranking information. simultaneously addressing all of these challenges  i.e., designing a compactly representable model which is amenable to efficient inference and can be learned using partial ranking data  is a difficult task, but is necessary if we would like to scale to problems with nontrivial size. in this paper, we show that the recently proposed riffled independence assumptions cleanly and efficiently address each of the above challenges. in particular, we establish a tight mathematical connection between the concepts of riffled independence and of partial rankings. this correspondence not only allows us to then develop efficient and exact algorithms for performing inference tasks using riffled independence based represen- tations with partial rankings, but somewhat surprisingly, also shows that efficient inference is not possible for riffle independent models (in a certain sense) with observations which do not take the form of partial rankings. finally, using our inference algorithm, we introduce the first method for learning riffled independence based models from partially ranked data.",2012
domain and function: a dual-space model of semantic relations and compositions,"given appropriate representations of the semantic relations between carpenter and wood and between mason and stone (for example, vectors in a vector space model), a suitable algorithm should be able to recognize that these relations are highly similar (carpenter is to wood as mason is to stone; the relations are analogous). likewise, with representations of dog, house, and kennel, an algorithm should be able to recognize that the semantic composition of dog and house, dog house, is highly similar to kennel (dog house and kennel are synonymous). it seems that these two tasks, recognizing relations and compositions, are closely connected. however, up to now, the best models for relations are significantly different from the best models for compositions. in this paper, we introduce a dual-space model that unifies these two tasks. this model matches the performance of the best previous models for relations and compositions. the dual-space model consists of a space for measuring domain similarity and a space for measuring function similarity. carpenter and wood share the same domain, the domain of carpentry. mason and stone share the same domain, the domain of masonry. carpenter and mason share the same function, the function of artisans. wood and stone share the same function, the function of materials. in the composition dog house, kennel has some domain overlap with both dog and house (the domains of pets and buildings). the function of kennel is similar to the function of house (the function of shelters). by combining domain and function similarities in various ways, we can model relations, compositions, and other aspects of semantics.",2012
sap speaks pddl: exploiting a software-engineering model for planning in business process management,"planning is concerned with the automated solution of action sequencing problems described in declarative languages giving the action preconditions and effects. one important application area for such technology is the creation of new processes in business process management (bpm), which is essential in an ever more dynamic business environment. a major obstacle for the application of planning in this area lies in the modeling. obtaining a suitable model to plan with -- ideally a description in pddl, the most commonly used planning language -- is often prohibitively complicated and/or costly. our core observation in this work is that this problem can be ameliorated by leveraging synergies with model-based software development. our application at sap, one of the leading vendors of enterprise software, demonstrates that even one-to-one model re-use is possible. the model in question is called status and action management (sam). it describes the behavior of business objects (bo), i.e., large-scale data structures, at a level of abstraction corresponding to the language of business experts. sam covers more than 400 kinds of bos, each of which is described in terms of a set of status variables and how their values are required for, and affected by, processing steps (actions) that are atomic from a business perspective. sam was developed by sap as part of a major model-based software engineering effort. we show herein that one can use this same model for planning, thus obtaining a bpm planning application that incurs no modeling overhead at all. we compile sam into a variant of pddl, and adapt an off-the-shelf planner to solve this kind of problem. thanks to the resulting technology, business experts may create new processes simply by specifying the desired behavior in terms of status variable value changes: effectively, by describing the process in their own language.",2012
the logical difference for the lightweight description logic el,"we study a logic-based approach to versioning of ontologies. under this view, ontologies provide answers to queries about some vocabulary of interest. the difference between two versions of an ontology is given by the set of queries that receive different answers. we investigate this approach for terminologies given in the description logic el extended with role inclusions and domain and range restrictions for three distinct types of queries: subsumption, instance, and conjunctive queries. in all three cases, we present polynomial-time algorithms that decide whether two terminologies give the same answers to queries over a given vocabulary and compute a succinct representation of the difference if it is non- empty. we present an implementation, cex2, of the developed algorithms for subsumption and instance queries and apply it to distinct versions of snomed ct and the nci ontology.",2012
online speedup learning for optimal planning,"domain-independent planning is one of the foundational areas in the field of artificial intelligence. a description of a planning task consists of an initial world state, a goal, and a set of actions for modifying the world state. the objective is to find a sequence of actions, that is, a plan, that transforms the initial world state into a goal state. in optimal planning, we are interested in finding not just a plan, but one of the cheapest plans. a prominent approach to optimal planning these days is heuristic state-space search, guided by admissible heuristic functions. numerous admissible heuristics have been developed, each with its own strengths and weaknesses, and it is well known that there is no single ""best'' heuristic for optimal planning in general. thus, which heuristic to choose for a given planning task is a difficult question. this difficulty can be avoided by combining several heuristics, but that requires computing numerous heuristic estimates at each state, and the tradeoff between the time spent doing so and the time saved by the combined advantages of the different heuristics might be high. we present a novel method that reduces the cost of combining admissible heuristics for optimal planning, while maintaining its benefits. using an idealized search space model, we formulate a decision rule for choosing the best heuristic to compute at each state. we then present an active online learning approach for learning a classifier with that decision rule as the target concept, and employ the learned classifier to decide which heuristic to compute at each state. we evaluate this technique empirically, and show that it substantially outperforms the standard method for combining several heuristics via their pointwise maximum.",2012
learning and reasoning with action-related places for robust mobile manipulation,"we propose the concept of action-related place (arplace) as a powerful and flexible representation of task-related place in the context of mobile manipulation. arplace represents robot base locations not as a single position, but rather as a collection of positions, each with an associated probability that the manipulation action will succeed when located there. arplaces are generated using a predictive model that is acquired through experience-based learning, and take into account the uncertainty the robot has about its own location and the location of the object to be manipulated. when executing the task, rather than choosing one specific goal position based only on the initial knowledge about the task context, the robot instantiates an arplace, and bases its decisions on this arplace, which is updated as new information about the task becomes available. to show the advantages of this least-commitment approach, we present a transformational planner that reasons about arplaces in order to optimize symbolic plans. our empirical evaluation demonstrates that using arplaces leads to more robust and efficient mobile manipulation in the face of state estimation uncertainty on our simulated robot.",2012
robust local search for solving rcpsp/max with durational uncertainty,"scheduling problems in manufacturing, logistics and project management have frequently been modeled using the framework of resource constrained project scheduling problems with minimum and maximum time lags (rcpsp/max). due to the importance of these problems, providing scalable solution schedules for rcpsp/max problems is a topic of extensive research. however, all existing methods for solving rcpsp/max assume that durations of activities are known with certainty, an assumption that does not hold in real world scheduling problems where unexpected external events such as manpower availability, weather changes, etc. lead to delays or advances in completion of activities. thus, in this paper, our focus is on providing a scalable method for solving rcpsp/max problems with durational uncertainty. to that end, we introduce the robust local search method consisting of three key ideas: (a) introducing and studying the properties of two decision rule approximations used to compute start times of activities with respect to dynamic realizations of the durational uncertainty; (b) deriving the expression for robust makespan of an execution strategy based on decision rule approximations; and (c) a robust local search mechanism to efficiently compute activity execution strategies that are robust against durational uncertainty. furthermore, we also provide enhancements to local search that exploit temporal dependencies between activities. our experimental results illustrate that robust local search is able to provide robust execution strategies efficiently.",2012
location-based reasoning about complex multi-agent behavior,"recent research has shown that surprisingly rich models of human activity can be learned from gps (positional) data. however, most effort to date has concentrated on modeling single individuals or statistical properties of groups of people. moreover, prior work focused solely on modeling actual successful executions (and not failed or attempted executions) of the activities of interest. we, in contrast, take on the task of understanding human interactions, attempted interactions, and intentions from noisy sensor data in a fully relational multi-agent setting. we use a real-world game of capture the flag to illustrate our approach in a well-defined domain that involves many distinct cooperative and competitive joint activities. we model the domain using markov logic, a statistical-relational language, and learn a theory that jointly denoises the data and infers occurrences of high-level activities, such as a player capturing an enemy. our unified model combines constraints imposed by the geometry of the game area, the motion model of the players, and by the rules and dynamics of the game in a probabilistically and logically sound fashion. we show that while it may be impossible to directly detect a multi-agent activity due to sensor noise or malfunction, the occurrence of the activity can still be inferred by considering both its impact on the future behaviors of the people involved as well as the events that could have preceded it. further, we show that given a model of successfully performed multi-agent activities, along with a set of examples of failed attempts at the same activities, our system automatically learns an augmented model that is capable of recognizing success and failure, as well as goals of people's actions with high accuracy. we compare our approach with other alternatives and show that our unified model, which takes into account not only relationships among individual players, but also relationships among activities over the entire length of a game, although more computationally costly, is significantly more accurate. finally, we demonstrate that explicitly modeling unsuccessful attempts boosts performance on other important recognition tasks.",2012
the cqc algorithm: cycling in graphs to semantically enrich and enhance a bilingual dictionary,"bilingual machine-readable dictionaries are knowledge resources useful in many automatic tasks. however, compared to monolingual computational lexicons like wordnet, bilingual dictionaries typically provide a lower amount of structured information, such as lexical and semantic relations, and often do not cover the entire range of possible translations for a word of interest. in this paper we present cycles and quasi-cycles (cqc), a novel algorithm for the automated disambiguation of ambiguous translations in the lexical entries of a bilingual machine-readable dictionary. the dictionary is represented as a graph, and cyclic patterns are sought in the graph to assign an appropriate sense tag to each translation in a lexical entry. further, we use the algorithm's output to improve the quality of the dictionary itself, by suggesting accurate solutions to structural problems such as misalignments, partial alignments and missing entries. finally, we successfully apply cqc to the task of synonym extraction.",2012
counting-based search: branching heuristics for constraint satisfaction problems,"designing a search heuristic for constraint programming that is reliable across problem domains has been an important research topic in recent years. this paper concentrates on one family of candidates: counting-based search. such heuristics seek to make branching decisions that preserve most of the solutions by determining what proportion of solutions to each individual constraint agree with that decision. whereas most generic search heuristics in constraint programming rely on local information at the level of the individual variable, our search heuristics are based on more global information at the constraint level. we design several algorithms that are used to count the number of solutions to specific families of constraints and propose some search heuristics exploiting such information. the experimental part of the paper considers eight problem domains ranging from well-established benchmark puzzles to rostering and sport scheduling. an initial empirical analysis identifies heuristic maxsd as a robust candidate among our proposals.ewe then evaluate the latter against the state of the art, including the latest generic search heuristics, restarts, and discrepancy-based tree traversals. experimental results show that counting-based search generally outperforms other generic heuristics.",2012
exploiting model equivalences for solving interactive dynamic influence diagrams,"we focus on the problem of sequential decision making in partially observable environments shared with other agents of uncertain types having similar or conflicting objectives. this problem has been previously formalized by multiple frameworks one of which is the interactive dynamic influence diagram (i-did), which generalizes the well-known influence diagram to the multiagent setting. i-dids are graphical models and may be used to compute the policy of an agent given its belief over the physical state and others' models, which changes as the agent acts and observes in the multiagent setting. as we may expect, solving i-dids is computationally hard. this is predominantly due to the large space of candidate models ascribed to the other agents and its exponential growth over time. we present two methods for reducing the size of the model space and stemming its exponential growth. both these methods involve aggregating individual models into equivalence classes. our first method groups together behaviorally equivalent models and selects only those models for updating which will result in predictive behaviors that are distinct from others in the updated model space. the second method further compacts the model space by focusing on portions of the behavioral predictions. specifically, we cluster actionally equivalent models that prescribe identical actions at a single time step. exactly identifying the equivalences would require us to solve all models in the initial set. we avoid this by selectively solving some of the models, thereby introducing an approximation. we discuss the error introduced by the approximation, and empirically demonstrate the improved efficiency in solving i-dids due to the equivalences.",2012
consistency techniques for flow-based projection-safe global cost functions in weighted constraint satisfaction,"many combinatorial problems deal with preferences and violations, the goal of which is to find solutions with the minimum cost. weighted constraint satisfaction is a framework for modeling such problems, which consists of a set of cost functions to measure the degree of violation or preferences of different combinations of variable assignments. typical solution methods for weighted constraint satisfaction problems (wcsps) are based on branch-and-bound search, which are made practical through the use of powerful consistency techniques such as ac*, fdac*, edac* to deduce hidden cost information and value pruning during search. these techniques, however, are designed to be efficient only on binary and ternary cost functions which are represented in table form. in tackling many real-life problems, high arity (or global) cost functions are required. we investigate efficient representation scheme and algorithms to bring the benefits of the consistency techniques to also high arity cost functions, which are often derived from hard global constraints from classical constraint satisfaction. the literature suggests some global cost functions can be represented as flow networks, and the minimum cost flow algorithm can be used to compute the minimum costs of such networks in polynomial time. we show that naive adoption of this flow-based algorithmic method for global cost functions can result in a stronger form of null-inverse consistency. we further show how the method can be modified to handle cost projections and extensions to maintain generalized versions of ac* and fdac* for cost functions with more than two variables. similar generalization for the stronger edac* is less straightforward. we reveal the oscillation problem when enforcing edac* on cost functions sharing more than one variable. to avoid oscillation, we propose a weak version of edac* and generalize it to weak edgac* for non-binary cost functions. using various benchmarks involving the soft variants of hard global constraints alldifferent, gcc, same, and regular, empirical results demonstrate that our proposal gives improvements of up to an order of magnitude when compared with the traditional constraint optimization approach, both in terms of time and pruning.",2012
sas+ planning as satisfiability,"planning as satisfiability is a principal approach to planning with many eminent advantages. the existing planning as satisfiability techniques usually use encodings compiled from strips. we introduce a novel sat encoding scheme (sase) based on the sas+ formalism. the new scheme exploits the structural information in sas+, resulting in an encoding that is both more compact and efficient for planning. we prove the correctness of the new encoding by establishing an isomorphism between the solution plans of sase and that of strips based encodings. we further analyze the transition variables newly introduced in sase to explain why it accommodates modern sat solving algorithms and improves performance. we give empirical statistical results to support our analysis. we also develop a number of techniques to further reduce the encoding size of sase, and conduct experimental studies to show the strength of each individual technique. finally, we report extensive experimental results to demonstrate significant improvements of sase over the state-of-the-art strips based encoding schemes in terms of both time and memory efficiency.",2012
local consistency and sat-solvers,"local consistency techniques such as k-consistency are a key component of specialised solvers for constraint satisfaction problems. in this paper we show that the power of using k-consistency techniques on a constraint satisfaction problem is precisely captured by using a particular inference rule, which we call negative-hyper-resolution, on the standard direct encoding of the problem into boolean clauses. we also show that current clause-learning sat-solvers will discover in expected polynomial time any inconsistency that can be deduced from a given set of clauses using negative-hyper-resolvents of a fixed size. we combine these two results to show that, without being explicitly designed to do so, current clause-learning sat-solvers efficiently simulate k-consistency techniques, for all fixed values of k. we then give some experimental results to show that this feature allows clause-learning sat-solvers to efficiently solve certain families of constraint problems which are challenging for conventional constraint-programming solvers.",2012
computing all-pairs shortest paths by leveraging low treewidth,"we present two new and efficient algorithms for computing all-pairs shortest paths. the algorithms operate on directed graphs with real (possibly negative) weights. they make use of directed path consistency along a vertex ordering d. both algorithms run in o(n^2 w_d) time, where w_d is the graph width induced by this vertex ordering. for graphs of constant treewidth, this yields o(n^2) time, which is optimal. on chordal graphs, the algorithms run in o(nm) time. in addition, we present a variant that exploits graph separators to arrive at a run time of o(n w_d^2 + n^2 s_d) on general graphs, where s_d",2012
generalized biwords for bitext compression and translation spotting,"large bilingual parallel texts (also known as bitexts) are usually stored in a compressed form, and previous work has shown that they can be more efficiently compressed if the fact that the two texts are mutual translations is exploited. for example, a bitext can be seen as a sequence of biwords ---pairs of parallel words with a high probability of co-occurrence--- that can be used as an intermediate representation in the compression process. however, the simple biword approach described in the literature can only exploit one-to-one word alignments and cannot tackle the reordering of words. we therefore introduce a generalization of biwords which can describe multi-word expressions and reorderings. we also describe some methods for the binary compression of generalized biword sequences, and compare their performance when different schemes are applied to the extraction of the biword sequence. in addition, we show that this generalization of biwords allows for the implementation of an efficient algorithm to look on the compressed bitext for words or text segments in one of the texts and retrieve their counterpart translations in the other text ---an application usually referred to as translation spotting--- with only some minor modifications in the compression algorithm.",2012
completeness guarantees for incomplete ontology reasoners: theory and practice,"to achieve scalability of query answering, the developers of semantic web applications are often forced to use incomplete owl 2 reasoners, which fail to derive all answers for at least one query, ontology, and data set. the lack of completeness guarantees, however, may be unacceptable for applications in areas such as health care and defence, where missing answers can adversely affect the application's functionality. furthermore, even if an application can tolerate some level of incompleteness, it is often advantageous to estimate how many and what kind of answers are being lost. in this paper, we present a novel logic-based framework that allows one to check whether a reasoner is complete for a given query q and ontology t---that is, whether the reasoner is guaranteed to compute all answers to q w.r.t. t and an arbitrary data set a. since ontologies and typical queries are often fixed at application design time, our approach allows application developers to check whether a reasoner known to be incomplete in general is actually complete for the kinds of input relevant for the application. we also present a technique that, given a query q, an ontology t, and reasoners r_1 and r_2 that satisfy certain assumptions, can be used to determine whether, for each data set a, reasoner r_1 computes more answers to q w.r.t. t and a than reasoner r_2. this allows application developers to select the reasoner that provides the highest degree of completeness for q and t that is compatible with the application's scalability requirements. our results thus provide a theoretical and practical foundation for the design of future ontology-based information systems that maximise scalability while minimising or even eliminating incompleteness of query answers.",2012
proximity-based non-uniform abstractions for approximate planning,"in a deterministic world, a planning agent can be certain of the consequences of its planned sequence of actions. not so, however, in dynamic, stochastic domains where markov decision processes are commonly used. unfortunately these suffer from the `curse of dimensionality': if the state space is a cartesian product of many small sets (`dimensions'), planning is exponential in the number of those dimensions. our new technique exploits the intuitive strategy of selectively ignoring various dimensions in different parts of the state space. the resulting non-uniformity has strong implications, since the approximation is no longer markovian, requiring the use of a modified planner. we also use a spatial and temporal proximity measure, which responds to continued planning as well as movement of the agent through the state space, to dynamically adapt the abstraction as planning progresses. we present qualitative and quantitative results across a range of experimental domains showing that an agent exploiting this novel approximation method successfully finds solutions to the planning problem using much less than the full state space. we assess and analyse the features of domains which our method can exploit.",2012
avoiding and escaping depressions in real-time heuristic search,"heuristics used for solving hard real-time search problems have regions with depressions. such regions are bounded areas of the search space in which the heuristic function is inaccurate compared to the actual cost to reach a solution. early real-time search algorithms, like lrta*, easily become trapped in those regions since the heuristic values of their states may need to be updated multiple times, which results in costly solutions. state-of-the-art real-time search algorithms, like lss-lrta* or lrta*(k), improve lrta*'s mechanism to update the heuristic, resulting in improved performance. those algorithms, however, do not guide search towards avoiding depressed regions. this paper presents depression avoidance, a simple real-time search principle to guide search towards avoiding states that have been marked as part of a heuristic depression. we propose two ways in which depression avoidance can be implemented: mark-and-avoid and move-to-border. we implement these strategies on top of lss-lrta* and rtaa*, producing 4 new real-time heuristic search algorithms: alss-lrta*, dalss-lrta*, artaa*, and dartaa*. when the objective is to find a single solution by running the real-time search algorithm once, we show that dalss-lrta* and dartaa* outperform their predecessors sometimes by one order of magnitude. of the four new algorithms, dartaa* produces the best solutions given a fixed deadline on the average time allowed per planning episode. we prove all our algorithms have good theoretical properties: in finite search spaces, they find a solution if one exists, and converge to an optimal after a number of trials.",2012
reformulating the situation calculus and the event calculus in the general theory of stable models and in answer set programming,"circumscription and logic programs under the stable model semantics are two well-known nonmonotonic formalisms. the former has served as a basis of classical logic based action formalisms, such as the situation calculus, the event calculus and temporal action logics; the latter has served as a basis of a family of action languages, such as language a and several of its descendants. based on the discovery that circumscription and the stable model semantics coincide on a class of canonical formulas, we reformulate the situation calculus and the event calculus in the general theory of stable models. we also present a translation that turns the reformulations further into answer set programs, so that efficient answer set solvers can be applied to compute the situation calculus and the event calculus.",2012
a market-inspired approach for intersection management in urban road traffic networks,"traffic congestion in urban road networks is a costly problem that affects all major cities in developed countries. to tackle this problem, it is possible (i) to act on the supply side, increasing the number of roads or lanes in a network, (ii) to reduce the demand, restricting the access to urban areas at specific hours or to specific vehicles, or (iii) to improve the efficiency of the existing network, by means of a widespread use of so-called intelligent transportation systems (its). in line with the recent advances in smart transportation management infrastructures, its has turned out to be a promising field of application for artificial intelligence techniques. in particular, multiagent systems seem to be the ideal candidates for the design and implementation of its. in fact, drivers can be naturally modelled as autonomous agents that interact with the transportation management infrastructure, thereby generating a large-scale, open, agent-based system. to regulate such a system and maintain a smooth and efficient flow of traffic, decentralised mechanisms for the management of the transportation infrastructure are needed. in this article we propose a distributed, market-inspired, mechanism for the management of a future urban road network, where intelligent autonomous vehicles, operated by software agents on behalf of their human owners, interact with the infrastructure in order to travel safely and efficiently through the road network. building on the reservation-based intersection control model proposed by dresner and stone, we consider two different scenarios: one with a single intersection and one with a network of intersections. in the former, we analyse the performance of a novel policy based on combinatorial auctions for the allocation of reservations. in the latter, we analyse the impact that a traffic assignment strategy inspired by competitive markets has on the drivers' route choices. finally we propose an adaptive management mechanism that integrates the auction-based traffic control policy with the competitive traffic assignment strategy.",2012
learning to win by reading manuals in a monte-carlo framework,"domain knowledge is crucial for effective performance in autonomous control systems. typically, human effort is required to encode this knowledge into a control algorithm. in this paper, we present an approach to language grounding which automatically interprets text in the context of a complex control application, such as a game, and uses domain knowledge extracted from the text to improve control performance. both text analysis and control strategies are learned jointly using only a feedback signal inherent to the application. to effectively leverage textual information, our method automatically extracts the text segment most relevant to the current game state, and labels it with a task-centric predicate structure. this labeled text is then used to bias an action selection policy for the game, guiding it towards promising regions of the action space. we encode our model for text analysis and game playing in a multi-layer neural network, representing linguistic decisions via latent variables in the hidden layers, and game action quality via the output layer. operating within the monte-carlo search framework, we estimate model parameters using feedback from simulated games. we apply our approach to the complex strategy game civilization ii using the official game manual as the text guide. our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 34% absolute improvement and winning over 65% of games when playing against the built-in ai of civilization.",2012
where are the hard manipulation problems?,"voting is a simple mechanism to combine together the preferences of multiple agents. unfortunately, agents may try to manipulate the result by mis-reporting their preferences. one barrier that might exist to such manipulation is computational complexity. in particular, it has been shown that it is np-hard to compute how to manipulate a number of different voting rules. how- ever, np-hardness only bounds the worst-case complexity. recent theoretical results suggest that manipulation may often be easy in practice. in this paper, we show that empirical studies are useful in improving our understanding of this issue. we consider two settings which represent the two types of complexity results that have been identified in this area: manipulation with un-weighted votes by a single agent, and manipulation with weighted votes by a coalition of agents. in the first case, we consider single transferable voting (stv), and in the second case, we consider veto voting. stv is one of the few voting rules used in practice where it is np-hard to compute how a single agent can manipulate the result when votes are unweighted. it also appears one of the harder voting rules to manipulate since it involves multiple rounds. on the other hand, veto voting is one of the simplest representatives of voting rules where it is np-hard to compute how a coalition of weighted agents can manipulate the result. in our experiments, we sample a number of distributions of votes including uniform, correlated and real world elections. in many of the elections in our experiments, it was easy to compute how to manipulate the result or to prove that manipulation was impossible. even when we were able to identify a situation in which manipulation was hard to compute (e.g. when votes are highly correlated and the election is hung), we found that the computational difficulty of computing manipulations was somewhat precarious (e.g. with such hung elections, even a single uncorrelated voter was enough to make manipulation easy to compute).",2011
"on the link between partial meet, kernel, and infra contraction and its application to horn logic","standard belief change assumes an underlying logic containing full classical propositional logic. however, there are good reasons for considering belief change in less expressive logics as well. in this paper we build on recent investigations by delgrande on contraction for horn logic. we show that the standard basic form of contraction, partial meet, is too strong in the horn case. this result stands in contrast to delgrandes conjecture that orderly maxichoice is the appropriate form of contraction for horn logic. we then define a more appropriate notion of basic contraction for the horn case, influenced by the convexity property holding for full propositional logic and which we refer to as infra contraction. the main contribution of this work is a result which shows that the construction method for horn contraction for belief sets based on our infra remainder sets corresponds exactly to hanssons classical kernel contraction for belief sets, when restricted to horn logic. this result is obtained via a detour through contraction for belief bases. we prove that kernel contraction for belief bases produces precisely the same results as the belief base version of infra contraction. the use of belief bases to obtain this result provides evidence for the conjecture that horn belief change is best viewed as a 'hybrid' version of belief set change and belief base change. one of the consequences of the link with base contraction is the provision of a representation result for horn contraction for belief sets in which a version of the core-retainment postulate features.",2011
mapp: a scalable multi-agent path planning algorithm with tractability and completeness guarantees,"multi-agent path planning is a challenging problem with numerous real-life applications. running a centralized search such as a* in the combined state space of all units is complete and cost-optimal, but scales poorly, as the state space size is exponential in the number of mobile units. traditional decentralized approaches, such as far and whca*, are faster and more scalable, being based on problem decomposition. however, such methods are incomplete and provide no guarantees with respect to the running time or the solution quality. they are not necessarily able to tell in a reasonable time whether they would succeed in finding a solution to a given instance. we introduce mapp, a tractable algorithm for multi-agent path planning on undirected graphs. we present a basic version and several extensions. they have low-polynomial worst-case upper bounds for the running time, the memory requirements, and the length of solutions. even though all algorithmic versions are incomplete in the general case, each provides formal guarantees on problems it can solve. for each version, we discuss the algorithm's completeness with respect to clearly defined subclasses of instances. experiments were run on realistic game grid maps. mapp solved 99.86% of all mobile units, which is 18--22% better than the percentage of far and whca*. mapp marked 98.82% of all units as provably solvable during the first stage of plan computation. parts of mapp's computation can be re-used across instances on the same map. speed-wise, mapp is competitive or significantly faster than whca*, depending on whether mapp performs all computations from scratch. when data that mapp can re-use are preprocessed offline and readily available, mapp is slower than the very fast far algorithm by a factor of 2.18 on average. mapp's solutions are on average 20% longer than far's solutions and 7--31% longer than whca*'s solutions.",2011
scheduling bipartite tournaments to minimize total travel distance,"in many professional sports leagues, teams from opposing leagues/conferences compete against one another, playing inter-league games. this is an example of a bipartite tournament. in this paper, we consider the problem of reducing the total travel distance of bipartite tournaments, by analyzing inter-league scheduling from the perspective of discrete optimization. this research has natural applications to sports scheduling, especially for leagues such as the national basketball association (nba) where teams must travel long distances across north america to play all their games, thus consuming much time, money, and greenhouse gas emissions. we introduce the bipartite traveling tournament problem (bttp), the inter-league variant of the well-studied traveling tournament problem. we prove that the 2n-team bttp is np-complete, but for small values of n, a distance-optimal inter-league schedule can be generated from an algorithm based on minimum-weight 4-cycle-covers. we apply our theoretical results to the 12-team nippon professional baseball (npb) league in japan, producing a provably-optimal schedule requiring 42950 kilometres of total team travel, a 16% reduction compared to the actual distance traveled by these teams during the 2010 npb season. we also develop a nearly-optimal inter-league tournament for the 30-team nba league, just 3.8% higher than the trivial theoretical lower bound.",2011
first-order stable model semantics and first-order loop formulas,"lin and zhao's theorem on loop formulas states that in the propositional case the stable model semantics of a logic program can be completely characterized by propositional loop formulas, but this result does not fully carry over to the first-order case. we investigate the precise relationship between the first-order stable model semantics and first-order loop formulas, and study conditions under which the former can be represented by the latter. in order to facilitate the comparison, we extend the definition of a first-order loop formula which was limited to a nondisjunctive program, to a disjunctive program and to an arbitrary first-order theory. based on the studied relationship we extend the syntax of a logic program with explicit quantifiers, which allows us to do reasoning involving non-herbrand stable models using first-order reasoners. such programs can be viewed as a special class of first-order theories under the stable model semantics, which yields more succinct loop formulas than the general language due to their restricted syntax.",2011
topological value iteration algorithms,"value iteration is a powerful yet inefficient algorithm for markov decision processes (mdps) because it puts the majority of its effort into backing up the entire state space, which turns out to be unnecessary in many cases. in order to overcome this problem, many approaches have been proposed. among them, ilao* and variants of rtdp are state-of-the-art ones. these methods use reachability analysis and heuristic search to avoid some unnecessary backups. however, none of these approaches build the graphical structure of the state transitions in a pre-processing step or use the structural information to systematically decompose a problem, whereby generating an intelligent backup sequence of the state space. in this paper, we present two optimal mdp algorithms. the first algorithm, topological value iteration (tvi), detects the structure of mdps and backs up states based on topological sequences. it (1) divides an mdp into strongly-connected components (sccs), and (2) solves these components sequentially. tvi outperforms vi and other state-of-the-art algorithms vastly when an mdp has multiple, close-to-equal-sized sccs. the second algorithm, focused topological value iteration (ftvi), is an extension of tvi. ftvi restricts its attention to connected components that are relevant for solving the mdp. specifically, it uses a small amount of heuristic search to eliminate provably sub-optimal actions; this pruning allows ftvi to find smaller connected components, thus running faster. we demonstrate that ftvi outperforms tvi by an order of magnitude, averaged across several domains. surprisingly, ftvi also significantly outperforms popular `heuristically-informed' mdp algorithms such as ilao*, lrtdp, brtdp and bayesian-rtdp in many domains, sometimes by as much as two orders of magnitude. finally, we characterize the type of domains where ftvi excels --- suggesting a way to an informed choice of solver.",2011
representing and reasoning with qualitative preferences for compositional systems,"many applications, e.g., web service composition, complex system design, team formation, etc., rely on methods for identifying collections of objects or entities satisfying some functional requirement. among the collections that satisfy the functional requirement, it is often necessary to identify one or more collections that are optimal with respect to user preferences over a set of attributes that describe the non-functional properties of the collection. we develop a formalism that lets users express the relative importance among attributes and qualitative preferences over the valuations of each attribute. we define a dominance relation that allows us to compare collections of objects in terms of preferences over attributes of the objects that make up the collection. we establish some key properties of the dominance relation. in particular, we show that the dominance relation is a strict partial order when the intra-attribute preference relations are strict partial orders and the relative importance preference relation is an interval order. we provide algorithms that use this dominance relation to identify the set of most preferred collections. we show that under certain conditions, the algorithms are guaranteed to return only (sound), all (complete), or at least one (weakly complete) of the most preferred collections. we present results of simulation experiments comparing the proposed algorithms with respect to (a) the quality of solutions (number of most preferred solutions) produced by the algorithms, and (b) their performance and efficiency. we also explore some interesting conjectures suggested by the results of our experiments that relate the properties of the user preferences, the dominance relation, and the algorithms.",2011
centrality-as-relevance: support sets and similarity as geometric proximity,"in automatic summarization, centrality-as-relevance means that the most important content of an information source, or a collection of information sources, corresponds to the most central passages, considering a representation where such notion makes sense (graph, spatial, etc.). we assess the main paradigms, and introduce a new centrality-based relevance model for automatic summarization that relies on the use of support sets to better estimate the relevant content. geometric proximity is used to compute semantic relatedness. centrality (relevance) is determined by considering the whole input source (and not only local information), and by taking into account the existence of minor topics or lateral subjects in the information sources to be summarized. the method consists in creating, for each passage of the input source, a support set consisting only of the most semantically related passages. then, the determination of the most relevant content is achieved by selecting the passages that occur in the largest number of support sets. this model produces extractive summaries that are generic, and language- and domain-independent. thorough automatic evaluation shows that the method achieves state-of-the-art performance, both in written text, and automatically transcribed speech summarization, including when compared to considerably more complex approaches.",2011
most relevant explanation in bayesian networks,"a major inference task in bayesian networks is explaining why some variables are observed in their particular states using a set of target variables. existing methods for solving this problem often generate explanations that are either too simple (underspecified) or too complex (overspecified). in this paper, we introduce a method called most relevant explanation (mre) which finds a partial instantiation of the target variables that maximizes the generalized bayes factor (gbf) as the best explanation for the given evidence. our study shows that gbf has several theoretical properties that enable mre to automatically identify the most relevant target variables in forming its explanation. in particular, conditional bayes factor (cbf), defined as the gbf of a new explanation conditioned on an existing explanation, provides a soft measure on the degree of relevance of the variables in the new explanation in explaining the evidence given the existing explanation. as a result, mre is able to automatically prune less relevant variables from its explanation. we also show that cbf is able to capture well the explaining-away phenomenon that is often represented in bayesian networks. moreover, we define two dominance relations between the candidate solutions and use the relations to generalize mre to find a set of top explanations that is both diverse and representative. case studies on several benchmark diagnostic bayesian networks show that mre is often able to find explanatory hypotheses that are not only precise but also concise.",2011
learning to make predictions in partially observable environments without a generative model,"when faced with the problem of learning a model of a high-dimensional environment, a common approach is to limit the model to make only a restricted set of predictions, thereby simplifying the learning problem. these partial models may be directly useful for making decisions or may be combined together to form a more complete, structured model. however, in partially observable (non-markov) environments, standard model-learning methods learn generative models, i.e. models that provide a probability distribution over all possible futures (such as pomdps). it is not straightforward to restrict such models to make only certain predictions, and doing so does not always simplify the learning problem. in this paper we present prediction profile models: non-generative partial models for partially observable systems that make only a given set of predictions, and are therefore far simpler than generative models in some cases. we formalize the problem of learning a prediction profile model as a transformation of the original model-learning problem, and show empirically that one can learn prediction profile models that make a small set of important predictions even in systems that are too complex for standard generative models.",2011
"making decisions using sets of probabilities: updating, time consistency, and calibration","we consider how an agent should update her beliefs when her beliefs are represented by a set p of probability distributions, given that the agent makes decisions using the minimax criterion, perhaps the best-studied and most commonly-used criterion in the literature. we adopt a game-theoretic framework, where the agent plays against a bookie, who chooses some distribution from p. we consider two reasonable games that differ in what the bookie knows when he makes his choice. anomalies that have been observed before, like time inconsistency, can be understood as arising because different games are being played, against bookies with different information. we characterize the important special cases in which the optimal decision rules according to the minimax criterion amount to either conditioning or simply ignoring the information. finally, we consider the relationship between updating and calibration when uncertainty is described by sets of probabilities. our results emphasize the key role of the rectangularity condition of epstein and schneider.",2011
adaptive submodularity: theory and applications in active learning and stochastic optimization,"many problems in artificial intelligence require adaptively making a sequence of decisions with uncertain outcomes under partial observability. solving such stochastic optimization problems is a fundamental but notoriously difficult challenge. in this paper, we introduce the concept of adaptive submodularity, generalizing submodular set functions to adaptive policies. we prove that if a problem satisfies this property, a simple adaptive greedy algorithm is guaranteed to be competitive with the optimal policy. in addition to providing performance guarantees for both stochastic maximization and coverage, adaptive submodularity can be exploited to drastically speed up the greedy algorithm by using lazy evaluations. we illustrate the usefulness of the concept by giving several examples of adaptive submodular objectives arising in diverse ai applications including management of sensing resources, viral marketing and active learning. proving adaptive submodularity for these problems allows us to recover existing results in these applications as special cases, improve approximation guarantees and handle natural generalizations.",2011
unfounded sets and well-founded semantics of answer set programs with aggregates,"logic programs with aggregates (lpa) are one of the major linguistic extensions to logic programming (lp). in this work, we propose a generalization of the notions of unfounded set and well-founded semantics for programs with monotone and antimonotone aggregates (lpama programs). in particular, we present a new notion of unfounded set for lpama programs, which is a sound generalization of the original definition for standard (aggregate-free) lp. on this basis, we define a well-founded operator for lpama programs, the fixpoint of which is called well-founded model (or well-founded semantics) for lpama programs. the most important properties of unfounded sets and the well-founded semantics for standard lp are retained by this generalization, notably existence and uniqueness of the well-founded model, together with a strong relationship to the answer set semantics for lpama programs. we show that one of the d-well-founded semantics, defined by pelov, denecker, and bruynooghe for a broader class of aggregates using approximating operators, coincides with the well-founded model as defined in this work on lpama programs. we also discuss some complexity issues, most importantly we give a formal proof of tractable computation of the well-founded model for lpa programs. moreover, we prove that for general lpa programs, which may contain aggregates that are neither monotone nor antimonotone, deciding satisfaction of aggregate expressions with respect to partial interpretations is conp-complete. as a consequence, a well-founded semantics for general lpa programs that allows for tractable computation is unlikely to exist, which justifies the restriction on lpama programs. finally, we present a prototype system extending dlv, which supports the well-founded semantics for lpama programs, at the time of writing the only implemented system that does so. experiments with this prototype show significant computational advantages of aggregate constructs over equivalent aggregate-free encodings.",2011
cloning in elections: finding the possible winners,"we consider the problem of manipulating elections by cloning candidates. in our model, a manipulator can replace each candidate c by several clones, i.e., new candidates that are so similar to c that each voter simply replaces c in his vote with a block of these new candidates, ranked consecutively. the outcome of the resulting election may then depend on the number of clones as well as on how each voter orders the clones within the block. we formalize what it means for a cloning manipulation to be successful (which turns out to be a surprisingly delicate issue), and, for a number of common voting rules, characterize the preference profiles for which a successful cloning manipulation exists. we also consider the model where there is a cost associated with producing each clone, and study the complexity of finding a minimum-cost cloning manipulation. finally, we compare cloning with two related problems: the problem of control by adding candidates and the problem of possible (co)winners when new alternatives can join.",2011
computing approximate nash equilibria and robust best-responses using sampling,"this article discusses two contributions to decision-making in complex partially observable stochastic games. first, we apply two state-of-the-art search techniques that use monte-carlo sampling to the task of approximating a nash-equilibrium (ne) in such games, namely monte-carlo tree search (mcts) and monte-carlo counterfactual regret minimization (mccfr). mcts has been proven to approximate a ne in perfect-information games. we show that the algorithm quickly finds a reasonably strong strategy (but not a ne) in a complex imperfect information game, i.e. poker. mccfr on the other hand has theoretical ne convergence guarantees in such a game. we apply mccfr for the first time in poker. based on our experiments, we may conclude that mcts is a valid approach if one wants to learn reasonably strong strategies fast, whereas mccfr is the better choice if the quality of the strategy is most important. our second contribution relates to the observation that a ne is not a best response against players that are not playing a ne. we present monte-carlo restricted nash response (mcrnr), a sample-based algorithm for the computation of restricted nash strategies. these are robust best-response strategies that (1) exploit non-ne opponents more than playing a ne and (2) are not (overly) exploitable by other strategies. we combine the advantages of two state-of-the-art algorithms, i.e. mccfr and restricted nash response (rnr). mcrnr samples only relevant parts of the game tree. we show that mcrnr learns quicker than standard rnr in smaller games. also we show in poker that mcrnr learns robust best-response strategies fast, and that these strategies exploit opponents more than playing a ne does.",2011
drake: an efficient executive for temporal plans with choice,"this work presents drake, a dynamic executive for temporal plans with choice. dynamic plan execution strategies allow an autonomous agent to react quickly to unfolding events, improving the robustness of the agent. prior work developed methods for dynamically dispatching simple temporal networks, and further research enriched the expressiveness of the plans executives could handle, including discrete choices, which are the focus of this work. however, in some approaches to date, these additional choices induce significant storage or latency requirements to make flexible execution possible. drake is designed to leverage the low latency made possible by a preprocessing step called compilation, while avoiding high memory costs through a compact representation. we leverage the concepts of labels and environments, taken from prior work in assumption-based truth maintenance systems (atms), to concisely record the implications of the discrete choices, exploiting the structure of the plan to avoid redundant reasoning or storage. our labeling and maintenance scheme, called the labeled value set maintenance system, is distinguished by its focus on properties fundamental to temporal problems, and, more generally, weighted graph algorithms. in particular, the maintenance system focuses on maintaining a minimal representation of non-dominated constraints. we benchmark drake's performance on random structured problems, and find that drake reduces the size of the compiled representation by a factor of over 500 for large problems, while incurring only a modest increase in run-time latency, compared to prior work in compiled executives for temporal plans with discrete choices.",2011
finding consensus bayesian network structures,"suppose that multiple experts (or learning algorithms) provide us with alternative bayesian network (bn) structures over a domain, and that we are interested in combining them into a single consensus bn structure. specifically, we are interested in that the consensus bn structure only represents independences all the given bn structures agree upon and that it has as few parameters associated as possible. in this paper, we prove that there may exist several non-equivalent consensus bn structures and that finding one of them is np-hard. thus, we decide to resort to heuristics to find an approximated consensus bn structure. in this paper, we consider the heuristic proposed by matzkevich and abramson, which builds upon two algorithms, called methods a and b, for efficiently deriving the minimal directed independence map of a bn structure relative to a given node ordering. methods a and b are claimed to be correct although no proof is provided (a proof is just sketched). in this paper, we show that methods a and b are not correct and propose a correction of them.",2011
combining evaluation metrics via the unanimous improvement ratio and its application to clustering tasks,"many artificial intelligence tasks cannot be evaluated with a single quality criterion and some sort of weighted combination is needed to provide system rankings. a problem of weighted combination measures is that slight changes in the relative weights may produce substantial changes in the system rankings. this paper introduces the unanimous improvement ratio (uir), a measure that complements standard metric combination criteria (such as van rijsbergens f-measure) and indicates how robust the measured differences are to changes in the relative weights of the individual metrics. uir is meant to elucidate whether a perceived difference between two systems is an artifact of how individual metrics are weighted. besides discussing the theoretical foundations of uir, this paper presents empirical results that confirm the validity and usefulness of the metric for the text clustering problem, where there is a tradeoff between precision and recall based metrics and results are particularly sensitive to the weighting scheme used to combine them. remarkably, our experiments show that uir can be used as a predictor of how well differences between systems measured on a given test bed will also hold in a different test bed.",2011
defeasible inclusions in low-complexity dls,"some of the applications of owl and rdf (e.g. biomedical knowledge representation and semantic policy formulation) call for extensions of these languages with nonmonotonic constructs such as inheritance with overriding. nonmonotonic description logics have been studied for many years, however no practical such knowledge representation languages exist, due to a combination of semantic difficulties and high computational complexity. independently, low-complexity description logics such as dl-lite and el have been introduced and incorporated in the owl standard. therefore, it is interesting to see whether the syntactic restrictions characterizing dl-lite and el bring computational benefits to their nonmonotonic versions, too. in this paper we extensively investigate the computational complexity of circumscription when knowledge bases are formulated in dl-lite_r, el, and fragments thereof. we identify fragments whose complexity ranges from p to the second level of the polynomial hierarchy, as well as fragments whose complexity raises to pspace and beyond.",2011
theoretical and practical foundations of large-scale agent-based micro-storage in the smart grid,"in this paper, we present a novel decentralised management technique that allows electricity micro-storage devices, deployed within individual homes as part of a smart electricity grid, to converge to profitable and efficient behaviours. specifically, we propose the use of software agents, residing on the users' smart meters, to automate and optimise the charging cycle of micro-storage devices in the home to minimise its costs, and we present a study of both the theoretical underpinnings and the implications of a practical solution, of using software agents for such micro-storage management. first, by formalising the strategic choice each agent makes in deciding when to charge its battery, we develop a game-theoretic framework within which we can analyse the competitive equilibria of an electricity grid populated by such agents and hence predict the best consumption profile for that population given their battery properties and individual load profiles. our framework also allows us to compute theoretical bounds on the amount of storage that will be adopted by the population. second, to analyse the practical implications of micro-storage deployments in the grid, we present a novel algorithm that each agent can use to optimise its battery storage profile in order to minimise its owner's costs. this algorithm uses a learning strategy that allows it to adapt as the price of electricity changes in real-time, and we show that the adoption of these strategies results in the system converging to the theoretical equilibria. finally, we empirically evaluate the adoption of our micro-storage management technique within a complex setting, based on the uk electricity market, where agents may have widely varying load profiles, battery types, and learning rates. in this case, our approach yields savings of up to 14% in energy cost for an average consumer using a storage device with a capacity of less than 4.5 kwh and up to a 7% reduction in carbon emissions resulting from electricity generation (with only domestic consumers adopting micro-storage and, commercial and industrial consumers not changing their demand). moreover, corroborating our theoretical bound, an equilibrium is shown to exist where no more than 48% of households would wish to own storage devices and where social welfare would also be improved (yielding overall annual savings of nearly &#163;1.5b).",2011
stochastic enforced hill-climbing,"enforced hill-climbing is an effective deterministic hill-climbing technique that deals with local optima using breadth-first search (a process called ``basin flooding''). we propose and evaluate a stochastic generalization of enforced hill-climbing for online use in goal-oriented probabilistic planning problems. we assume a provided heuristic function estimating expected cost to the goal with flaws such as local optima and plateaus that thwart straightforward greedy action choice. while breadth-first search is effective in exploring basins around local optima in deterministic problems, for stochastic problems we dynamically build and solve a heuristic-based markov decision process (mdp) model of the basin in order to find a good escape policy exiting the local optimum. we note that building this model involves integrating the heuristic into the mdp problem because the local goal is to improve the heuristic. we evaluate our proposal in twenty-four recent probabilistic planning-competition benchmark domains and twelve probabilistically interesting problems from recent literature. for evaluation, we show that stochastic enforced hill-climbing (seh) produces better policies than greedy heuristic following for value/cost functions derived in two very different ways: one type derived by using deterministic heuristics on a deterministic relaxation and a second type derived by automatic learning of bellman-error features from domain-specific experience. using the first type of heuristic, seh is shown to generally outperform all planners from the first three international probabilistic planning competitions.",2011
dr.fill: crosswords and an implemented solver for singly weighted csps,"we describe dr.fill, a program that solves american-style crossword puzzles. from a technical perspective, dr.fill works by converting crosswords to weighted csps, and then using a variety of novel techniques to find a solution. these techniques include generally applicable heuristics for variable and value selection, a variant of limited discrepancy search, and postprocessing and partitioning ideas. branch and bound is not used, as it was incompatible with postprocessing and was determined experimentally to be of little practical value. dr.filll's performance on crosswords from the american crossword puzzle tournament suggests that it ranks among the top fifty or so crossword solvers in the world.",2011
multi-robot adversarial patrolling: facing a full-knowledge opponent,"the problem of adversarial multi-robot patrol has gained interest in recent years, mainly due to its immediate relevance to various security applications. in this problem, robots are required to repeatedly visit a target area in a way that maximizes their chances of detecting an adversary trying to penetrate through the patrol path. when facing a strong adversary that knows the patrol strategy of the robots, if the robots use a deterministic patrol algorithm, then in many cases it is easy for the adversary to penetrate undetected (in fact, in some of those cases the adversary can guarantee penetration). therefore this paper presents a non-deterministic patrol framework for the robots. assuming that the strong adversary will take advantage of its knowledge and try to penetrate through the patrol's weakest spot, hence an optimal algorithm is one that maximizes the chances of detection in that point. we therefore present a polynomial-time algorithm for determining an optimal patrol under the markovian strategy assumption for the robots, such that the probability of detecting the adversary in the patrol's weakest spot is maximized. we build upon this framework and describe an optimal patrol strategy for several robotic models based on their movement abilities (directed or undirected) and sensing abilities (perfect or imperfect), and in different environment models - either patrol around a perimeter (closed polygon) or an open fence (open polyline).",2011
interpolable formulas in equilibrium logic and answer set programming,"interpolation is an important property of classical and many non-classical logics that has been shown to have interesting applications in computer science and ai. here we study the interpolation property for the the non-monotonic system of equilibrium logic, establishing weaker or stronger forms of interpolation depending on the precise interpretation of the inference relation. these results also yield a form of interpolation for ground logic programs under the answer sets semantics. for disjunctive logic programs we also study the property of uniform interpolation that is closely related to the concept of variable forgetting. the first-order version of equilibrium logic has analogous interpolation properties whenever the collection of equilibrium models is (first-order) definable. since this is the case for so-called safe programs and theories, it applies to the usual situations that arise in practical answer set programming.",2011
properties of bethe free energies and message passing in gaussian models,"we address the problem of computing approximate marginals in gaussian probabilistic models by using mean field and fractional bethe approximations. we define the gaussian fractional bethe free energy in terms of the moment parameters of the approximate marginals, derive a lower and an upper bound on the fractional bethe free energy and establish a necessary condition for the lower bound to be bounded from below. it turns out that the condition is identical to the pairwise normalizability condition, which is known to be a sufficient condition for the convergence of the message passing algorithm. we show that stable fixed points of the gaussian message passing algorithm are local minima of the gaussian bethe free energy. by a counterexample, we disprove the conjecture stating that the unboundedness of the free energy implies the divergence of the message passing algorithm.",2011
determining possible and necessary winners given partial orders,"usually a voting rule requires agents to give their preferences as linear orders. however, in some cases it is impractical for an agent to give a linear order over all the alternatives. it has been suggested to let agents submit partial orders instead. then, given a voting rule, a profile of partial orders, and an alternative (candidate) c, two important questions arise: first, is it still possible for c to win, and second, is c guaranteed to win? these are the possible winner and necessary winner problems, respectively. each of these two problems is further divided into two sub-problems: determining whether c is a unique winner (that is, c is the only winner), or determining whether c is a co-winner (that is, c is in the set of winners). we consider the setting where the number of alternatives is unbounded and the votes are unweighted. we completely characterize the complexity of possible/necessary winner problems for the following common voting rules: a class of positional scoring rules (including borda), copeland, maximin, bucklin, ranked pairs, voting trees, and plurality with runoff.",2011
value of information lattice: exploiting probabilistic independence for effective feature subset acquisition,"we address the cost-sensitive feature acquisition problem, where misclassifying an instance is costly but the expected misclassification cost can be reduced by acquiring the values of the missing features. because acquiring the features is costly as well, the objective is to acquire the right set of features so that the sum of the feature acquisition cost and misclassification cost is minimized. we describe the value of information lattice (voila), an optimal and efficient feature subset acquisition framework. unlike the common practice, which is to acquire features greedily, voila can reason with subsets of features. voila efficiently searches the space of possible feature subsets by discovering and exploiting conditional independence properties between the features and it reuses probabilistic inference computations to further speed up the process. through empirical evaluation on five medical datasets, we show that the greedy strategy is often reluctant to acquire features, as it cannot forecast the benefit of acquiring multiple features in combination.",2011
soft constraints of difference and equality,"in many combinatorial problems one may need to model the diversity or similarity of assignments in a solution. for example, one may wish to maximise or minimise the number of distinct values in a solution. to formulate problems of this type, we can use soft variants of the well known alldifferent and allequal constraints. we present a taxonomy of six soft global constraints, generated by combining the two latter ones and the two standard cost functions, which are either maximised or minimised. we characterise the complexity of achieving arc and bounds consistency on these constraints, resolving those cases for which np-hardness was neither proven nor disproven. in particular, we explore in depth the constraint ensuring that at least k pairs of variables have a common value. we show that achieving arc consistency is np-hard, however achieving bounds consistency can be done in polynomial time through dynamic programming. moreover, we show that the maximum number of pairs of equal variables can be approximated by a factor 1/2 with a linear time greedy algorithm. finally, we provide a fixed parameter tractable algorithm with respect to the number of values appearing in more than two distinct domains. interestingly, this taxonomy shows that enforcing equality is harder than enforcing difference.",2011
redistribution mechanisms for assignment of heterogeneous objects,"there are p heterogeneous objects to be assigned to n competing agents (n > p) each with unit demand. it is required to design a groves mechanism for this assignment problem satisfying weak budget balance, individual rationality, and minimizing the budget imbalance. this calls for designing an appropriate rebate function. when the objects are identical, this problem has been solved which we refer as wco mechanism. we measure the performance of such mechanisms by the redistribution index. we first prove an impossibility theorem which rules out linear rebate functions with non-zero redistribution index in heterogeneous object assignment. motivated by this theorem, we explore two approaches to get around this impossibility. in the first approach, we show that linear rebate functions with non-zero redistribution index are possible when the valuations for the objects have a certain type of relationship and we design a mechanism with linear rebate function that is worst case optimal. in the second approach, we show that rebate functions with non-zero efficiency are possible if linearity is relaxed. we extend the rebate functions of the wco mechanism to heterogeneous objects assignment and conjecture them to be worst case optimal.",2011
analyzing search topology without running any search: on the connection between causal graphs and h+,"the ignoring delete lists relaxation is of paramount importance for both satisficing and optimal planning. in earlier work, it was observed that the optimal relaxation heuristic h+ has amazing qualities in many classical planning benchmarks, in particular pertaining to the complete absence of local minima. the proofs of this are hand-made, raising the question whether such proofs can be lead automatically by domain analysis techniques. in contrast to earlier disappointing results -- the analysis method has exponential runtime and succeeds only in two extremely simple benchmark domains -- we herein answer this question in the affirmative. we establish connections between causal graph structure and h+ topology. this results in low-order polynomial time analysis methods, implemented in a tool we call torchlight. of the 12 domains where the absence of local minima has been proved, torchlight gives strong success guarantees in 8 domains. empirically, its analysis exhibits strong performance in a further 2 of these domains, plus in 4 more domains where local minima may exist but are rare. in this way, torchlight can distinguish ``easy'' domains from ``hard'' ones. by summarizing structural reasons for analysis failure, torchlight also provides diagnostic output indicating domain aspects that may cause local minima.",2011
probabilistic relational planning with first order decision diagrams,"dynamic programming algorithms have been successfully applied to propositional stochastic planning problems by using compact representations, in particular algebraic decision diagrams, to capture domain dynamics and value functions. work on symbolic dynamic programming lifted these ideas to first order logic using several representation schemes. recent work introduced a first order variant of decision diagrams (fodd) and developed a value iteration algorithm for this representation. this paper develops several improvements to the fodd algorithm that make the approach practical. these include, new reduction operators that decrease the size of the representation, several speedup techniques, and techniques for value approximation. incorporating these, the paper presents a planning system, fodd-planner, for solving relational stochastic planning problems. the system is evaluated on several domains, including problems from the recent international planning competition, and shows competitive performance with top ranking systems. this is the first demonstration of feasibility of this approach and it shows that abstraction through compact representation is a promising approach to stochastic planning.",2011
from ''identical'' to ''similar'': fusing retrieved lists based on inter-document similarities,"methods for fusing document lists that were retrieved in response to a query often utilize the retrieval scores and/or ranks of documents in the lists. we present a novel fusion approach that is based on using, in addition, information induced from inter-document similarities. specifically, our methods let similar documents from different lists provide relevance-status support to each other. we use a graph-based method to model relevance-status propagation between documents. the propagation is governed by inter-document-similarities and by retrieval scores of documents in the lists. empirical evaluation demonstrates the effectiveness of our methods in fusing trec runs. the performance of our most effective methods transcends that of effective fusion methods that utilize only retrieval scores or ranks.",2011
"stackelberg vs. nash in security games: an extended investigation of interchangeability, equivalence, and uniqueness","there has been significant recent interest in game-theoretic approaches to security, with much of the recent research focused on utilizing the leader-follower stackelberg game model. among the major applications are the armor program deployed at lax airport and the iris program in use by the us federal air marshals (fams). the foundational assumption for using stackelberg games is that security forces (leaders), acting first, commit to a randomized strategy; while their adversaries (followers) choose their best response after surveillance of this randomized strategy. yet, in many situations, a leader may face uncertainty about the followers surveillance capability. previous work fails to address how a leader should compute her strategy given such uncertainty. we provide five contributions in the context of a general class of security games. first, we show that the nash equilibria in security games are interchangeable, thus alleviating the equilibrium selection problem. second, under a natural restriction on security games, any stackelberg strategy is also a nash equilibrium strategy; and furthermore, the solution is unique in a class of security games of which armor is a key exemplar. third, when faced with a follower that can attack multiple targets, many of these properties no longer hold. fourth, we show experimentally that in most (but not all) games where the restriction does not hold, the stackelberg strategy is still a nash equilibrium strategy, but this is no longer true when the attacker can attack multiple targets. finally, as a possible direction for future research, we propose an extensive-form game model that makes the defenders uncertainty about the attackers ability to observe explicit.",2011
sequential diagnosis by abstraction,"when a system behaves abnormally, sequential diagnosis takes a sequence of measurements of the system until the faults causing the abnormality are identified, and the goal is to reduce the diagnostic cost, defined here as the number of measurements. to propose measurement points, previous work employs a heuristic based on reducing the entropy over a computed set of diagnoses. this approach generally has good performance in terms of diagnostic cost, but can fail to diagnose large systems when the set of diagnoses is too large. focusing on a smaller set of probable diagnoses scales the approach but generally leads to increased average diagnostic costs. in this paper, we propose a new diagnostic framework employing four new techniques, which scales to much larger systems with good performance in terms of diagnostic cost. first, we propose a new heuristic for measurement point selection that can be computed efficiently, without requiring the set of diagnoses, once the system is modeled as a bayesian network and compiled into a logical form known as d-dnnf. second, we extend hierarchical diagnosis, a technique based on system abstraction from our previous work, to handle probabilities so that it can be applied to sequential diagnosis to allow larger systems to be diagnosed. third, for the largest systems where even hierarchical diagnosis fails, we propose a novel method that converts the system into one that has a smaller abstraction and whose diagnoses form a superset of those of the original system; the new system can then be diagnosed and the result mapped back to the original system. finally, we propose a novel cost estimation function which can be used to choose an abstraction of the system that is more likely to provide optimal average cost. experiments with iscas-85 benchmark circuits indicate that our approach scales to all circuits in the suite except one that has a flat structure not susceptible to useful abstraction.",2011
the opposite of smoothing: a language model approach to ranking query-specific document clusters,"exploiting information induced from (query-specific) clustering of top-retrieved documents has long been proposed as a means for improving precision at the very top ranks of the returned results. we present a novel language model approach to ranking query-specific clusters by the presumed percentage of relevant documents that they contain. while most previous cluster ranking approaches focus on the cluster as a whole, our model utilizes also information induced from documents associated with the cluster. our model substantially outperforms previous approaches for identifying clusters containing a high relevant-document percentage. furthermore, using the model to produce document ranking yields precision-at-top-ranks performance that is consistently better than that of the initial ranking upon which clustering is performed. the performance also favorably compares with that of a state-of-the-art pseudo-feedback-based retrieval method.",2011
policy invariance under reward transformations for general-sum stochastic games,we extend the potential-based shaping method from markov decision processes to multi-player general-sum stochastic games. we prove that the nash equilibria in a stochastic game remains unchanged after potential-based shaping is applied to the environment. the property of policy invariance provides a possible way of speeding convergence when learning to play a stochastic game.,2011
efficient multi-start strategies for local search algorithms,"local search algorithms applied to optimization problems often suffer from getting trapped in a local optimum. the common solution for this deficiency is to restart the algorithm when no progress is observed. alternatively, one can start multiple instances of a local search algorithm, and allocate computational resources (in particular, processing time) to the instances depending on their behavior. hence, a multi-start strategy has to decide (dynamically) when to allocate additional resources to a particular instance and when to start new instances. in this paper we propose multi-start strategies motivated by works on multi-armed bandit problems and lipschitz optimization with an unknown constant. the strategies continuously estimate the potential performance of each algorithm instance by supposing a convergence rate of the local search algorithm up to an unknown constant, and in every phase allocate resources to those instances that could converge to the optimum for a particular range of the constant. asymptotic bounds are given on the performance of the strategies. in particular, we prove that at most a quadratic increase in the number of times the target function is evaluated is needed to achieve the performance of a local search algorithm started from the attraction region of the optimum. experiments are provided using spsa (simultaneous perturbation stochastic approximation) and k-means as local search algorithms, and the results indicate that the proposed strategies work well in practice, and, in all cases studied, need only logarithmically more evaluations of the target function as opposed to the theoretically suggested quadratic increase.",2011
on the intertranslatability of argumentation semantics,"translations between different nonmonotonic formalisms always have been an important topic in the field, in particular to understand the knowledge-representation capabilities those formalisms offer. we provide such an investigation in terms of different semantics proposed for abstract argumentation frameworks, a nonmonotonic yet simple formalism which received increasing interest within the last decade. although the properties of these different semantics are nowadays well understood, there are no explicit results about intertranslatability. we provide such translations wrt. different properties and also give a few novel complexity results which underlie some negative results.",2011
a probabilistic framework for learning kinematic models of articulated objects,"robots operating in domestic environments generally need to interact with articulated objects, such as doors, cabinets, dishwashers or fridges. in this work, we present a novel, probabilistic framework for modeling articulated objects as kinematic graphs. vertices in this graph correspond to object parts, while edges between them model their kinematic relationship. in particular, we present a set of parametric and non-parametric edge models and how they can robustly be estimated from noisy pose observations. we furthermore describe how to estimate the kinematic structure and how to use the learned kinematic models for pose prediction and for robotic manipulation tasks. we finally present how the learned models can be generalized to new and previously unseen objects. in various experiments using real robots with different camera systems as well as in simulation, we show that our approach is valid, accurate and efficient. further, we demonstrate that our approach has a broad set of applications, in particular for the emerging fields of mobile manipulation and service robotics.",2011
controlling complexity in part-of-speech induction,"we consider the problem of fully unsupervised learning of grammatical (part-of-speech) categories from unlabeled text. the standard maximum-likelihood hidden markov model for this task performs poorly, because of its weak inductive bias and large model capacity. we address this problem by refining the model and modifying the learning objective to control its capacity via para- metric and non-parametric constraints. our approach enforces word-category association sparsity, adds morphological and orthographic features, and eliminates hard-to-estimate parameters for rare words. we develop an efficient learning algorithm that is not much more computationally intensive than standard training. we also provide an open-source implementation of the algorithm. our experiments on five diverse languages (bulgarian, danish, english, portuguese, spanish) achieve significant improvements compared with previous methods for the same task.",2011
non-deterministic policies in markovian decision processes,"markovian processes have long been used to model stochastic environments. reinforcement learning has emerged as a framework to solve sequential planning and decision-making problems in such environments. in recent years, attempts were made to apply methods from reinforcement learning to construct decision support systems for action selection in markovian environments. although conventional methods in reinforcement learning have proved to be useful in problems concerning sequential decision-making, they cannot be applied in their current form to decision support systems, such as those in medical domains, as they suggest policies that are often highly prescriptive and leave little room for the user's input. without the ability to provide flexible guidelines, it is unlikely that these methods can gain ground with users of such systems. this paper introduces the new concept of non-deterministic policies to allow more flexibility in the user's decision-making process, while constraining decisions to remain near optimal solutions. we provide two algorithms to compute non-deterministic policies in discrete domains. we study the output and running time of these method on a set of synthetic and real-world problems. in an experiment with human subjects, we show that humans assisted by hints based on non-deterministic policies outperform both human-only and computer-only agents in a web navigation task.",2011
a logical study of partial entailment,"we introduce a novel logical notion--partial entailment--to propositional logic. in contrast with classical entailment, that a formula p partially entails another formula q with respect to a background formula set \gamma intuitively means that under the circumstance of \gamma, if p is true then some ""part"" of q will also be true. we distinguish three different kinds of partial entailments and formalize them by using an extended notion of prime implicant. we study their semantic properties, which show that, surprisingly, partial entailments fail for many simple inference rules. then, we study the related computational properties, which indicate that partial entailments are relatively difficult to be computed. finally, we consider a potential application of partial entailments in reasoning about rational agents.",2011
false-name manipulations in weighted voting games,"weighted voting is a classic model of cooperation among agents in decision-making domains. in such games, each player has a weight, and a coalition of players wins the game if its total weight meets or exceeds a given quota. a player's power in such games is usually not directly proportional to his weight, and is measured by a power index, the most prominent among which are the shapley-shubik index and the banzhaf index.in this paper, we investigate by how much a player can change his power, as measured by the shapley-shubik index or the banzhaf index, by means of a false-name manipulation, i.e., splitting his weight among two or more identities. for both indices, we provide upper and lower bounds on the effect of weight-splitting. we then show that checking whether a beneficial split exists is np-hard, and discuss efficient algorithms for restricted cases of this problem, as well as randomized algorithms for the general case. we also provide an experimental evaluation of these algorithms. finally, we examine related forms of manipulative behavior, such as annexation, where a player subsumes other players, or merging, where several players unite into one. we characterize the computational complexity of such manipulations and provide limits on their effects. for the banzhaf index, we describe a new paradox, which we term the annexation non-monotonicity paradox.",2011
a monte-carlo aixi approximation,"this paper introduces a principled approach for the design of a scalable general reinforcement learning agent. our approach is based on a direct approximation of aixi, a bayesian optimality notion for general reinforcement learning agents. previously, it has been unclear whether the theory of aixi could motivate the design of practical algorithms. we answer this hitherto open question in the affirmative, by providing the first computationally feasible approximation to the aixi agent. to develop our approximation, we introduce a new monte-carlo tree search algorithm along with an agent-specific extension to the context tree weighting algorithm. empirically, we present a set of encouraging results on a variety of stochastic and partially observable domains. we conclude by proposing a number of directions for future research.",2011
automated search for impossibility theorems in social choice theory: ranking sets of objects,"we present a method for using standard techniques from satisfiability checking to automatically verify and discover theorems in an area of economic theory known as ranking sets of objects. the key question in this area, which has important applications in social choice theory and decision making under uncertainty, is how to extend an agent's preferences over a number of objects to a preference relation over nonempty sets of such objects. certain combinations of seemingly natural principles for this kind of preference extension can result in logical inconsistencies, which has led to a number of important impossibility theorems. we first prove a general result that shows that for a wide range of such principles, characterised by their syntactic form when expressed in a many-sorted first-order logic, any impossibility exhibited at a fixed (small) domain size will necessarily extend to the general case. we then show how to formulate candidates for impossibility theorems at a fixed domain size in propositional logic, which in turn enables us to automatically search for (general) impossibility theorems using a sat solver. when applied to a space of 20 principles for preference extension familiar from the literature, this method yields a total of 84 impossibility theorems, including both known and nontrivial new results.",2011
second-order consistencies,"in this paper, we propose a comprehensive study of second-order consistencies (i.e., consistencies identifying inconsistent pairs of values) for constraint satisfaction. we build a full picture of the relationships existing between four basic second-order consistencies, namely path consistency (pc), 3-consistency (3c), dual consistency (dc) and 2-singleton arc consistency (2sac), as well as their conservative and strong variants. interestingly, dual consistency is an original property that can be established by using the outcome of the enforcement of generalized arc consistency (gac), which makes it rather easy to obtain since constraint solvers typically maintain gac during search. on binary constraint networks, dc is equivalent to pc, but its restriction to existing constraints, called conservative dual consistency (cdc), is strictly stronger than traditional conservative consistencies derived from path consistency, namely partial path consistency (ppc) and conservative path consistency (cpc). after introducing a general algorithm to enforce strong (c)dc, we present the results of an experimentation over a wide range of benchmarks that demonstrate the interest of (conservative) dual consistency. in particular, we show that enforcing (c)dc before search clearly improves the performance of mac (the algorithm that maintains gac during search) on several binary and non-binary structured problems.",2011
a probabilistic approach for maintaining trust based on evidence,"leading agent-based trust models address two important needs. first, they show how an agent may estimate the trustworthiness of another agent based on prior interactions. second, they show how agents may share their knowledge in order to cooperatively assess the trustworthiness of others. however, in real-life settings, information relevant to trust is usually obtained piecemeal, not all at once. unfortunately, the problem of maintaining trust has drawn little attention. existing approaches handle trust updates in a heuristic, not a principled, manner. this paper builds on a formal model that considers probability and certainty as two dimensions of trust. it proposes a mechanism using which an agent can update the amount of trust it places in other agents on an ongoing basis. this paper shows via simulation that the proposed approach (a) provides accurate estimates of the trustworthiness of agents that change behavior frequently; and (b) captures the dynamic behavior of the agents. this paper includes an evaluation based on a real dataset drawn from amazon marketplace, a leading e-commerce site.",2011
iterated belief change due to actions and observations,"in action domains where agents may have erroneous beliefs, reasoning about the effects of actions involves reasoning about belief change. in this paper, we use a transition system approach to reason about the evolution of an agent's beliefs as actions are executed. some actions cause an agent to perform belief revision while others cause an agent to perform belief update, but the interaction between revision and update can be non-elementary. we present a set of rationality properties describing the interaction between revision and update, and we introduce a new class of belief change operators for reasoning about alternating sequences of revisions and updates. our belief change operators can be characterized in terms of a natural shifting operation on total pre-orderings over interpretations. we compare our approach with related work on iterated belief change due to action, and we conclude with some directions for future research.",2011
multimode control attacks on elections,"in 1992, bartholdi, tovey, and trick opened the study of control attacks on elections---attempts to improve the election outcome by such actions as adding/deleting candidates or voters. that work has led to many results on how algorithms can be used to find attacks on elections and how complexity-theoretic hardness results can be used as shields against attacks. however, all the work in this line has assumed that the attacker employs just a single type of attack. in this paper, we model and study the case in which the attacker launches a multipronged (i.e., multimode) attack. we do so to more realistically capture the richness of real-life settings. for example, an attacker might simultaneously try to suppress some voters, attract new voters into the election, and introduce a spoiler candidate. our model provides a unified framework for such varied attacks. by constructing polynomial-time multiprong attack algorithms we prove that for various election systems even such concerted, flexible attacks can be perfectly planned in deterministic polynomial time.",2011
clause-learning algorithms with many restarts and bounded-width resolution,"we offer a new understanding of some aspects of practical sat-solvers that are based on dpll with unit-clause propagation, clause-learning, and restarts. we do so by analyzing a concrete algorithm which we claim is faithful to what practical solvers do. in particular, before making any new decision or restart, the solver repeatedly applies the unit-resolution rule until saturation, and leaves no component to the mercy of non-determinism except for some internal randomness. we prove the perhaps surprising fact that, although the solver is not explicitly designed for it, with high probability it ends up behaving as width-k resolution after no more than o(n^{2k+2}) conflicts and restarts, where n is the number of variables. in other words, width-k resolution can be thought of as o(n^{2k+2}) restarts of the unit-resolution rule with learning.",2011
evaluating temporal graphs built from texts via transitive reduction,"temporal information has been the focus of recent attention in information extraction, leading to some standardization effort, in particular for the task of relating events in a text. this task raises the problem of comparing two annotations of a given text, because relations between events in a story are intrinsically interdependent and cannot be evaluated separately. a proper evaluation measure is also crucial in the context of a machine learning approach to the problem. finding a common comparison referent at the text level is not obvious, and we argue here in favor of a shift from event-based measures to measures on a unique textual object, a minimal underlying temporal graph, or more formally the transitive reduction of the graph of relations between event boundaries. we support it by an investigation of its properties on synthetic data and on a well-know temporal corpus.",2011
on-line planning and scheduling: an application to controlling modular printers,"we present a case study of artificial intelligence techniques applied to the control of production printing equipment. like many other real-world applications, this complex domain requires high-speed autonomous decision-making and robust continual operation. to our knowledge, this work represents the first successful industrial application of embedded domain-independent temporal planning. our system handles execution failures and multi-objective preferences. at its heart is an on-line algorithm that combines techniques from state-space planning and partial-order scheduling. we suggest that this general architecture may prove useful in other applications as more intelligent systems operate in continual, on-line settings. our system has been used to drive several commercial prototypes and has enabled a new product architecture for our industrial partner. when compared with state-of-the-art off-line planners, our system is hundreds of times faster and often finds better plans. our experience demonstrates that domain-independent ai planning based on heuristic search can flexibly handle time, resources, replanning, and multiple objectives in a high-speed practical application without requiring hand-coded control knowledge.",2011
narrowing the modeling gap: a cluster-ranking approach to coreference resolution,"traditional learning-based coreference resolvers operate by training the mention-pair model for determining whether two mentions are coreferent or not. though conceptually simple and easy to understand, the mention-pair model is linguistically rather unappealing and lags far behind the heuristic-based coreference models proposed in the pre-statistical nlp era in terms of sophistication. two independent lines of recent research have attempted to improve the mention-pair model, one by acquiring the mention-ranking model to rank preceding mentions for a given anaphor, and the other by training the entity-mention model to determine whether a preceding cluster is coreferent with a given mention. we propose a cluster-ranking approach to coreference resolution, which combines the strengths of the mention-ranking model and the entity-mention model, and is therefore theoretically more appealing than both of these models. in addition, we seek to improve cluster rankers via two extensions: (1) lexicalization and (2) incorporating knowledge of anaphoricity by jointly modeling anaphoricity determination and coreference resolution. experimental results on the ace data sets demonstrate the superior performance of cluster rankers to competing approaches as well as the effectiveness of our two extensions.",2011
efficient planning under uncertainty with macro-actions,"deciding how to act in partially observable environments remains an active area of research. identifying good sequences of decisions is particularly challenging when good control performance requires planning multiple steps into the future in domains with many states. towards addressing this challenge, we present an online, forward-search algorithm called the posterior belief distribution (pbd). pbd leverages a novel method for calculating the posterior distribution over beliefs that result after a sequence of actions is taken, given the set of observation sequences that could be received during this process. this method allows us to efficiently evaluate the expected reward of a sequence of primitive actions, which we refer to as macro-actions. we present a formal analysis of our approach, and examine its performance on two very large simulation experiments: scientific exploration and a target monitoring domain. we also demonstrate our algorithm being used to control a real robotic helicopter in a target monitoring experiment, which suggests that our approach has practical potential for planning in real-world, large partially observable domains where a multi-step lookahead is required to achieve good performance.",2011
multiagent learning in large anonymous games,"in large systems, it is important for agents to learn to act effectively, but sophisticated multi-agent learning algorithms generally do not scale. an alternative approach is to find restricted classes of games where simple, efficient algorithms converge. it is shown that stage learning efficiently converges to nash equilibria in large anonymous games if best-reply dynamics converge. two features are identified that improve convergence. first, rather than making learning more difficult, more agents are actually beneficial in many settings. second, providing agents with statistical information about the behavior of others can significantly reduce the number of observations needed.",2011
decidability and undecidability results for propositional schemata,"we define a logic of propositional formula schemata adding to the syntax of propositional logic indexed propositions and iterated connectives ranging over intervals parameterized by arithmetic variables. the satisfiability problem is shown to be undecidable for this new logic, but we introduce a very general class of schemata, called bound-linear, for which this problem becomes decidable. this result is obtained by reduction to a particular class of schemata called regular, for which we provide a sound and complete terminating proof procedure. this schemata calculus allows one to capture proof patterns corresponding to a large class of problems specified in propositional logic. we also show that the satisfiability problem becomes again undecidable for slight extensions of this class, thus demonstrating that bound-linear schemata represent a good compromise between expressivity and decidability.",2011
the complexity of integer bound propagation,"bound propagation is an important artificial intelligence technique used in constraint programming tools to deal with numerical constraints. it is typically embedded within a search procedure (branch and prune) and used at every node of the search tree to narrow down the search space, so it is critical that it be fast. the procedure invokes constraint propagators until a common fixpoint is reached, but the known algorithms for this have a pseudo-polynomial worst-case time complexity: they are fast indeed when the variables have a small numerical range, but they have the well-known problem of being prohibitively slow when these ranges are large. an important question is therefore whether strongly-polynomial algorithms exist that compute the common bound consistent fixpoint of a set of constraints. this paper answers this question. in particular we show that this fixpoint computation is in fact np-complete, even when restricted to binary linear constraints.",2011
identifying aspects for web-search queries,"many web-search queries serve as the beginning of an exploration of an unknown space of information, rather than looking for a specific web page. to answer such queries effec- tively, the search engine should attempt to organize the space of relevant information in a way that facilitates exploration. we describe the aspector system that computes aspects for a given query. each aspect is a set of search queries that together represent a distinct information need relevant to the original search query. to serve as an effective means to explore the space, aspector computes aspects that are orthogonal to each other and to have high combined coverage. aspector combines two sources of information to compute aspects. we discover candidate aspects by analyzing query logs, and cluster them to eliminate redundancies. we then use a mass-collaboration knowledge base (e.g., wikipedia) to compute candidate aspects for queries that occur less frequently and to group together aspects that are likely to be semantically related. we present a user study that indicates that the aspects we compute are rated favorably against three competing alternatives  related searches proposed by google, cluster labels assigned by the clusty search engine, and navigational searches proposed by bing.",2011
computing small unsatisfiable cores in satisfiability modulo theories,"the problem of finding small unsatisfiable cores for sat formulas has recently received a lot of interest, mostly for its applications in formal verification. however, propositional logic is often not expressive enough for representing many interesting verification problems, which can be more naturally addressed in the framework of satisfiability modulo theories, smt. surprisingly, the problem of finding unsatisfiable cores in smt has received very little attention in the literature. in this paper we present a novel approach to this problem, called the lemma-lifting approach. the main idea is to combine an smt solver with an external propositional core extractor. the smt solver produces the theory lemmas found during the search, dynamically lifting the suitable amount of theory information to the boolean level. the core extractor is then called on the boolean abstraction of the original smt problem and of the theory lemmas. this results in an unsatisfiable core for the original smt problem, once the remaining theory lemmas are removed. the approach is conceptually interesting, and has several advantages in practice. in fact, it is extremely simple to implement and to update, and it can be interfaced with every propositional core extractor in a plug-and-play manner, so as to benefit for free of all unsat-core reduction techniques which have been or will be made available. we have evaluated our algorithm with a very extensive empirical test on smt-lib benchmarks, which confirms the validity and potential of this approach.",2011
exploiting structure in weighted model counting approaches to probabilistic inference,"previous studies have demonstrated that encoding a bayesian network into a sat formula and then performing weighted model counting using a backtracking search algorithm can be an effective method for exact inference. in this paper, we present techniques for improving this approach for bayesian networks with noisy-or and noisy-max relations---two relations that are widely used in practice as they can dramatically reduce the number of probabilities one needs to specify. in particular, we present two sat encodings for noisy-or and two encodings for noisy-max that exploit the structure or semantics of the relations to improve both time and space efficiency, and we prove the correctness of the encodings. we experimentally evaluated our techniques on large-scale real and randomly generated bayesian networks. on these benchmarks, our techniques gave speedups of up to two orders of magnitude over the best previous approaches for networks with noisy-or/max relations and scaled up to larger networks. as well, our techniques extend the weighted model counting approach for exact inference to networks that were previously intractable for the approach.",2011
scaling up heuristic planning with relational decision trees,"current evaluation functions for heuristic planning are expensive to compute. in numerous planning problems these functions provide good guidance to the solution, so they are worth the expense. however, when evaluation functions are misguiding or when planning problems are large enough, lots of node evaluations must be computed, which severely limits the scalability of heuristic planners. in this paper, we present a novel solution for reducing node evaluations in heuristic planning based on machine learning. particularly, we define the task of learning search control for heuristic planning as a relational classification task, and we use an off-the-shelf relational classification tool to address this learning task. our relational classification task captures the preferred action to select in the different planning contexts of a specific planning domain. these planning contexts are defined by the set of helpful actions of the current state, the goals remaining to be achieved, and the static predicates of the planning task. this paper shows two methods for guiding the search of a heuristic planner with the learned classifiers. the first one consists of using the resulting classifier as an action policy. the second one consists of applying the classifier to generate lookahead states within a best first search algorithm. experiments over a variety of domains reveal that our heuristic planner using the learned classifiers solves larger problems than state-of-the-art planners.",2011
regression conformal prediction with nearest neighbours,"in this paper we apply conformal prediction (cp) to the k-nearest neighbours regression (k-nnr) algorithm and propose ways of extending the typical nonconformity measure used for regression so far. unlike traditional regression methods which produce point predictions, conformal predictors output predictive regions that satisfy a given confidence level. the regions produced by any conformal predictor are automatically valid, however their tightness and therefore usefulness depends on the nonconformity measure used by each cp. in effect a nonconformity measure evaluates how strange a given example is compared to a set of other examples based on some traditional machine learning algorithm. we define six novel nonconformity measures based on the k-nearest neighbours regression algorithm and develop the corresponding cps following both the original (transductive) and the inductive cp approaches. a comparison of the predictive regions produced by our measures with those of the typical regression measure suggests that a major improvement in terms of predictive region tightness is achieved by the new measures.",2011
planning with noisy probabilistic relational rules,"noisy probabilistic relational rules are a promising world model representation for several reasons. they are compact and generalize over world instantiations. they are usually interpretable and they can be learned effectively from the action experiences in complex worlds. we investigate reasoning with such rules in grounded relational domains. our algorithms exploit the compactness of rules for efficient and flexible decision-theoretic planning. as a first approach, we combine these rules with the upper confidence bounds applied to trees (uct) algorithm based on look-ahead trees. our second approach converts these rules into a structured dynamic bayesian network representation and predicts the effects of action sequences using approximate inference and beliefs over world states. we evaluate the effectiveness of our approaches for planning in a simulated complex 3d robot manipulation scenario with an articulated manipulator and realistic physics and in domains of the probabilistic planning competition. empirical results show that our methods can solve problems where existing methods fail.",2010
implicit abstraction heuristics,"state-space search with explicit abstraction heuristics is at the state of the art of cost-optimal planning. these heuristics are inherently limited, nonetheless, because the size of the abstract space must be bounded by some, even if a very large, constant. targeting this shortcoming, we introduce the notion of (additive) implicit abstractions, in which the planning task is abstracted by instances of tractable fragments of optimal planning. we then introduce a concrete setting of this framework, called fork-decomposition, that is based on two novel fragments of tractable cost-optimal planning. the induced admissible heuristics are then studied formally and empirically. this study testifies for the accuracy of the fork decomposition heuristics, yet our empirical evaluation also stresses the tradeoff between their accuracy and the runtime complexity of computing them. indeed, some of the power of the explicit abstraction heuristics comes from precomputing the heuristic function offline and then determining h(s) for each evaluated state s by a very fast lookup in a ``database.'' by contrast, while fork-decomposition heuristics can be calculated in polynomial time, computing them is far from being fast. to address this problem, we show that the time-per-node complexity bottleneck of the fork-decomposition heuristics can be successfully overcome. we demonstrate that an equivalent of the explicit abstraction notion of a ``database'' exists for the fork-decomposition abstractions as well, despite their exponential-size abstract spaces. we then verify empirically that heuristic search with the ``databased"" fork-decomposition heuristics favorably competes with the state of the art of cost-optimal planning.",2010
the lama planner: guiding cost-based anytime planning with landmarks,"lama is a classical planning system based on heuristic forward search. its core feature is the use of a pseudo-heuristic derived from landmarks, propositional formulas that must be true in every solution of a planning task. lama builds on the fast downward planning system, using finite-domain rather than binary state variables and multi-heuristic search. the latter is employed to combine the landmark heuristic with a variant of the well-known ff heuristic. both heuristics are cost-sensitive, focusing on high-quality solutions in the case where actions have non-uniform cost. a weighted a* search is used with iteratively decreasing weights, so that the planner continues to search for plans of better quality until the search is terminated. lama showed best performance among all planners in the sequential satisficing track of the international planning competition 2008. in this paper we present the system in detail and investigate which features of lama are crucial for its performance. we present individual results for some of the domains used at the competition, demonstrating good and bad cases for the techniques implemented in lama. overall, we find that using landmarks improves performance, whereas the incorporation of action costs into the heuristic estimators proves not to be beneficial. we show that in some domains a search that ignores cost solves far more problems, raising the question of how to deal with action costs more effectively in the future. the iterated weighted a* search greatly improves results, and shows synergy effects with the use of landmarks.",2010
cooperative games with overlapping coalitions,"in the usual models of cooperative game theory, the outcome of a coalition formation process is either the grand coalition or a coalition structure that consists of disjoint coalitions. however, in many domains where coalitions are associated with tasks, an agent may be involved in executing more than one task, and thus may distribute his resources among several coalitions. to tackle such scenarios, we introduce a model for cooperative games with overlapping coalitionsor overlapping coalition formation (ocf) games. we then explore the issue of stability in this setting. in particular, we introduce a notion of the core, which generalizes the corresponding notion in the traditional (non-overlapping) scenario. then, under some quite general conditions, we characterize the elements of the core, and show that any element of the core maximizes the social welfare. we also introduce a concept of balancedness for overlapping coalitional games, and use it to characterize coalition structures that can be extended to elements of the core. finally, we generalize the notion of convexity to our setting, and show that under some natural assumptions convex games have a non-empty core. moreover, we introduce two alternative notions of stability in ocf that allow a wider range of deviations, and explore the relationships among the corresponding definitions of the core, as well as the classic (non-overlapping) core and the aubin core. we illustrate the general properties of the three cores, and also study them from a computational perspective, thus obtaining additional insights into their fundamental structure.",2010
narrative planning: balancing plot and character,"narrative, and in particular storytelling, is an important part of the human experience. consequently, computational systems that can reason about narrative can be more effective communicators, entertainers, educators, and trainers. one of the central challenges in computational narrative reasoning is narrative generation, the automated creation of meaningful event sequences. there are many factors -- logical and aesthetic -- that contribute to the success of a narrative artifact. central to this success is its understandability. we argue that the following two attributes of narratives are universal: (a) the logical causal progression of plot, and (b) character believability. character believability is the perception by the audience that the actions performed by characters do not negatively impact the audience's suspension of disbelief. specifically, characters must be perceived by the audience to be intentional agents. in this article, we explore the use of refinement search as a technique for solving the narrative generation problem -- to find a sound and believable sequence of character actions that transforms an initial world state into a world state in which goal propositions hold. we describe a novel refinement search planning algorithm -- the intent-based partial order causal link (ipocl) planner -- that, in addition to creating causally sound plot progression, reasons about character intentionality by identifying possible character goals that explain their actions and creating plan structures that explain why those characters commit to their goals. we present the results of an empirical evaluation that demonstrates that narrative plans generated by the ipocl algorithm support audience comprehension of character intentions better than plans generated by conventional partial-order planners.",2010
case-based subgoaling in real-time heuristic search for video game pathfinding,"real-time heuristic search algorithms satisfy a constant bound on the amount of planning per action, independent of problem size. as a result, they scale up well as problems become larger. this property would make them well suited for video games where artificial intelligence controlled agents must react quickly to user commands and to other agents' actions. on the downside, real-time search algorithms employ learning methods that frequently lead to poor solution quality and cause the agent to appear irrational by re-visiting the same problem states repeatedly. the situation changed recently with a new algorithm, d lrta*, which attempted to eliminate learning by automatically selecting subgoals. d lrta* is well poised for video games, except it has a complex and memory-demanding pre-computation phase during which it builds a database of subgoals. in this paper, we propose a simpler and more memory-efficient way of pre-computing subgoals thereby eliminating the main obstacle to applying state-of-the-art real-time search methods in video games. the new algorithm solves a number of randomly chosen problems off-line, compresses the solutions into a series of subgoals and stores them in a database. when presented with a novel problem on-line, it queries the database for the most similar previously solved case and uses its subgoals to solve the problem. in the domain of pathfinding on four large video game maps, the new algorithm delivers solutions eight times better while using 57 times less memory and requiring 14% less pre-computation time.",2010
a model-based active testing approach to sequential diagnosis,"model-based diagnostic reasoning often leads to a large number of diagnostic hypotheses. the set of diagnoses can be reduced by taking into account extra observations (passive monitoring), measuring additional variables (probing) or executing additional tests (sequential diagnosis/test sequencing). in this paper we combine the above approaches with techniques from automated test pattern generation (atpg) and model-based diagnosis (mbd) into a framework called fractal (framework for active testing algorithms). apart from the inputs and outputs that connect a system to its environment, in active testing we consider additional input variables to which a sequence of test vectors can be supplied. we address the computationally hard problem of computing optimal control assignments (as defined in fractal) in terms of a greedy approximation algorithm called fractal-g. we compare the decrease in the number of remaining minimal cardinality diagnoses of fractal-g to that of two more fractal algorithms: fractal-atpg and fractal-p. fractal-atpg is based on atpg and sequential diagnosis while fractal-p is based on probing and, although not an active testing algorithm, provides a baseline for comparing the lower bound on the number of reachable diagnoses for the fractal algorithms. we empirically evaluate the trade-offs of the three fractal algorithms by performing extensive experimentation on the iscas85/74xxx benchmark of combinational circuits.",2010
active tuples-based scheme for bounding posterior beliefs,"the paper presents a scheme for computing lower and upper bounds on the posterior marginals in bayesian networks with discrete variables. its power lies in its ability to use any available scheme that bounds the probability of evidence or posterior marginals and enhance its performance in an anytime manner. the scheme uses the cutset conditioning principle to tighten existing bounding schemes and to facilitate anytime behavior, utilizing a fixed number of cutset tuples. the accuracy of the bounds improves as the number of used cutset tuples increases and so does the computation time. we demonstrate empirically the value of our scheme for bounding posterior marginals and probability of evidence using a variant of the bound propagation algorithm as a plug-in scheme.",2010
a constraint satisfaction framework for executing perceptions and actions in diagrammatic reasoning,"diagrammatic reasoning (dr) is pervasive in human problem solving as a powerful adjunct to symbolic reasoning based on language-like representations. the research reported in this paper is a contribution to building a general purpose dr system as an extension to a soar-like problem solving architecture. the work is in a framework in which dr is modeled as a process where subtasks are solved, as appropriate, either by inference from symbolic representations or by interaction with a diagram, i.e., perceiving specified information from a diagram or modifying/creating objects in a diagram in specified ways according to problem solving needs. the perceptions and actions in most dr systems built so far are hand-coded for the specific application, even when the rest of the system is built using the general architecture. the absence of a general framework for executing perceptions/actions poses as a major hindrance to using them opportunistically -- the essence of open-ended search in problem solving. our goal is to develop a framework for executing a wide variety of specified perceptions and actions across tasks/domains without human intervention. we observe that the domain/task-specific visual perceptions/actions can be transformed into domain/task-independent spatial problems. we specify a spatial problem as a quantified constraint satisfaction problem in the real domain using an open-ended vocabulary of properties, relations and actions involving three kinds of diagrammatic objects -- points, curves, regions. solving a spatial problem from this specification requires computing the equivalent simplified quantifier-free expression, the complexity of which is inherently doubly exponential. we represent objects as configuration of simple elements to facilitate decomposition of complex problems into simpler and similar subproblems. we show that, if the symbolic solution to a subproblem can be expressed concisely, quantifiers can be eliminated from spatial problems in low-order polynomial time using similar previously solved subproblems. this requires determining the similarity of two problems, the existence of a mapping between them computable in polynomial time, and designing a memory for storing previously solved problems so as to facilitate search. the efficacy of the idea is shown by time complexity analysis. we demonstrate the proposed approach by executing perceptions and actions involved in dr tasks in two army applications.",2010
"nominals, inverses, counting, and conjunctive queries or: why infinity is your friend!","description logics are knowledge representation formalisms that provide, for example, the logical underpinning of the w3c owl standards. conjunctive queries, the standard query language in databases, have recently gained significant attention as an expressive formalism for querying description logic knowledge bases. several different techniques for deciding conjunctive query entailment are available for a wide range of dls. nevertheless, the combination of nominals, inverse roles, and number restrictions in owl 1 and owl 2 dl causes unsolvable problems for the techniques hitherto available. we tackle this problem and present a decidability result for entailment of unions of conjunctive queries in the dl alchoiqb that contains all three problematic constructors simultaneously. provided that queries contain only simple roles, our result also shows decidability of entailment of (unions of) conjunctive queries in the logic that underpins owl 1 dl and we believe that the presented results will pave the way for further progress towards conjunctive query entailment decision procedures for the description logics underlying the owl standards.",2010
kalman temporal differences,"because reinforcement learning suffers from a lack of scalability, online value (and q-) function approximation has received increasing interest this last decade. this contribution introduces a novel approximation scheme, namely the kalman temporal differences (ktd) framework, that exhibits the following features: sample-efficiency, non-linear approximation, non-stationarity handling and uncertainty management. a first ktd-based algorithm is provided for deterministic markov decision processes (mdp) which produces biased estimates in the case of stochastic transitions. than the extended ktd framework (xktd), solving stochastic mdp, is described. convergence is analyzed for special cases for both deterministic and stochastic transitions. related algorithms are experimented on classical benchmarks. they compare favorably to the state of the art while exhibiting the announced features.",2010
theta*: any-angle path planning on grids,"grids with blocked and unblocked cells are often used to represent terrain in robotics and video games. however, paths formed by grid edges can be longer than true shortest paths in the terrain since their headings are artificially constrained. we present two new correct and complete any-angle path-planning algorithms that avoid this shortcoming. basic theta* and angle-propagation theta* are both variants of a* that propagate information along grid edges without constraining paths to grid edges. basic theta* is simple to understand and implement, fast and finds short paths. however, it is not guaranteed to find true shortest paths. angle-propagation theta* achieves a better worst-case complexity per vertex expansion than basic theta* by propagating angle ranges when it expands vertices, but is more complex, not as fast and finds slightly longer paths. we refer to basic theta* and angle-propagation theta* collectively as theta*. theta* has unique properties, which we analyze in detail. we show experimentally that it finds shorter paths than both a* with post-smoothed paths and field d* (the only other version of a* we know of that propagates information along grid edges without constraining paths to grid edges) with a runtime comparable to that of a* on grids. finally, we extend theta* to grids that contain unblocked cells with non-uniform traversal costs and introduce variants of theta* which provide different tradeoffs between path length and runtime.",2010
which clustering do you want? inducing your ideal clustering with minimal feedback,"while traditional research on text clustering has largely focused on grouping documents by topic, it is conceivable that a user may want to cluster documents along other dimensions, such as the author's mood, gender, age, or sentiment. without knowing the user's intention, a clustering algorithm will only group documents along the most prominent dimension, which may not be the one the user desires. to address the problem of clustering documents along the user-desired dimension, previous work has focused on learning a similarity metric from data manually annotated with the user's intention or having a human construct a feature space in an interactive manner during the clustering process. with the goal of reducing reliance on human knowledge for fine-tuning the similarity function or selecting the relevant features required by these approaches, we propose a novel active clustering algorithm, which allows a user to easily select the dimension along which she wants to cluster the documents by inspecting only a small number of words. we demonstrate the viability of our algorithm on a variety of commonly-used sentiment datasets.",2010
a utility-theoretic approach to privacy in online services,"online offerings such as web search, news portals, and e-commerce applications face the challenge of providing high-quality service to a large, heterogeneous user base. recent efforts have highlighted the potential to improve performance by introducing methods to personalize services based on special knowledge about users and their context. for example, a user's demographics, location, and past search and browsing may be useful in enhancing the results offered in response to web search queries. however, reasonable concerns about privacy by both users, providers, and government agencies acting on behalf of citizens, may limit access by services to such information. we introduce and explore an economics of privacy in personalization, where people can opt to share personal information, in a standing or on-demand manner, in return for expected enhancements in the quality of an online service. we focus on the example of web search and formulate realistic objective functions for search efficacy and privacy. we demonstrate how we can find a provably near-optimal optimization of the utility-privacy tradeoff in an efficient manner. we evaluate our methodology on data drawn from a log of the search activity of volunteer participants. we separately assess users preferences about privacy and utility via a large-scale survey, aimed at eliciting preferences about peoples willingness to trade the sharing of personal data in returns for gains in search efficiency. we show that a significant level of personalization can be achieved using a relatively small amount of information about users.",2010
an effective algorithm for and phase transitions of the directed hamiltonian cycle problem,"the hamiltonian cycle problem (hcp) is an important combinatorial problem with applications in many areas. it is among the first problems used for studying intrinsic properties, including phase transitions, of combinatorial problems. while thorough theoretical and experimental analyses have been made on the hcp in undirected graphs, a limited amount of work has been done for the hcp in directed graphs (dhcp). the main contribution of this work is an effective algorithm for the dhcp. our algorithm explores and exploits the close relationship between the dhcp and the assignment problem (ap) and utilizes a technique based on boolean satisfiability (sat). by combining effective algorithms for the ap and sat, our algorithm significantly outperforms previous exact dhcp algorithms, including an algorithm based on the award-winning concorde tsp algorithm. the second result of the current study is an experimental analysis of phase transitions of the dhcp, verifying and refining a known phase transition of the dhcp.",2010
best-first heuristic search for multicore machines,"to harness modern multicore processors, it is imperative to develop parallel versions of fundamental algorithms. in this paper, we compare different approaches to parallel best-first search in a shared-memory setting. we present a new method, pbnf, that uses abstraction to partition the state space and to detect duplicate states without requiring frequent locking. pbnf allows speculative expansions when necessary to keep threads busy. we identify and fix potential livelock conditions in our approach, proving its correctness using temporal logic. our approach is general, allowing it to extend easily to suboptimal and anytime heuristic search. in an empirical comparison on strips planning, grid pathfinding, and sliding tile puzzle problems using 8-core machines, we show that a*, weighted a* and anytime weighted a* implemented using pbnf yield faster search than improved versions of previous parallel search proposals.",2010
intrusion detection using continuous time bayesian networks,"intrusion detection systems (idss) fall into two high-level categories: network-based systems (nids) that monitor network behaviors, and host-based systems (hids) that monitor system calls. in this work, we present a general technique for both systems. we use anomaly detection, which identifies patterns not conforming to a historic norm. in both types of systems, the rates of change vary dramatically over time (due to burstiness) and over components (due to service difference). to efficiently model such systems, we use continuous time bayesian networks (ctbns) and avoid specifying a fixed update interval common to discrete-time models. we build generative models from the normal training data, and abnormal behaviors are flagged based on their likelihood under this norm. for nids, we construct a hierarchical ctbn model for the network packet traces and use rao-blackwellized particle filtering to learn the parameters. we illustrate the power of our method through experiments on detecting real worms and identifying hosts on two publicly available network traces, the mawi dataset and the lbnl dataset. for hids, we develop a novel learning method to deal with the finite resolution of system log file time stamps, without losing the benefits of our continuous time model. we demonstrate the method by detecting intrusions in the darpa 1998 bsm dataset.",2010
using local alignments for relation recognition,"this paper discusses the problem of marrying structural similarity with semantic relatedness for information extraction from text. aiming at accurate recognition of relations, we introduce local alignment kernels and explore various possibilities of using them for this task. we give a definition of a local alignment (la) kernel based on the smith-waterman score as a sequence similarity measure and proceed with a range of possibilities for computing similarity between elements of sequences. we show how distributional similarity measures obtained from unlabeled data can be incorporated into the learning task as semantic knowledge. our experiments suggest that the la kernel yields promising results on various biomedical corpora outperforming two baselines by a large margin. additional series of experiments have been conducted on the data sets of seven general relation types, where the performance of the la kernel is comparable to the current state-of-the-art results.",2010
change in abstract argumentation frameworks: adding an argument,"in this paper, we address the problem of change in an abstract argumentation system. we focus on a particular change: the addition of a new argument which interacts with previous arguments. we study the impact of such an addition on the outcome of the argumentation system, more particularly on the set of its extensions. several properties for this change operation are defined by comparing the new set of extensions to the initial one, these properties are called structural when the comparisons are based on set-cardinality or set-inclusion relations. several other properties are proposed where comparisons are based on the status of some particular arguments: the accepted arguments; these properties refer to the evolution of this status during the change, e.g., monotony and priority to recency. all these properties may be more or less desirable according to specific applications. they are studied under two particular semantics: the grounded and preferred semantics.",2010
bnb-adopt: an asynchronous branch-and-bound dcop algorithm,"distributed constraint optimization (dcop) problems are a popular way of formulating and solving agent-coordination problems. a dcop problem is a problem where several agents coordinate their values such that the sum of the resulting constraint costs is minimal. it is often desirable to solve dcop problems with memory-bounded and asynchronous algorithms. we introduce branch-and-bound adopt (bnb-adopt), a memory-bounded asynchronous dcop search algorithm that uses the message-passing and communication framework of adopt (modi, shen, tambe, & yokoo, 2005), a well known memory-bounded asynchronous dcop search algorithm, but changes the search strategy of adopt from best-first search to depth-first branch-and-bound search. our experimental results show that bnb-adopt finds cost-minimal solutions up to one order of magnitude faster than adopt for a variety of large dcop problems and is as fast as ncbb, a memory-bounded synchronous dcop search algorithm, for most of these dcop problems. additionally, it is often desirable to find bounded-error solutions for dcop problems within a reasonable amount of time since finding cost-minimal solutions is np-hard. the existing bounded-error approximation mechanism allows users only to specify an absolute error bound on the solution cost but a relative error bound is often more intuitive. thus, we present two new bounded-error approximation mechanisms that allow for relative error bounds and implement them on top of bnb-adopt.",2010
a survey of paraphrasing and textual entailment methods,"paraphrasing methods recognize, generate, or extract phrases, sentences, or longer natural language expressions that convey almost the same information. textual entailment methods, on the other hand, recognize, generate, or extract pairs of natural language expressions, such that a human who reads (and trusts) the first element of a pair would most likely infer that the other element is also true. paraphrasing can be seen as bidirectional textual entailment and methods from the two areas are often similar. both kinds of methods are useful, at least in principle, in a wide range of natural language processing applications, including question answering, summarization, text generation, and machine translation. we summarize key ideas from the two areas by considering in turn recognition, generation, and extraction methods, also pointing to prominent articles and resources.",2010
"constructing reference sets from unstructured, ungrammatical text","vast amounts of text on the web are unstructured and ungrammatical, such as classified ads, auction listings, forum postings, etc. we call such text posts. despite their inconsistent structure and lack of grammar, posts are full of useful information. this paper presents work on semi-automatically building tables of relational information, called reference sets, by analyzing such posts directly. reference sets can be applied to a number of tasks such as ontology maintenance and information extraction. our reference-set construction method starts with just a small amount of background knowledge, and constructs tuples representing the entities in the posts to form a reference set. we also describe an extension to this approach for the special case where even this small amount of background knowledge is impossible to discover and use. to evaluate the utility of the machine-constructed reference sets, we compare them to manually constructed reference sets in the context of reference-set-based information extraction. our results show the reference sets constructed by our method outperform manually constructed reference sets. we also compare the reference-set-based extraction approach using the machine-constructed reference set to supervised extraction approaches using generic features. these results demonstrate that using machine-constructed reference sets outperforms the supervised methods, even though the supervised methods require training data.",2010
grounding fo and fo(id) with bounds,"grounding is the task of reducing a first-order theory and finite domain to an equivalent propositional theory. it is used as preprocessing phase in many logic-based reasoning systems. such systems provide a rich first-order input language to a user and can rely on efficient propositional solvers to perform the actual reasoning. besides a first-order theory and finite domain, the input for grounders contains in many applications also additional data. by exploiting this data, the size of the grounder's output can often be reduced significantly. a common practice to improve the efficiency of a grounder in this context is by manually adding semantically redundant information to the input theory, indicating where and when the grounder should exploit the data. in this paper we present a method to compute and add such redundant information automatically. our method therefore simplifies the task of writing input theories that can be grounded efficiently by current systems. we first present our method for classical first-order logic (fo) theories. then we extend it to fo(id), the extension of fo with inductive definitions, which allows for more concise and comprehensive input theories. we discuss implementation issues and experimentally validate the practical applicability of our method.",2010
developing approaches for solving a telecommunications feature subscription problem,"call control features (e.g., call-divert, voice-mail) are primitive options to which users can subscribe off-line to personalise their service. the configuration of a feature subscription involves choosing and sequencing features from a catalogue and is subject to constraints that prevent undesirable feature interactions at run-time. when the subscription requested by a user is inconsistent, one problem is to find an optimal relaxation, which is a generalisation of the feedback vertex set problem on directed graphs, and thus it is an np-hard task. we present several constraint programming formulations of the problem. we also present formulations using partial weighted maximum boolean satisfiability and mixed integer linear programming. we study all these formulations by experimentally comparing them on a variety of randomly generated instances of the feature subscription problem.",2010
fast set bounds propagation using a bdd-sat hybrid,"binary decision diagram (bdd) based set bounds propagation is a powerful approach to solving set-constraint satisfaction problems. however, prior bdd based techniques in- cur the significant overhead of constructing and manipulating graphs during search. we present a set-constraint solver which combines bdd-based set-bounds propagators with the learning abilities of a modern sat solver. together with a number of improvements beyond the basic algorithm, this solver is highly competitive with existing propagation based set constraint solvers.",2010
mixed strategies in combinatorial agency,"in many multiagent domains a set of agents exert effort towards a joint outcome, yet the individual effort levels cannot be easily observed. a typical example for such a scenario is routing in communication networks, where the sender can only observe whether the packet reached its destination, but often has no information about the actions of the intermediate routers, which influences the final outcome. we study a setting where a principal needs to motivate a team of agents whose combination of hidden efforts stochastically determines an outcome. in a companion paper we devise and study a basic ''combinatorial agency'' model for this setting, where the principal is restricted to inducing a pure nash equilibrium. here we study various implications of this restriction. first, we show that, in contrast to the case of observable efforts, inducing a mixed-strategies equilibrium may be beneficial for the principal. second, we present a sufficient condition for technologies for which no gain can be generated. third, we bound the principal's gain for various families of technologies. finally, we study the robustness of mixed equilibria to coalitional deviations and the computational hardness of the optimal mixed equilibria.",2010
approximate model-based diagnosis using greedy stochastic search,"we propose a stochastic fault diagnosis algorithm, called safari, which trades off guarantees of computing minimal diagnoses for computational efficiency. we empirically demonstrate, using the 74xxx and iscas-85 suites of benchmark combinatorial circuits, that safari achieves several orders-of-magnitude speedup over two well-known deterministic algorithms, cda* and ha*, for multiple-fault diagnoses; further, safari can compute a range of multiple-fault diagnoses that cda* and ha* cannot. we also prove that safari is optimal for a range of propositional fault models, such as the widely-used weak-fault models (models with ignorance of abnormal behavior). we discuss the optimality of safari in a class of strong-fault circuit models with stuck-at failure modes. by modeling the algorithm itself as a markov chain, we provide exact bounds on the minimality of the diagnosis computed. safari also displays strong anytime behavior, and will return a diagnosis after any non-trivial inference time.",2010
resource-driven mission-phasing techniques for constrained agents in stochastic environments,"because an agent's resources dictate what actions it can possibly take, it should plan which resources it holds over time carefully, considering its inherent limitations (such as power or payload restrictions), the competing needs of other agents for the same resources, and the stochastic nature of the environment. such agents can, in general, achieve more of their objectives if they can use --- and even create --- opportunities to change which resources they hold at various times. driven by resource constraints, the agents could break their overall missions into an optimal series of phases, optimally reconfiguring their resources at each phase, and optimally using their assigned resources in each phase, given their knowledge of the stochastic environment. in this paper, we formally define and analyze this constrained, sequential optimization problem in both the single-agent and multi-agent contexts. we present a family of mixed integer linear programming (milp) formulations of this problem that can optimally create phases (when phases are not predefined) accounting for costs and limitations in phase creation. because our formulations multaneously also find the optimal allocations of resources at each phase and the optimal policies for using the allocated resources at each phase, they exploit structure across these coupled problems. this allows them to find solutions significantly faster(orders of magnitude faster in larger problems) than alternative solution techniques, as we demonstrate empirically.",2010
a minimum relative entropy principle for learning and acting,"this paper proposes a method to construct an adaptive agent that is universal with respect to a given class of experts, where each expert is designed specifically for a particular environment. this adaptive control problem is formalized as the problem of minimizing the relative entropy of the adaptive agent from the expert that is most suitable for the unknown environment. if the agent is a passive observer, then the optimal solution is the well-known bayesian predictor. however, if the agent is active, then its past actions need to be treated as causal interventions on the i/o stream rather than normal probability conditions. here it is shown that the solution to this new variational problem is given by a stochastic controller called the bayesian control rule, which implements adaptive behavior as a mixture of experts. furthermore, it is shown that under mild assumptions, the bayesian control rule converges to the control law of the most suitable expert.",2010
algorithms for closed under rational behavior (curb) sets,"we provide a series of algorithms demonstrating that solutions according to the fundamental game-theoretic solution concept of closed under rational behavior (curb) sets in two-player, normal-form games can be computed in polynomial time (we also discuss extensions to n-player games). first, we describe an algorithm that identifies all of a players best responses conditioned on the belief that the other player will play from within a given subset of its strategy space. this algorithm serves as a subroutine in a series of polynomial-time algorithms for finding all minimal curb sets, one minimal curb set, and the smallest minimal curb set in a game. we then show that the complexity of finding a nash equilibrium can be exponential only in the size of a games smallest curb set. related to this, we show that the smallest curb set can be an arbitrarily small portion of the game, but it can also be arbitrarily larger than the supports of its only enclosed nash equilibrium. we test our algorithms empirically and find that most commonly studied academic games tend to have either very large or very small minimal curb sets.",2010
logical foundations of rdf(s) with datatypes,"the resource description framework (rdf) is a semantic web standard that provides a data language, simply called rdf, as well as a lightweight ontology language, called rdf schema. we investigate embeddings of rdf in logic and show how standard logic programming and description logic technology can be used for reasoning with rdf. we subsequently consider extensions of rdf with datatype support, considering d entailment, defined in the rdf semantics specification, and d* entailment, a semantic weakening of d entailment, introduced by ter horst. we use the embeddings and properties of the logics to establish novel upper bounds for the complexity of deciding entailment. we subsequently establish two novel lower bounds, establishing that rdfs entailment is ptime-complete and that simple-d entailment is conp-hard, when considering arbitrary datatypes, both in the size of the entailing graph. the results indicate that rdfs may not be as lightweight as one may expect.",2010
cause identification from aviation safety incident reports via weakly supervised semantic lexicon construction,"the aviation safety reporting system collects voluntarily submitted reports on aviation safety incidents to facilitate research work aiming to reduce such incidents. to effectively reduce these incidents, it is vital to accurately identify why these incidents occurred. more precisely, given a set of possible causes, or shaping factors, this task of cause identification involves identifying all and only those shaping factors that are responsible for the incidents described in a report. we investigate two approaches to cause identification. both approaches exploit information provided by a semantic lexicon, which is automatically constructed via thelen and riloff's basilisk framework augmented with our linguistic and algorithmic modifications. the first approach labels a report using a simple heuristic, which looks for the words and phrases acquired during the semantic lexicon learning process in the report. the second approach recasts cause identification as a text classification problem, employing supervised and transductive text classification algorithms to learn models from incident reports labeled with shaping factors and using the models to label unseen reports. our experiments show that both the heuristic-based approach and the learning-based approach (when given sufficient training data) outperform the baseline system significantly.",2010
non-transferable utility coalitional games via mixed-integer linear constraints,"coalitional games serve the purpose of modeling payoff distribution problems in scenarios where agents can collaborate by forming coalitions in order to obtain higher worths than by acting in isolation. in the classical transferable utility (tu) setting, coalition worths can be freely distributed amongst agents. however, in several application scenarios, this is not the case and the non-transferable utility setting (ntu) must be considered, where additional application-oriented constraints are imposed on the possible worth distributions. in this paper, an approach to define ntu games is proposed which is based on describing allowed distributions via a set of mixed-integer linear constraints applied to an underlying tu game. it is shown that such games allow non-transferable conditions on worth distributions to be specified in a natural and succinct way. the properties and the relationships among the most prominent solution concepts for ntu games that hold when they are applied on (mixed-integer) constrained games are investigated. finally, a thorough analysis is carried out to assess the impact of issuing constraints on the computational complexity of some of these solution concepts.",2010
automatic induction of bellman-error features for probabilistic planning,"domain-specific features are important in representing problem structure throughout machine learning and decision-theoretic planning. in planning, once state features are provided, domain-independent algorithms such as approximate value iteration can learn weighted combinations of those features that often perform well as heuristic estimates of state value (e.g., distance to the goal). successful applications in real-world domains often require features crafted by human experts. here, we propose automatic processes for learning useful domain-specific feature sets with little or no human intervention. our methods select and add features that describe state-space regions of high inconsistency in the bellman equation (statewise bellman error) during approximate value iteration. our method can be applied using any real-valued-feature hypothesis space and corresponding learning method for selecting features from training sets of state-value pairs. we evaluate the method with hypothesis spaces defined by both relational and propositional feature languages, using nine probabilistic planning domains. we show that approximate value iteration using a relational feature space performs at the state-of-the-art in domain-independent stochastic relational planning. our method provides the first domain-independent approach that plays tetris successfully (without human-engineered features).",2010
text relatedness based on a word thesaurus,"the computation of relatedness between two fragments of text in an automated manner requires taking into account a wide range of factors pertaining to the meaning the two fragments convey, and the pairwise relations between their words. without doubt, a measure of relatedness between text segments must take into account both the lexical and the semantic relatedness between words. such a measure that captures well both aspects of text relatedness may help in many tasks, such as text retrieval, classification and clustering. in this paper we present a new approach for measuring the semantic relatedness between words based on their implicit semantic links. the approach exploits only a word thesaurus in order to devise implicit semantic links between words. based on this approach, we introduce omiotis, a new measure of semantic relatedness between texts which capitalizes on the word-to-word semantic relatedness measure (sr) and extends it to measure the relatedness between texts. we gradually validate our method: we first evaluate the performance of the semantic relatedness measure between individual words, covering word-to-word similarity and relatedness, synonym identification and word analogy; then, we proceed with evaluating the performance of our method in measuring text-to-text semantic relatedness in two tasks, namely sentence-to-sentence similarity and paraphrase recognition. experimental evaluation shows that the proposed method outperforms every lexicon-based method of semantic relatedness in the selected tasks and the used data sets, and competes well against corpus-based and hybrid approaches.",2010
predicting the performance of ida* using conditional distributions,"korf, reid, and edelkamp introduced a formula to predict the number of nodes ida* will expand on a single iteration for a given consistent heuristic, and experimentally demonstrated that it could make very accurate predictions. in this paper we show that, in addition to requiring the heuristic to be consistent, their formula's predictions are accurate only at levels of the brute-force search tree where the heuristic values obey the unconditional distribution that they defined and then used in their formula. we then propose a new formula that works well without these requirements, i.e., it can make accurate predictions of ida*'s performance for inconsistent heuristics and if the heuristic values in any level do not obey the unconditional distribution. in order to achieve this we introduce the conditional distribution of heuristic values which is a generalization of their unconditional heuristic distribution. we also provide extensions of our formula that handle individual start states and the augmentation of ida* with bidirectional pathmax (bpmx), a technique for propagating heuristic values when inconsistent heuristics are used. experimental results demonstrate the accuracy of our new method and all its variations.",2010
mechanisms for multi-unit auctions,"we present an incentive-compatible polynomial-time approximation scheme for multi-unit auctions with general k-minded player valuations. the mechanism fully optimizes over an appropriately chosen sub-range of possible allocations and then uses vcg payments over this sub-range. we show that obtaining a fully polynomial-time incentive-compatible approximation scheme, at least using vcg payments, is np-hard. for the case of valuations given by black boxes, we give a polynomial-time incentive-compatible 2-approximation mechanism and show that no better is possible, at least using vcg payments.",2010
interactive cost configuration over decision diagrams,"in many ai domains such as product configuration, a user should interactively specify a solution that must satisfy a set of constraints. in such scenarios, offline compilation of feasible solutions into a tractable representation is an important approach to delivering efficient backtrack-free user interaction online. in particular,binary decision diagrams (bdds) have been successfully used as a compilation target for product and service configuration. in this paper we discuss how to extend bdd-based configuration to scenarios involving cost functions which express user preferences. we first show that an efficient, robust and easy to implement extension is possible if the cost function is additive, and feasible solutions are represented using multi-valued decision diagrams (mdds). we also discuss the effect on mdd size if the cost function is non-additive or if it is encoded explicitly into mdd. we then discuss interactive configuration in the presence of multiple cost functions. we prove that even in its simplest form, multiple-cost configuration is np-hard in the input mdd. however, for solving two-cost configuration we develop a pseudo-polynomial scheme and a fully polynomial approximation scheme. the applicability of our approach is demonstrated through experiments over real-world configuration models and product-catalogue datasets. response times are generally within a fraction of a second even for very large instances.",2010
from frequency to meaning: vector space models of semantics,"computers understand very little of the meaning of human language. this profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. vector space models (vsms) of semantics are beginning to address these limits. this paper surveys the use of vsms for semantic processing of text. we organize the literature on vsms according to the structure of the matrix in a vsm. there are currently three broad classes of vsms, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. we survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. our goal in this survey is to show the breadth of applications of vsms for semantics, to provide a new perspective on vsms for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.",2010
on action theory change,"as historically acknowledged in the reasoning about actions and change community, intuitiveness of a logical domain description cannot be fully automated. moreover, like any other logical theory, action theories may also evolve, and thus knowledge engineers need revision methods to help in accommodating new incoming information about the behavior of actions in an adequate manner. the present work is about changing action domain descriptions in multimodal logic. its contribution is threefold: first we revisit the semantics of action theory contraction proposed in previous work, giving more robust operators that express minimal change based on a notion of distance between kripke-models. second we give algorithms for syntactical action theory contraction and establish their correctness with respect to our semantics for those action theories that satisfy a principle of modularity investigated in previous work. since modularity can be ensured for every action theory and, as we show here, needs to be computed at most once during the evolution of a domain description, it does not represent a limitation at all to the method here studied. finally we state agm-like postulates for action theory contraction and assess the behavior of our operators with respect to them. moreover, we also address the revision counterpart of action theory change, showing that it benefits from our semantics for contraction.",2010
context-based word acquisition for situated dialogue in a virtual world,"to tackle the vocabulary problem in conversational systems, previous work has applied unsupervised learning approaches on co-occurring speech and eye gaze during interaction to automatically acquire new words. although these approaches have shown promise, several issues related to human language behavior and human-machine conversation have not been addressed. first, psycholinguistic studies have shown certain temporal regularities between human eye movement and language production. while these regularities can potentially guide the acquisition process, they have not been incorporated in the previous unsupervised approaches. second, conversational systems generally have an existing knowledge base about the domain and vocabulary. while the existing knowledge can potentially help bootstrap and constrain the acquired new words, it has not been incorporated in the previous models. third, eye gaze could serve different functions in human-machine conversation. some gaze streams may not be closely coupled with speech stream, and thus are potentially detrimental to word acquisition. automated recognition of closely-coupled speech-gaze streams based on conversation context is important. to address these issues, we developed new approaches that incorporate user language behavior, domain knowledge, and conversation context in word acquisition. we evaluated these approaches in the context of situated dialogue in a virtual world. our experimental results have shown that incorporating the above three types of contextual information significantly improves word acquisition performance.",2010
join-graph propagation algorithms,"the paper investigates parameterized approximate message-passing schemes that are based on bounded inference and are inspired by pearls belief propagation algorithm (bp). we start with the bounded inference mini-clustering algorithm and then move to the iterative scheme called iterative join-graph propagation (ijgp), that combines both iteration and bounded inference. algorithm ijgp belongs to the class of generalized belief propagation algorithms, a framework that allowed connections with approximate algorithms from statistical physics and is shown empirically to surpass the performance of mini-clustering and belief propagation, as well as a number of other state-of-the-art algorithms on several classes of networks. we also provide insight into the accuracy of iterative bp and ijgp by relating these algorithms to well known classes of constraint propagation schemes.",2010
an investigation into mathematical programming for finite horizon decentralized pomdps,"decentralized planning in uncertain environments is a complex task generally dealt with by using a decision-theoretic approach, mainly through the framework of decentralized partially observable markov decision processes (dec-pomdps). although dec-pomdps are a general and powerful modeling tool, solving them is a task with an overwhelming complexity that can be doubly exponential. in this paper, we study an alternate formulation of dec-pomdps relying on a sequence-form representation of policies. from this formulation, we show how to derive mixed integer linear programming (milp) problems that, once solved, give exact optimal solutions to the dec-pomdps. we show that these milps can be derived either by using some combinatorial characteristics of the optimal solutions of the dec-pomdps or by using concepts borrowed from game theory. through an experimental validation on classical test problems from the dec-pomdp literature, we compare our approach to existing algorithms. results show that mathematical programming outperforms dynamic programming but is less efficient than forward search, except for some particular problems. the main contributions of this work are the use of mathematical programming for dec-pomdps and a better understanding of dec-pomdps and of their solutions. besides, we argue that our alternate representation of dec-pomdps could be helpful for designing novel algorithms looking for approximate solutions to dec-pomdps.",2010
training a multilingual sportscaster: using perceptual context to learn language,we present a novel framework for learning to interpret and generate language using only perceptual context as supervision. we demonstrate its capabilities by developing a system that learns to sportscast simulated robot soccer games in both english and korean without any language-specific prior knowledge. training employs only ambiguous supervision consisting of a stream of descriptive textual comments and a sequence of events extracted from the simulation trace. the system simultaneously establishes correspondences between individual comments and the events that they describe while building a translation model that supports both parsing and generation. we also present a novel algorithm for learning which events are worth describing. human evaluations of the generated commentaries indicate they are of reasonable quality and in some cases even on par with those produced by humans for our limited domain.,2010
reasoning about the transfer of control,"we present dcl-pc: a logic for reasoning about how the abilities of agents and coalitions of agents are altered by transferring control from one agent to another. the logical foundation of dcl-pc is cl-pc, a logic for reasoning about cooperation in which the abilities of agents and coalitions of agents stem from a distribution of atomic boolean variables to individual agents -- the choices available to a coalition correspond to assignments to the variables the coalition controls. the basic modal constructs of dcl-pc are of the form `coalition c can cooperate to bring about phi'. dcl-pc extends cl-pc with dynamic logic modalities in which atomic programs are of the form `agent i gives control of variable p to agent j'; as usual in dynamic logic, these atomic programs may be combined using sequence, iteration, choice, and test operators to form complex programs. by combining such dynamic transfer programs with cooperation modalities, it becomes possible to reason about how the power of agents and coalitions is affected by the transfer of control. we give two alternative semantics for the logic: a `direct' semantics, in which we capture the distributions of boolean variables to agents; and a more conventional kripke semantics. we prove that these semantics are equivalent, and then present an axiomatization for the logic. we investigate the computational complexity of model checking and satisfiability for dcl-pc, and show that both problems are pspace-complete (and hence no worse than the underlying logic cl-pc). finally, we investigate the characterisation of control in dcl-pc. we distinguish between first-order control -- the ability of an agent or coalition to control some state of affairs through the assignment of values to the variables under the control of the agent or coalition -- and second-order control -- the ability of an agent to exert control over the control that other agents have by transferring variables to other agents. we give a logical characterisation of second-order control.",2010
multiattribute auctions based on generalized additive independence,"we develop multiattribute auctions that accommodate generalized additive independent (gai) preferences. we propose an iterative auction mechanism that maintains prices on potentially overlapping gai clusters of attributes, thus decreases elicitation and computational burden, and creates an open competition among suppliers over a multidimensional domain. most significantly, the auction is guaranteed to achieve surplus which approximates optimal welfare up to a small additive factor, under reasonable equilibrium strategies of traders. the main departure of gai auctions from previous literature is to accommodate non-additive trader preferences, hence allowing traders to condition their evaluation of specific attributes on the value of other attributes. at the same time, the gai structure supports a compact representation of prices, enabling a tractable auction process. we perform a simulation study, demonstrating and quantifying the significant efficiency advantage of more expressive preference modeling. we draw random gai-structured utility functions with various internal structures, generate additive functions that approximate the gai utility, and compare the performance of the auctions using the two representations. we find that allowing traders to express existing dependencies among attributes improves the economic efficiency of multiattribute auctions.",2010
the dl-lite family and relations,"the recently introduced series of description logics under the common moniker `dl-lite' has attracted attention of the description logic and semantic web communities due to the low computational complexity of inference, on the one hand, and the ability to represent conceptual modeling formalisms, on the other. the main aim of this article is to carry out a thorough and systematic investigation of inference in extensions of the original dl-lite logics along five axes: by (i) adding the boolean connectives and (ii) number restrictions to concept constructs, (iii) allowing role hierarchies, (iv) allowing role disjointness, symmetry, asymmetry, reflexivity, irreflexivity and transitivity constraints, and (v) adopting or dropping the unique same assumption. we analyze the combined complexity of satisfiability for the resulting logics, as well as the data complexity of instance checking and answering positive existential queries. our approach is based on embedding dl-lite logics in suitable fragments of the one-variable first-order logic, which provides useful insights into their properties and, in particular, computational behavior.",2009
prime implicates and prime implicants: from propositional to modal logic,"prime implicates and prime implicants have proven relevant to a number of areas of artificial intelligence, most notably abductive reasoning and knowledge compilation. the purpose of this paper is to examine how these notions might be appropriately extended from propositional logic to the modal logic k. we begin the paper by considering a number of potential definitions of clauses and terms for k. the different definitions are evaluated with respect to a set of syntactic, semantic, and complexity-theoretic properties characteristic of the propositional definition. we then compare the definitions with respect to the properties of the notions of prime implicates and prime implicants that they induce. while there is no definition that perfectly generalizes the propositional notions, we show that there does exist one definition which satisfies many of the desirable properties of the propositional case. in the second half of the paper, we consider the computational properties of the selected definition. to this end, we provide sound and complete algorithms for generating and recognizing prime implicates, and we show the prime implicate recognition task to be pspace-complete. we also prove upper and lower bounds on the size and number of prime implicates. while the paper focuses on the logic k, all of our results hold equally well for multi-modal k and for concept expressions in the description logic alc.",2009
content modeling using latent permutations,"we present a novel bayesian topic model for learning discourse-level document structure. our model leverages insights from discourse theory to constrain latent topic assignments in a way that reflects the underlying organization of document topics. we propose a global model in which both topic selection and ordering are biased to be similar across a collection of related documents. we show that this space of orderings can be effectively represented using a distribution over permutations called the generalized mallows model. we apply our method to three complementary discourse-level tasks: cross-document alignment, document segmentation, and information ordering. our experiments show that incorporating our permutation-based model in these applications yields substantial improvements in performance over previously proposed methods.",2009
hypertableau reasoning for description logics,"we present a novel reasoning calculus for the description logic shoiq^+---a knowledge representation formalism with applications in areas such as the semantic web. unnecessary nondeterminism and the construction of large models are two primary sources of inefficiency in the tableau-based reasoning calculi used in state-of-the-art reasoners. in order to reduce nondeterminism, we base our calculus on hypertableau and hyperresolution calculi, which we extend with a blocking condition to ensure termination. in order to reduce the size of the constructed models, we introduce anywhere pairwise blocking. we also present an improved nominal introduction rule that ensures termination in the presence of nominals, inverse roles, and number restrictions---a combination of dl constructs that has proven notoriously difficult to handle. our implementation shows significant performance improvements over state-of-the-art reasoners on several well-known ontologies.",2009
relaxed survey propagation for the weighted maximum satisfiability problem,"the survey propagation (sp) algorithm has been shown to work well on large instances of the random 3-sat problem near its phase transition. it was shown that sp estimates marginals over covers that represent clusters of solutions. the sp-y algorithm generalizes sp to work on the maximum satisfiability (max-sat) problem, but the cover interpretation of sp does not generalize to sp-y. in this paper, we formulate the relaxed survey propagation (rsp) algorithm, which extends the sp algorithm to apply to the weighted max-sat problem. we show that rsp has an interpretation of estimating marginals over covers violating a set of clauses with minimal weight. this naturally generalizes the cover interpretation of sp. empirically, we show that rsp outperforms sp-y and other state-of-the-art max-sat solvers on random max-sat instances. rsp also outperforms state-of-the-art weighted max-sat solvers on random weighted max-sat instances.",2009
paramils: an automatic algorithm configuration framework,"the identification of performance-optimizing parameter settings is an important part of the development and application of algorithms. we describe an automatic framework for this algorithm configuration problem. more formally, we provide methods for optimizing a target algorithms performance on a given class of problem instances by varying a set of ordinal and/or categorical parameters. we review a family of local-search-based algorithm configuration procedures and present novel techniques for accelerating them by adaptively limiting the time spent for evaluating individual configurations. we describe the results of a comprehensive experimental evaluation of our methods, based on the configuration of prominent complete and incomplete algorithms for sat. we also present what is, to our knowledge, the first published work on automatically configuring the cplex mixed integer programming solver. all the algorithms we considered had default parameter settings that were manually identified with considerable effort. nevertheless, using our automated algorithm configuration procedures, we achieved substantial and consistent performance improvements.",2009
cross-lingual annotation projection for semantic roles,"this article considers the task of automatically inducing role-semantic annotations in the framenet paradigm for new languages. we propose a general framework that is based on annotation projection, phrased as a graph optimization problem. it is relatively inexpensive and has the potential to reduce the human effort involved in creating role-semantic resources. within this framework, we present projection models that exploit lexical and syntactic information. we provide an experimental evaluation on an english-german parallel corpus which demonstrates the feasibility of inducing high-precision german semantic role annotation both for manually and automatically annotated english data.",2009
multilingual part-of-speech tagging: two unsupervised approaches,"we demonstrate the effectiveness of multilingual learning for unsupervised part-of-speech tagging. the central assumption of our work is that by combining cues from multiple languages, the structure of each becomes more apparent. we consider two ways of applying this intuition to the problem of unsupervised part-of-speech tagging: a model that directly merges tag structures for a pair of languages into a single sequence and a second model which instead incorporates multilingual context using latent variables. both approaches are formulated as hierarchical bayesian models, using markov chain monte carlo sampling techniques for inference. our results demonstrate that by incorporating multilingual evidence we can achieve impressive performance gains across a range of scenarios. we also found that performance improves steadily as the number of available languages increases.",2009
approximate strong equilibrium in job scheduling games,"a nash equilibrium (ne) is a strategy profile resilient to unilateral deviations, and is predominantly used in the analysis of multiagent systems. a downside of ne is that it is not necessarily stable against deviations by coalitions. yet, as we show in this paper, in some cases, ne does exhibit stability against coalitional deviations, in that the benefits from a joint deviation are bounded. in this sense, ne approximates strong equilibrium. coalition formation is a key issue in multiagent systems. we provide a framework for quantifying the stability and the performance of various assignment policies and solution concepts in the face of coalitional deviations. within this framework we evaluate a given configuration according to three measures: (i) ir_min: the maximal number alpha, such that there exists a coalition in which the minimal improvement ratio among the coalition members is alpha, (ii) ir_max: the maximal number alpha, such that there exists a coalition in which the maximal improvement ratio among the coalition members is alpha, and (iii) dr_max: the maximal possible damage ratio of an agent outside the coalition. we analyze these measures in job scheduling games on identical machines. in particular, we provide upper and lower bounds for the above three measures for both ne and the well-known assignment rule longest processing time (lpt). our results indicate that lpt performs better than a general ne. however, lpt is not the best possible approximation. in particular, we present a polynomial time approximation scheme (ptas) for the makespan minimization problem which provides a schedule with ir_min of 1+epsilon for any given epsilon. with respect to computational complexity, we show that given an ne on m >= 3 identical machines or m >= 2 unrelated machines, it is np-hard to determine whether a given coalition can deviate such that every member decreases its cost.",2009
friends or foes? on planning as satisfiability and abstract cnf encodings,"planning as satisfiability, as implemented in, for instance, the satplan tool, is a highly competitive method for finding parallel step-optimal plans. a bottleneck in this approach is to *prove the absence* of plans of a certain length. specifically, if the optimal plan has n steps, then it is typically very costly to prove that there is no plan of length n-1. we pursue the idea of leading this proof within solution length preserving abstractions (over-approximations) of the original planning task. this is promising because the abstraction may have a much smaller state space; related methods are highly successful in model checking. in particular, we design a novel abstraction technique based on which one can, in several widely used planning benchmarks, construct abstractions that have exponentially smaller state spaces while preserving the length of an optimal plan. surprisingly, the idea turns out to appear quite hopeless in the context of planning as satisfiability. evaluating our idea empirically, we run experiments on almost all benchmarks of the international planning competitions up to ipc 2004, and find that even hand-made abstractions do not tend to improve the performance of satplan. exploring these findings from a theoretical point of view, we identify an interesting phenomenon that may cause this behavior. we compare various planning-graph based cnf encodings f of the original planning task with the cnf encodings f_abs of the abstracted planning task. we prove that, in many cases, the shortest resolution refutation for f_abs can never be shorter than that for f. this suggests a fundamental weakness of the approach, and motivates further investigation of the interplay between declarative transition-systems, over-approximating abstractions, and sat encodings.",2009
the role of macros in tractable planning,"this paper presents several new tractability results for planning based on macros. we describe an algorithm that optimally solves planning problems in a class that we call inverted tree reducible, and is provably tractable for several subclasses of this class. by using macros to store partial plans that recur frequently in the solution, the algorithm is polynomial in time and space even for exponentially long plans. we generalize the inverted tree reducible class in several ways and describe modifications of the algorithm to deal with these new classes. theoretical results are validated in experiments.",2009
roxybot-06: stochastic prediction and optimization in tac travel,"in this paper, we describe our autonomous bidding agent, roxybot, who emerged victorious in the travel division of the 2006 trading agent competition in a photo finish. at a high level, the design of many successful trading agents can be summarized as follows: (i) price prediction: build a model of market prices; and (ii) optimization: solve for an approximately optimal set of bids, given this model. to predict, roxybot builds a stochastic model of market prices by simulating simultaneous ascending auctions. to optimize, roxybot relies on the sample average approximation method, a stochastic optimization technique.",2009
soft goals can be compiled away,"soft goals extend the classical model of planning with a simple model of preferences. the best plans are then not the ones with least cost but the ones with maximum utility, where the utility of a plan is the sum of the utilities of the soft goals achieved minus the plan cost. finding plans with high utility appears to involve two linked problems: choosing a subset of soft goals to achieve and finding a low-cost plan to achieve them. new search algorithms and heuristics have been developed for planning with soft goals, and a new track has been introduced in the international planning competition (ipc) to test their performance. in this note, we show however that these extensions are not needed: soft goals do not increase the expressive power of the basic model of planning with action costs, as they can easily be compiled away. we apply this compilation to the problems of the net-benefit track of the most recent ipc, and show that optimal and satisficing cost-based planners do better on the compiled problems than optimal and satisficing net-benefit planners on the original problems with explicit soft goals. furthermore, we show that penalties, or negative preferences expressing conditions to avoid, can also be compiled away using a similar idea.",2009
complex question answering: unsupervised learning approaches and experiments,"complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topic-oriented, informative multi-document summarization where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information. in this paper, we experiment with one empirical method and two unsupervised statistical machine learning techniques: k-means and expectation maximization (em), for computing relative importance of the sentences. we compare the results of these approaches. our experiments show that the empirical approach outperforms the other two techniques and em performs better than k-means. however, the performance of these approaches depends entirely on the feature set used and the weighting of these features. in order to measure the importance and relevance to the user query we extract different kinds of features (i.e. lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences. we use a local search technique to learn the weights of the features. to the best of our knowledge, no study has used tree kernel functions to encode syntactic/semantic information for more complex tasks such as computing the relatedness between the query sentences and the document sentences in order to generate query-focused summaries (or answers to complex questions). for each of our methods of generating summaries (i.e. empirical, k-means and em) we show the effects of syntactic and shallow-semantic features over the bag-of-words (bow) features.",2009
"message-based web service composition, integrity constraints, and planning under uncertainty: a new connection","thanks to recent advances, ai planning has become the underlying technique for several applications. figuring prominently among these is automated web service composition (wsc) at the ""capability"" level, where services are described in terms of preconditions and effects over ontological concepts. a key issue in addressing wsc as planning is that ontologies are not only formal vocabularies; they also axiomatize the possible relationships between concepts. such axioms correspond to what has been termed ""integrity constraints"" in the actions and change literature, and applying a web service is essentially a belief update operation. the reasoning required for belief update is known to be harder than reasoning in the ontology itself. the support for belief update is severely limited in current planning tools. our first contribution consists in identifying an interesting special case of wsc which is both significant and more tractable. the special case, which we term ""forward effects"", is characterized by the fact that every ramification of a web service application involves at least one new constant generated as output by the web service. we show that, in this setting, the reasoning required for belief update simplifies to standard reasoning in the ontology itself. this relates to, and extends, current notions of ""message-based"" wsc, where the need for belief update is removed by a strong (often implicit or informal) assumption of ""locality"" of the individual messages. we clarify the computational properties of the forward effects case, and point out a strong relation to standard notions of planning under uncertainty, suggesting that effective tools for the latter can be successfully adapted to address the former. furthermore, we identify a significant sub-case, named ""strictly forward effects"", where an actual compilation into planning under uncertainty exists. this enables us to exploit off-the-shelf planning tools to solve message-based wsc in a general form that involves powerful ontologies, and requires reasoning about partial matches between concepts. we provide empirical evidence that this approach may be quite effective, using conformant-ff as the underlying planner.",2009
trust-based mechanisms for robust and efficient task allocation in the presence of execution uncertainty,"vickrey-clarke-groves (vcg) mechanisms are often used to allocate tasks to selfish and rational agents. vcg mechanisms are incentive compatible, direct mechanisms that are efficient (i.e., maximise social utility) and individually rational (i.e., agents prefer to join rather than opt out). however, an important assumption of these mechanisms is that the agents will ""always"" successfully complete their allocated tasks. clearly, this assumption is unrealistic in many real-world applications, where agents can, and often do, fail in their endeavours. moreover, whether an agent is deemed to have failed may be perceived differently by different agents. such subjective perceptions about an agent's probability of succeeding at a given task are often captured and reasoned about using the notion of ""trust"". given this background, in this paper we investigate the design of novel mechanisms that take into account the trust between agents when allocating tasks. specifically, we develop a new class of mechanisms, called ""trust-based mechanisms"", that can take into account multiple subjective measures of the probability of an agent succeeding at a given task and produce allocations that maximise social utility, whilst ensuring that no agent obtains a negative utility. we then show that such mechanisms pose a challenging new combinatorial optimisation problem (that is np-complete), devise a novel representation for solving the problem, and develop an effective integer programming solution (that can solve instances with about 2x10^5 possible allocations in 40 seconds).",2009
eliciting single-peaked preferences using comparison queries,"voting is a general method for aggregating the preferences of multiple agents. each agent ranks all the possible alternatives, and based on this, an aggregate ranking of the alternatives (or at least a winning alternative) is produced. however, when there are many alternatives, it is impractical to simply ask agents to report their complete preferences. rather, the agents' preferences, or at least the relevant parts thereof, need to be elicited. this is done by asking the agents a (hopefully small) number of simple queries about their preferences, such as comparison queries, which ask an agent to compare two of the alternatives. prior work on preference elicitation in voting has focused on the case of unrestricted preferences. it has been shown that in this setting, it is sometimes necessary to ask each agent (almost) as many queries as would be required to determine an arbitrary ranking of the alternatives. in contrast, in this paper, we focus on single-peaked preferences. we show that such preferences can be elicited using only a linear number of comparison queries, if either the order with respect to which preferences are single-peaked is known, or at least one other agent's complete preferences are known. we show that using a sublinear number of queries does not suffice. we also consider the case of cardinally single-peaked preferences. for this case, we show that if the alternatives' cardinal positions are known, then an agent's preferences can be elicited using only a logarithmic number of queries; however, we also show that if the cardinal positions are not known, then a sublinear number of queries does not suffice. we present experimental results for all elicitation algorithms. we also consider the problem of only eliciting enough information to determine the aggregate ranking, and show that even for this more modest objective, a sublinear number of queries per agent does not suffice for known ordinal or unknown cardinal positions. finally, we discuss whether and how these techniques can be applied when preferences are almost single-peaked.",2009
transductive rademacher complexity and its applications,"we develop a technique for deriving data-dependent error bounds for transductive learning algorithms based on transductive rademacher complexity. our technique is based on a novel general error bound for transduction in terms of transductive rademacher complexity, together with a novel bounding technique for rademacher averages for particular algorithms, in terms of their ""unlabeled-labeled"" representation. this technique is relevant to many advanced graph-based transductive algorithms and we demonstrate its effectiveness by deriving error bounds to three well known algorithms. finally, we present a new pac-bayesian bound for mixtures of transductive algorithms based on our rademacher bounds.",2009
a bilinear programming approach for multiagent planning,"multiagent planning and coordination problems are common and known to be computationally hard. we show that a wide range of two-agent problems can be formulated as bilinear programs. we present a successive approximation algorithm that significantly outperforms the coverage set algorithm, which is the state-of-the-art method for this class of multiagent problems. because the algorithm is formulated for bilinear programs, it is more general and simpler to implement. the new algorithm can be terminated at any time and-unlike the coverage set algorithm-it facilitates the derivation of a useful online performance bound. it is also much more efficient, on average reducing the computation time of the optimal solution by about four orders of magnitude. finally, we introduce an automatic dimensionality reduction method that improves the effectiveness of the algorithm, extending its applicability to new domains and providing a new way to analyze a subclass of bilinear programs.",2009
llull and copeland voting computationally resist bribery and constructive control,"control and bribery are settings in which an external agent seeks to influence the outcome of an election. constructive control of elections refers to attempts by an agent to, via such actions as addition/deletion/partition of candidates or voters, ensure that a given candidate wins. destructive control refers to attempts by an agent to, via the same actions, preclude a given candidate's victory. an election system in which an agent can sometimes affect the result and it can be determined in polynomial time on which inputs the agent can succeed is said to be vulnerable to the given type of control. an election system in which an agent can sometimes affect the result, yet in which it is np-hard to recognize the inputs on which the agent can succeed, is said to be resistant to the given type of control. aside from election systems with an np-hard winner problem, the only systems previously known to be resistant to all the standard control types were highly artificial election systems created by hybridization. this paper studies a parameterized version of copeland voting, denoted by copeland^\alpha, where the parameter \alpha is a rational number between 0 and 1 that specifies how ties are valued in the pairwise comparisons of candidates. in every previously studied constructive or destructive control scenario, we determine which of resistance or vulnerability holds for copeland^\alpha for each rational \alpha, 0 \leq \alpha \leq 1. in particular, we prove that copeland^{0.5}, the system commonly referred to as ``copeland voting,'' provides full resistance to constructive control, and we prove the same for copeland^\alpha, for all rational \alpha, 0 < \alpha < 1. among systems with a polynomial-time winner problem, copeland voting is the first natural election system proven to have full resistance to constructive control. in addition, we prove that both copeland^0 and copeland^1 (interestingly, copeland^1 is an election system developed by the thirteenth-century mystic llull) are resistant to all standard types of constructive control other than one variant of addition of candidates. moreover, we show that for each rational \alpha, 0 \leq \alpha \leq 1, copeland^\alpha voting is fully resistant to bribery attacks, and we establish fixed-parameter tractability of bounded-case control for copeland^\alpha. we also study copeland^\alpha elections under more flexible models such as microbribery and extended control, we integrate the potential irrationality of voter preferences into many of our results, and we prove our results in both the unique-winner model and the nonunique-winner model. our vulnerability results for microbribery are proven via a novel technique involving min-cost network flow.",2009
automated reasoning in modal and description logics via sat encoding: the case study of k(m)/alc-satisfiability,"in the last two decades, modal and description logics have been applied to numerous areas of computer science, including knowledge representation, formal verification, database theory, distributed computing and, more recently, semantic web and ontologies. for this reason, the problem of automated reasoning in modal and description logics has been thoroughly investigated. in particular, many approaches have been proposed for efficiently handling the satisfiability of the core normal modal logic k(m), and of its notational variant, the description logic alc. although simple in structure, k(m)/alc is computationally very hard to reason on, its satisfiability being pspace-complete. in this paper we start exploring the idea of performing automated reasoning tasks in modal and description logics by encoding them into sat, so that to be handled by state-of-the-art sat tools; as with most previous approaches, we begin our investigation from the satisfiability in k(m). we propose an efficient encoding, and we test it on an extensive set of benchmarks, comparing the approach with the main state-of-the-art tools available. although the encoding is necessarily worst-case exponential, from our experiments we notice that, in practice, this approach can handle most or all the problems which are at the reach of the other approaches, with performances which are comparable with, or even better than, those of the current state-of-the-art tools.",2009
learning bayesian network equivalence classes with ant colony optimization,"bayesian networks are a useful tool in the representation of uncertain knowledge. this paper proposes a new algorithm called aco-e, to learn the structure of a bayesian network. it does this by conducting a search through the space of equivalence classes of bayesian networks using ant colony optimization (aco). to this end, two novel extensions of traditional aco techniques are proposed and implemented. firstly, multiple types of moves are allowed. secondly, moves can be given in terms of indices that are not based on construction graph nodes. the results of testing show that aco-e performs better than a greedy search and other state-of-the-art and metaheuristic algorithms whilst searching in the space of equivalence classes.",2009
efficient markov network structure discovery using independence tests,"we present two algorithms for learning the structure of a markov network from data: gsmn* and gsimn. both algorithms use statistical independence tests to infer the structure by successively constraining the set of structures consistent with the results of these tests. until very recently, algorithms for structure learning were based on maximum likelihood estimation, which has been proved to be np-hard for markov networks due to the difficulty of estimating the parameters of the network, needed for the computation of the data likelihood. the independence-based approach does not require the computation of the likelihood, and thus both gsmn* and gsimn can compute the structure efficiently (as shown in our experiments). gsmn* is an adaptation of the grow-shrink algorithm of margaritis and thrun for learning the structure of bayesian networks. gsimn extends gsmn* by additionally exploiting pearl's well-known properties of the conditional independence relation to infer novel independences from known ones, thus avoiding the performance of statistical tests to estimate them. to accomplish this efficiently gsimn uses the triangle theorem, also introduced in this work, which is a simplified version of the set of markov axioms. experimental comparisons on artificial and real-world data sets show gsimn can yield significant savings with respect to gsmn*, while generating a markov network with comparable or in some cases improved quality. we also compare gsimn to a forward-chaining implementation, called gsimn-fch, that produces all possible conditional independences resulting from repeatedly applying pearl's theorems on the known conditional independence tests. the results of this comparison show that gsimn, by the sole use of the triangle theorem, is nearly optimal in terms of the set of independences tests that it infers.",2009
how hard is bribery in elections?,"we study the complexity of influencing elections through bribery: how computationally complex is it for an external actor to determine whether by paying certain voters to change their preferences a specified candidate can be made the elections winner? we study this problem for election systems as varied as scoring protocols and dodgson voting, and in a variety of settings regarding homogeneous-vs.-nonhomogeneous electorate bribability, bounded-size-vs.-arbitrary-sized candidate sets, weighted-vs.-unweighted voters, and succinct-vs.-nonsuccinct input specification. we obtain both polynomial-time bribery algorithms and proofs of the intractability of bribery, and indeed our results show that the complexity of bribery is extremely sensitive to the setting. for example, we find settings in which bribery is np-complete but manipulation (by voters) is in p, and we find settings in which bribing weighted voters is np-complete but bribing voters with individual bribe thresholds is in p. for the broad class of elections (including plurality, borda, k-approval, and veto) known as scoring protocols, we prove a dichotomy result for bribery of weighted voters: we find a simple-to-evaluate condition that classifies every case as either np-complete or in p.",2009
solving weighted constraint satisfaction problems with memetic/exact hybrid algorithms,"a weighted constraint satisfaction problem (wcsp) is a constraint satisfaction problem in which preferences among solutions can be expressed. bucket elimination is a complete technique commonly used to solve this kind of constraint satisfaction problem. when the memory required to apply bucket elimination is too high, a heuristic method based on it (denominated mini-buckets) can be used to calculate bounds for the optimal solution. nevertheless, the curse of dimensionality makes these techniques impractical on large scale problems. in response to this situation, we present a memetic algorithm for wcsps in which bucket elimination is used as a mechanism for recombining solutions, providing the best possible child from the parental set. subsequently, a multi-level model in which this exact/metaheuristic hybrid is further hybridized with branch-and-bound techniques and mini-buckets is studied. as a case study, we have applied these algorithms to the resolution of the maximum density still life problem, a hard constraint optimization problem based on conway's game of life. the resulting algorithm consistently finds optimal patterns for up to date solved instances in less time than current approaches. moreover, it is shown that this proposal provides new best known solutions for very large instances.",2009
optimal value of information in graphical models,"many real-world decision making tasks require us to choose among several expensive observations. in a sensor network, for example, it is important to select the subset of sensors that is expected to provide the strongest reduction in uncertainty. in medical decision making tasks, one needs to select which tests to administer before deciding on the most effective treatment. it has been general practice to use heuristic-guided procedures for selecting observations. in this paper, we present the first efficient optimal algorithms for selecting observations for a class of probabilistic graphical models. for example, our algorithms allow to optimally label hidden variables in hidden markov models (hmms). we provide results for both selecting the optimal subset of observations, and for obtaining an optimal conditional observation plan. furthermore we prove a surprising result: in most graphical models tasks, if one designs an efficient algorithm for chain graphs, such as hmms, this procedure can be generalized to polytree graphical models. we prove that the optimizing value of information is $np^{pp}$-hard even for polytrees. it also follows from our results that just computing decision theoretic value of information objective functions, which are commonly used in practice, is a #p-complete problem even on naive bayes models (a simple special case of polytrees). in addition, we consider several extensions, such as using our algorithms for scheduling observation selection for multiple sensors. we demonstrate the effectiveness of our approach on several real-world datasets, including a prototype sensor network deployment for energy conservation in buildings.",2009
bounds arc consistency for weighted csps,"the weighted constraint satisfaction problem (wcsp) framework allows representing and solving problems involving both hard constraints and cost functions. it has been applied to various problems, including resource allocation, bioinformatics, scheduling, etc. to solve such problems, solvers usually rely on branch-and-bound algorithms equipped with local consistency filtering, mostly soft arc consistency. however, these techniques are not well suited to solve problems with very large domains. motivated by the resolution of an rna gene localization problem inside large genomic sequences, and in the spirit of bounds consistency for large domains in crisp csps, we introduce soft bounds arc consistency, a new weighted local consistency specifically designed for wcsp with very large domains. compared to soft arc consistency, bac provides significantly improved time and space asymptotic complexity. in this paper, we show how the semantics of cost functions can be exploited to further improve the time complexity of bac. we also compare both in theory and in practice the efficiency of bac on a wcsp with bounds consistency enforced on a crisp csp using cost variables. on two different real problems modeled as wcsp, including our rna gene localization problem, we observe that maintaining bounds arc consistency outperforms arc consistency and also improves over bounds consistency enforced on a constraint model with cost variables.",2009
compiling uncertainty away in conformant planning problems with bounded width,"conformant planning is the problem of finding a sequence of actions for achieving a goal in the presence of uncertainty in the initial state or action effects. the problem has been approached as a path-finding problem in belief space where good belief representations and heuristics are critical for scaling up. in this work, a different formulation is introduced for conformant problems with deterministic actions where they are automatically converted into classical ones and solved by an off-the-shelf classical planner. the translation maps literals l and sets of assumptions t about the initial situation, into new literals kl/t that represent that l must be true if t is initially true. we lay out a general translation scheme that is sound and establish the conditions under which the translation is also complete. we show that the complexity of the complete translation is exponential in a parameter of the problem called the conformant width, which for most benchmarks is bounded. the planner based on this translation exhibits good performance in comparison with existing planners, and is the basis for t0, the best performing planner in the conformant track of the 2006 international planning competition.",2009
variable forgetting in reasoning about knowledge,"in this paper, we investigate knowledge reasoning within a simple framework called knowledge structure. we use variable forgetting as a basic operation for one agent to reason about its own or other agents\' knowledge. in our framework, two notions namely agents\' observable variables and the weakest sufficient condition play important roles in knowledge reasoning. given a background knowledge base and a set of observable variables for each agent, we show that the notion of an agent knowing a formula can be defined as a weakest sufficient condition of the formula under background knowledge base. moreover, we show how to capture the notion of common knowledge by using a generalized notion of weakest sufficient condition. also, we show that public announcement operator can be conveniently dealt with via our notion of knowledge structure. further, we explore the computational complexity of the problem whether an epistemic formula is realized in a knowledge structure. in the general case, this problem is pspace-hard; however, for some interesting subcases, it can be reduced to co-np. finally, we discuss possible applications of our framework in some interesting domains such as the automated analysis of the well-known muddy children puzzle and the verification of the revised needham-schroeder protocol. we believe that there are many scenarios where the natural presentation of the available information about knowledge is under the form of a knowledge structure. what makes it valuable compared with the corresponding multi-agent s5 kripke structure is that it can be much more succinct.",2009
the complexity of circumscription in dls,"as fragments of first-order logic, description logics (dls) do not provide nonmonotonic features such as defeasible inheritance and default rules. since many applications would benefit from the availability of such features, several families of nonmonotonic dls have been developed that are mostly based on default logic and autoepistemic logic. in this paper, we consider circumscription as an interesting alternative approach to nonmonotonic dls that, in particular, supports defeasible inheritance in a natural way. we study dls extended with circumscription under different language restrictions and under different constraints on the sets of minimized, fixed, and varying predicates, and pinpoint the exact computational complexity of reasoning for dls ranging from alc to alcio and alcqo. when the minimized and fixed predicates include only concept names but no role names, then reasoning is complete for nexptime^np. it becomes complete for np^nexptime when the number of minimized and fixed predicates is bounded by a constant. if roles can be minimized or fixed, then complexity ranges from nexptime^np to undecidability.",2009
enhancing qa systems with complex temporal question processing capabilities,"this paper presents a multilayered architecture that enhances the capabilities of current qa systems and allows different types of complex questions or queries to be processed. the answers to these questions need to be gathered from factual information scattered throughout different documents. specifically, we designed a specialized layer to process the different types of temporal questions. complex temporal questions are first decomposed into simple questions, according to the temporal relations expressed in the original question. in the same way, the answers to the resulting simple questions are recomposed, fulfilling the temporal restrictions of the original complex question. a novel aspect of this approach resides in the decomposition which uses a minimal quantity of resources, with the final aim of obtaining a portable platform that is easily extensible to other languages. in this paper we also present a methodology for evaluation of the decomposition of the questions as well as the ability of the implemented temporal layer to perform at a multilingual level. the temporal layer was first performed for english, then evaluated and compared with: a) a general purpose qa system (f-measure 65.47% for qa plus english temporal layer vs. 38.01% for the general qa system), and b) a well-known qa system. much better results were obtained for temporal questions with the multilayered system. this system was therefore extended to spanish and very good results were again obtained in the evaluation (f-measure 40.36% for qa plus spanish temporal layer vs. 22.94% for the general qa system).",2009
modularity aspects of disjunctive stable models,"practically all programming languages allow the programmer to split a program into several modules which brings along several advantages in software development. in this paper, we are interested in the area of answer-set programming where fully declarative and nonmonotonic languages are applied. in this context, obtaining a modular structure for programs is by no means straightforward since the output of an entire program cannot in general be composed from the output of its components. to better understand the effects of disjunctive information on modularity we restrict the scope of analysis to the case of disjunctive logic programs (dlps) subject to stable-model semantics. we define the notion of a dlp-function, where a well-defined input/output interface is provided, and establish a novel module theorem which indicates the compositionality of stable-model semantics for dlp-functions. the module theorem extends the well-known splitting-set theorem and enables the decomposition of dlp-functions given their strongly connected components based on positive dependencies induced by rules. in this setting, it is also possible to split shared disjunctive rules among components using a generalized shifting technique. the concept of modular equivalence is introduced for the mutual comparison of dlp-functions using a generalization of a translation-based verification method.",2009
interactive policy learning through confidence-based autonomy,"we present confidence-based autonomy (cba), an interactive algorithm for policy learning from demonstration. the cba algorithm consists of two components which take advantage of the complimentary abilities of humans and computer agents. the first component, confident execution, enables the agent to identify states in which demonstration is required, to request a demonstration from the human teacher and to learn a policy based on the acquired data. the algorithm selects demonstrations based on a measure of action selection confidence, and our results show that using confident execution the agent requires fewer demonstrations to learn the policy than when demonstrations are selected by a human teacher. the second algorithmic component, corrective demonstration, enables the teacher to correct any mistakes made by the agent through additional demonstrations in order to improve the policy and future task performance. cba and its individual components are compared and evaluated in a complex simulated driving domain. the complete cba algorithm results in the best overall learning performance, successfully reproducing the behavior of the teacher while balancing the tradeoff between number of demonstrations and number of incorrect actions during learning.",2009
a heuristic search approach to planning with continuous resources in stochastic domains,"we consider the problem of optimal planning in stochastic domains with resource constraints, where the resources are continuous and the choice of action at each step depends on resource availability. we introduce the hao* algorithm, a generalization of the ao* algorithm that performs search in a hybrid state space that is modeled using both discrete and continuous state variables, where the continuous variables represent monotonic resources. like other heuristic search algorithms, hao* leverages knowledge of the start state and an admissible heuristic to focus computational effort on those parts of the state space that could be reached from the start state by following an optimal policy. we show that this approach is especially effective when resource constraints limit how much of the state space is reachable. experimental results demonstrate its effectiveness in the domain that motivates our research: automated planning for planetary exploration rovers.",2009
asynchronous forward bounding for distributed cops,"a new search algorithm for solving distributed constraint optimization problems (discops) is presented. agents assign variables sequentially and compute bounds on partial assignments asynchronously. the asynchronous bounds computation is based on the propagation of partial assignments. the asynchronous forward-bounding algorithm (afb) is a distributed optimization search algorithm that keeps one consistent partial assignment at all times. the algorithm is described in detail and its correctness proven. experimental evaluation shows that afb outperforms synchronous branch and bound by many orders of magnitude, and produces a phase transition as the tightness of the problem increases. this is an analogous effect to the phase transition that has been observed when local consistency maintenance is applied to maxcsps. the afb algorithm is further enhanced by the addition of a backjumping mechanism, resulting in the afb-bj algorithm. distributed backjumping is based on accumulated information on bounds of all values and on processing concurrently a queue of candidate goals for the next move back. the afb-bj algorithm is compared experimentally to other discop algorithms (adopt, dpop, optapo) and is shown to be a very efficient algorithm for discops.",2009
policy iteration for decentralized control of markov decision processes,"coordination of distributed agents is required for problems arising in many areas, including multi-robot systems, networking and e-commerce. as a formal framework for such problems, we use the decentralized partially observable markov decision process (dec-pomdp). though much work has been done on optimal dynamic programming algorithms for the single-agent version of the problem, optimal algorithms for the multiagent case have been elusive. the main contribution of this paper is an optimal policy iteration algorithm for solving dec-pomdps. the algorithm uses stochastic finite-state controllers to represent policies. the solution can include a correlation device, which allows agents to correlate their actions without communicating. this approach alternates between expanding the controller and performing value-preserving transformations, which modify the controller without sacrificing value. we present two efficient value-preserving transformations: one can reduce the size of the controller and the other can improve its value while keeping the size fixed. empirical results demonstrate the usefulness of value-preserving transformations in increasing value while keeping controller size to a minimum. to broaden the applicability of the approach, we also present a heuristic version of the policy iteration algorithm, which sacrifices convergence to optimality. this algorithm further reduces the size of the controllers at each step by assuming that probability distributions over the other agents' actions are known. while this assumption may not hold in general, it helps produce higher quality solutions in our test problems.",2009
generic preferences over subsets of structured objects,"various tasks in decision making and decision support systems require selecting a preferred subset of a given set of items. here we focus on problems where the individual items are described using a set of characterizing attributes, and a generic preference specification is required, that is, a specification that can work with an arbitrary set of items. for example, preferences over the content of an online newspaper should have this form: at each viewing, the newspaper contains a subset of the set of articles currently available. our preference specification over this subset should be provided offline, but we should be able to use it to select a subset of any currently available set of articles, e.g., based on their tags. we present a general approach for lifting formalisms for specifying preferences over objects with multiple attributes into ones that specify preferences over subsets of such objects. we also show how we can compute an optimal subset given such a specification in a relatively efficient manner. we provide an empirical evaluation of the approach as well as some worst-case complexity results.",2009
behavior bounding: an efficient method for high-level behavior comparison,"in this paper, we explore methods for comparing agent behavior with human behavior to assist with validation. our exploration begins by considering a simple method of behavior comparison. motivated by shortcomings in this initial approach, we introduce behavior bounding, an automated model-based approach for comparing behavior that is inspired, in part, by mitchells version spaces. we show that behavior bounding can be used to compactly represent both human and agent behavior. we argue that relatively low amounts of human e&#64256;ort are required to build, maintain, and use the data structures that underlie behavior bounding, and we provide a theoretical basis for these arguments using notions of pac learnability. next, we show empirical results indicating that this approach is e&#64256;ective at identifying differences in certain types of behaviors and that it performs well when compared against our initial benchmark methods. finally, we demonstrate that behavior bounding can produce information that allows developers to identify and &#64257;x problems in an agents behavior much more e&#64259;ciently than standard debugging techniques.",2009
mechanisms for making crowds truthful,"we consider schemes for obtaining truthful reports on a common but hidden signal from large groups of rational, self-interested agents. one example are online feedback mechanisms, where users provide observations about the quality of a product or service so that other users can have an accurate idea of what quality they can expect. however, (i) providing such feedback is costly, and (ii) there are many motivations for providing incorrect feedback. both problems can be addressed by reward schemes which (i) cover the cost of obtaining and reporting feedback, and (ii) maximize the expected reward of a rational agent who reports truthfully. we address the design of such incentive-compatible rewards for feedback generated in environments with pure adverse selection. here, the correlation between the true knowledge of an agent and her beliefs regarding the likelihoods of reports of other agents can be exploited to make honest reporting a nash equilibrium. in this paper we extend existing methods for designing incentive-compatible rewards by also considering collusion. we analyze different scenarios, where, for example, some or all of the agents collude. for each scenario we investigate whether a collusion-resistant, incentive-compatible reward scheme exists, and use automated mechanism design to specify an algorithm for deriving an efficient reward mechanism.",2009
unsupervised methods for determining object and relation synonyms on the web,"the task of identifying synonymous relations and objects, or synonym resolution, is critical for high-quality information extraction. this paper investigates synonym resolution in the context of unsupervised information extraction, where neither hand-tagged training examples nor domain knowledge is available. the paper presents a scalable, fully-implemented system that runs in o(kn log n) time in the number of extractions, n, and the maximum number of synonyms per word, k. the system, called resolver , introduces a probabilistic relational model for predicting whether two strings are co-referential based on the similarity of the assertions containing them. on a set of two million assertions extracted from the web, resolver resolves objects with 78% precision and 68% recall, and resolves relations with 90% precision and 35% recall. several variations of resolver's probabilistic model are explored, and experiments demonstrate that under appropriate conditions these variations can improve f1 by 5%. an extension to the basic resolver system allows it to handle polysemous names with 97% precision and 95% recall on a data set from the trec corpus.",2009
monte carlo sampling methods for approximating interactive pomdps,"partially observable markov decision processes (pomdps) provide a principled framework for sequential planning in uncertain single agent settings. an extension of pomdps to multiagent settings, called interactive pomdps (i-pomdps), replaces pomdp belief spaces with interactive hierarchical belief systems which represent an agents belief about the physical world, about beliefs of other agents, and about their beliefs about others beliefs. this modification makes the difficulties of obtaining solutions due to complexity of the belief and policy spaces even more acute. we describe a general method for obtaining approximate solutions of i-pomdps based on particle filtering (pf). we introduce the interactive pf, which descends the levels of the interactive belief hierarchies and samples and propagates beliefs at each level. the interactive pf is able to mitigate the belief space complexity, but it does not address the policy space complexity. to mitigate the policy space complexity  sometimes also called the curse of history  we utilize a complementary method based on sampling likely observations while building the look ahead reachability tree. while this approach does not completely address the curse of history, it beats back the curses impact substantially. we provide experimental results and chart future work.",2009
identification of pleonastic it using the web,"in a significant minority of cases, certain pronouns, especially the pronoun it, can be used without referring to any specific entity. this phenomenon of pleonastic pronoun usage poses serious problems for systems aiming at even a shallow understanding of natural language texts. in this paper, a novel approach is proposed to identify such uses of it: the extrapositional cases are identified using a series of queries against the web, and the cleft cases are identified using a simple set of syntactic rules. the system is evaluated with four sets of news articles containing 679 extrapositional cases as well as 78 cleft constructs. the identification results are comparable to those obtained by human efforts.",2009
solving #sat and bayesian inference with backtracking search,"inference in bayes nets (bayes) is an important problem with numerous applications in probabilistic reasoning. counting the number of satisfying assignments of a propositional formula (#sat) is a closely related problem of fundamental theoretical importance. both these problems, and others, are members of the class of sum-of-products (sumprod) problems. in this paper we show that standard backtracking search when augmented with a simple memoization scheme (caching) can solve any sum-of-products problem with time complexity that is at least as good any other state-of-the-art exact algorithm, and that it can also achieve the best known time-space tradeoff. furthermore, backtrackings ability to utilize more flexible variable orderings allows us to prove that it can achieve an exponential speedup over other standard algorithms for sumprod on some instances. the ideas presented here have been utilized in a number of solvers that have been applied to various types of sum-of-product problems. these systems have exploited the fact that backtracking can naturally exploit more of the problems structure to achieve improved performance on a range of probleminstances. empirical evidence of this performance gain has appeared in published works describing these solvers, and we provide references to these works.",2009
wikipedia-based semantic interpretation for natural language processing,"adequate representation of natural language semantics requires access to vast amounts of common sense and domain-specific world knowledge. prior work in the field was based on purely statistical techniques that did not make use of background knowledge, on limited lexicographic knowledge bases such as wordnet, or on huge manual efforts such as the cyc project. here we propose a novel method, called explicit semantic analysis (esa), for fine-grained semantic interpretation of unrestricted natural language texts. our method represents meaning in a high-dimensional space of concepts derived from wikipedia, the largest encyclopedia in existence. we explicitly represent the meaning of any text in terms of wikipedia-based concepts. we evaluate the effectiveness of our method on text categorization and on computing the degree of semantic relatedness between fragments of natural language text. using esa results in significant improvements over the previous state of the art in both tasks. importantly, due to the use of natural concepts, the esa model is easy to explain to human users.",2009
exploiting single-cycle symmetries in continuous constraint problems,"symmetries in discrete constraint satisfaction problems have been explored and exploited in the last years, but symmetries in continuous constraint problems have not received the same attention. here we focus on permutations of the variables consisting of one single cycle. we propose a procedure that takes advantage of these symmetries by interacting with a continuous constraint solver without interfering with it. a key concept in this procedure are the classes of symmetric boxes formed by bisecting a n-dimensional cube at the same point in all dimensions at the same time. we analyze these classes and quantify them as a function of the cube dimensionality. moreover, we propose a simple algorithm to generate the representatives of all these classes for any number of variables at very high rates. a problem example from the chemical &#64257;eld and the cyclic n-roots problem are used to show the performance of the approach in practice.",2009
an anytime algorithm for optimal coalition structure generation,"coalition formation is a fundamental type of interaction that involves the creation of coherent groupings of distinct, autonomous, agents in order to efficiently achieve their individual or collective goals. forming effective coalitions is a major research challenge in the field of multi-agent systems. central to this endeavour is the problem of determining which of the many possible coalitions to form in order to achieve some goal. this usually requires calculating a value for every possible coalition, known as the coalition value, which indicates how beneficial that coalition would be if it was formed. once these values are calculated, the agents usually need to find a combination of coalitions, in which every agent belongs to exactly one coalition, and by which the overall outcome of the system is maximized. however, this coalition structure generation problem is extremely challenging due to the number of possible solutions that need to be examined, which grows exponentially with the number of agents involved. to date, therefore, many algorithms have been proposed to solve this problem using different techniques ranging from dynamic programming, to integer programming, to stochastic search all of which suffer from major limitations relating to execution time, solution quality, and memory requirements. with this in mind, we develop an anytime algorithm to solve the coalition structure generation problem. specifically, the algorithm uses a novel representation of the search space, which partitions the space of possible solutions into sub-spaces such that it is possible to compute upper and lower bounds on the values of the best coalition structures in them. these bounds are then used to identify the sub-spaces that have no potential of containing the optimal solution so that they can be pruned. the algorithm, then, searches through the remaining sub-spaces very efficiently using a branch-and-bound technique to avoid examining all the solutions within the searched subspace(s). in this setting, we prove that our algorithm enumerates all coalition structures efficiently by avoiding redundant and invalid solutions automatically. moreover, in order to effectively test our algorithm we develop a new type of input distribution which allows us to generate more reliable benchmarks compared to the input distributions previously used in the field. given this new distribution, we show that for 27 agents our algorithm is able to find solutions that are optimal in 0.175% of the time required by the fastest available algorithm in the literature. the algorithm is anytime, and if interrupted before it would have normally terminated, it can still provide a solution that is guaranteed to be within a bound from the optimal one. moreover, the guarantees we provide on the quality of the solution are significantly better than those provided by the previous state of the art algorithms designed for this purpose. for example, for the worst case distribution given 25 agents, our algorithm is able to find a 90% efficient solution in around 10% of time it takes to find the optimal solution.",2009
learning document-level semantic properties from free-text annotations,"this paper presents a new method for inferring the semantic properties of documents by leveraging free-text keyphrase annotations. such annotations are becoming increasingly abundant due to the recent dramatic growth in semi-structured, user-generated online content. one especially relevant domain is product reviews, which are often annotated by their authors with pros/cons keyphrases such as ``a real bargain'' or ``good value.'' these annotations are representative of the underlying semantic properties; however, unlike expert annotations, they are noisy: lay authors may use different labels to denote the same property, and some labels may be missing. to learn using such noisy annotations, we find a hidden paraphrase structure which clusters the keyphrases. the paraphrase structure is linked with a latent topic model of the review texts, enabling the system to predict the properties of unannotated documents and to effectively aggregate the semantic properties of multiple reviews. our approach is implemented as a hierarchical bayesian model with joint inference. we find that joint inference increases the robustness of the keyphrase clustering and encourages the latent topics to correlate with semantically meaningful properties. multiple evaluations demonstrate that our model substantially outperforms alternative approaches for summarizing single and multiple documents into a set of semantically salient keyphrases.",2009
inferring shallow-transfer machine translation rules from small parallel corpora,"this paper describes a method for the automatic inference of structural transfer rules to be used in a shallow-transfer machine translation (mt) system from small parallel corpora. the structural transfer rules are based on alignment templates, like those used in statistical mt. alignment templates are extracted from sentence-aligned parallel corpora and extended with a set of restrictions which are derived from the bilingual dictionary of the mt system and control their application as transfer rules. the experiments conducted using three different language pairs in the free/open-source mt platform apertium show that translation quality is improved as compared to word-for-word translation (when no transfer rules are used), and that the resulting translation quality is close to that obtained using hand-coded transfer rules. the method we present is entirely unsupervised and benefits from information in the rest of modules of the mt system in which the inferred rules are applied.",2009
sentence compression as tree transduction,"this paper presents a tree-to-tree transduction method for sentence compression. our model is based on synchronous tree substitution grammar, a formalism that allows local distortion of the tree topology and can thus naturally capture structural mismatches. we describe an algorithm for decoding in this framework and show how the model can be trained discriminatively within a large margin framework. experimental results on sentence compression bring significant improvements over a state-of-the-art model.",2009
planning over chain causal graphs for variables with domains of size 5 is np-hard,"recently, considerable focus has been given to the problem of determining the boundary between tractable and intractable planning problems. in this paper, we study the complexity of planning in the class c_n of planning problems, characterized by unary operators and directed path causal graphs. although this is one of the simplest forms of causal graphs a planning problem can have, we show that planning is intractable for c_n (unless p = np), even if the domains of state variables have bounded size. in particular, we show that plan existence for c_n^k is np-hard for k>=5 by reduction from cnfsat. here, k denotes the upper bound on the size of the state variable domains. our result reduces the complexity gap for the class c_n^k to cases k=3 and k=4 only, since c_n^2 is known to be tractable.",2009
efficient informative sensing using multiple robots,"the need for efficient monitoring of spatio-temporal dynamics in large environmental applications, such as the water quality monitoring in rivers and lakes, motivates the use of robotic sensors in order to achieve sufficient spatial coverage. typically, these robots have bounded resources, such as limited battery or limited amounts of time to obtain measurements. thus, careful coordination of their paths is required in order to maximize the amount of information collected, while respecting the resource constraints. in this paper, we present an efficient approach for near-optimally solving the np-hard optimization problem of planning such informative paths. in particular, we first develop esip (efficient single-robot informative path planning), an approximation algorithm for optimizing the path of a single robot. hereby, we use a gaussian process to model the underlying phenomenon, and use the mutual information between the visited locations and remainder of the space to quantify the amount of information collected. we prove that the mutual information collected using paths obtained by using esip is close to the information obtained by an optimal solution. we then provide a general technique, sequential allocation, which can be used to extend any single robot planning algorithm, such as esip, for the multi-robot problem. this procedure approximately generalizes any guarantees for the single-robot problem to the multi-robot case. we extensively evaluate the effectiveness of our approach on several experiments performed in-field for two important environmental sensing applications, lake and river monitoring, and simulation experiments performed using several real world sensor network data sets.",2009
conservative inference rule for uncertain reasoning under incompleteness,"in this paper we formulate the problem of inference under incomplete information in very general terms. this includes modelling the process responsible for the incompleteness, which we call the incompleteness process. we allow the process' behaviour to be partly unknown. then we use walley's theory of coherent lower previsions, a generalisation of the bayesian theory to imprecision, to derive the rule to update beliefs under incompleteness that logically follows from our assumptions, and that we call conservative inference rule. this rule has some remarkable properties: it is an abstract rule to update beliefs that can be applied in any situation or domain; it gives us the opportunity to be neither too optimistic nor too pessimistic about the incompleteness process, which is a necessary condition to draw reliable while strong enough conclusions; and it is a coherent rule, in the sense that it cannot lead to inconsistencies. we give examples to show how the new rule can be applied in expert systems, in parametric statistical inference, and in pattern classification, and discuss more generally the view of incompleteness processes defended here as well as some of its consequences.",2009
"anytime induction of low-cost, low-error classifiers: a sampling-based approach","machine learning techniques are gaining prevalence in the production of a wide range of classifiers for complex real-world applications with nonuniform testing and misclassification costs. the increasing complexity of these applications poses a real challenge to resource management during learning and classification. in this work we introduce act (anytime cost-sensitive tree learner), a novel framework for operating in such complex environments. act is an anytime algorithm that allows learning time to be increased in return for lower classification costs. it builds a tree top-down and exploits additional time resources to obtain better estimations for the utility of the different candidate splits. using sampling techniques, act approximates the cost of the subtree under each candidate split and favors the one with a minimal cost. as a stochastic algorithm, act is expected to be able to escape local minima, into which greedy methods may be trapped. experiments with a variety of datasets were conducted to compare act to the state-of-the-art cost-sensitive tree learners. the results show that for the majority of domains act produces significantly less costly trees. act also exhibits good anytime behavior with diminishing returns.",2008
ice: an expressive iterative combinatorial exchange,"we present the design and analysis of the first fully expressive, iterative combinatorial exchange (ice). the exchange incorporates a tree-based bidding language (tbbl) that is concise and expressive for ces. bidders specify lower and upper bounds in tbbl on their value for different trades and refine these bounds across rounds. these bounds allow price discovery and useful preference elicitation in early rounds, and allow termination with an efficient trade despite partial information on bidder valuations. all computation in the exchange is carefully optimized to exploit the structure of the bid-trees and to avoid enumerating trades. a proxied interpretation of a revealed-preference activity rule, coupled with simple linear prices, ensures progress across rounds. the exchange is fully implemented, and we give results demonstrating several aspects of its scalability and economic properties with simulated bidding strategies.",2008
on the use of automatically acquired examples for all-nouns word sense disambiguation,"this article focuses on word sense disambiguation (wsd), which is a natural language processing task that is thought to be important for many language technology applications, such as information retrieval, information extraction, or machine translation. one of the main issues preventing the deployment of wsd technology is the lack of training examples for machine learning systems, also known as the knowledge acquisition bottleneck. a method which has been shown to work for small samples of words is the automatic acquisition of examples. we have previously shown that one of the most promising example acquisition methods scales up and produces a freely available database of 150 million examples from web snippets for all polysemous nouns in wordnet. this paper focuses on the issues that arise when using those examples, all alone or in addition to manually tagged examples, to train a supervised wsd system for all nouns. the extensive evaluation on both lexical-sample and all-words senseval benchmarks shows that we are able to improve over commonly used baselines and to achieve top-rank performance. the good use of the prior distributions from the senses proved to be a crucial factor.",2008
networks of influence diagrams: a formalism for representing agents' beliefs and decision-making processes,"this paper presents networks of influence diagrams (nid), a compact, natural and highly expressive language for reasoning about agents' beliefs and decision-making processes. nids are graphical structures in which agents' mental models are represented as nodes in a network; a mental model for an agent may itself use descriptions of the mental models of other agents. nids are demonstrated by examples, showing how they can be used to describe conflicting and cyclic belief structures, and certain forms of bounded rationality. in an opponent modeling domain, nids were able to outperform other computational agents whose strategies were not known in advance. nids are equivalent in representation to bayesian games but they are more compact and structured than this formalism. in particular, the equilibrium definition for nids makes an explicit distinction between agents' optimal strategies, and how they actually behave in reality.",2008
complexity of strategic behavior in multi-winner elections,"although recent years have seen a surge of interest in the computational aspects of social choice, no specific attention has previously been devoted to elections with multiple winners, e.g., elections of an assembly or committee. in this paper, we characterize the worst-case complexity of manipulation and control in the context of four prominent multi-winner voting systems, under different formulations of the strategic agent&#226;s goal.",2008
a rigorously bayesian beam model and an adaptive full scan model for range finders in dynamic environments,"this paper proposes and experimentally validates a bayesian network model of a range finder adapted to dynamic environments. all modeling assumptions are rigorously explained, and all model parameters have a physical interpretation. this approach results in a transparent and intuitive model. with respect to the state of the art beam model this paper: (i) proposes a different functional form for the probability of range measurements caused by unmodeled objects, (ii) intuitively explains the discontinuity encountered in te state of the art beam model, and (iii) reduces the number of model parameters, while maintaining the same representational power for experimental data. the proposed beam model is called rbbm, short for rigorously bayesian beam model. a maximum likelihood and a variational bayesian estimator (both based on expectation-maximization) are proposed to learn the model parameters. furthermore, the rbbm is extended to a full scan model in two steps: first, to a full scan model for static environments and next, to a full scan model for general, dynamic environments. the full scan model accounts for the dependency between beams and adapts to the local sample density when using a particle filter. in contrast to gaussian-based state of the art models, the proposed full scan model uses a sample-based approximation. this sample-based approximation enables handling dynamic environments and capturing multi-modality, which occurs even in simple static environments.",2008
completeness and performance of the apo algorithm,"asynchronous partial overlay (apo) is a search algorithm that uses cooperative mediation to solve distributed constraint satisfaction problems (discsps). the algorithm partitions the search into different subproblems of the discsp. the original proof of completeness of the apo algorithm is based on the growth of the size of the subproblems. the present paper demonstrates that this expected growth of subproblems does not occur in some situations, leading to a termination problem of the algorithm. the problematic parts in the apo algorithm that interfere with its completeness are identified and necessary modifications to the algorithm that fix these problematic parts are given. the resulting version of the algorithm, complete asynchronous partial overlay (compapo), ensures its completeness. formal proofs for the soundness and completeness of compapo are given. a detailed performance evaluation of compapo comparing it to other discsp algorithms is presented, along with an extensive experimental evaluation of the algorithms unique behavior. additionally, an optimization version of the algorithm, compoptapo, is presented, discussed, and evaluated.",2008
on similarities between inference in game theory and machine learning,"in this paper, we elucidate the equivalence between inference in game theory and machine learning. our aim in so doing is to establish an equivalent vocabulary between the two domains so as to facilitate developments at the intersection of both &#64257;elds, and as proof of the usefulness of this approach, we use recent developments in each &#64257;eld to make useful improvements to the other. more speci&#64257;cally, we consider the analogies between smooth best responses in &#64257;ctitious play and bayesian inference methods. initially, we use these insights to develop and demonstrate an improved algorithm for learning in games based on probabilistic moderation. that is, by integrating over the distribution of opponent strategies (a bayesian approach within machine learning) rather than taking a simple empirical average (the approach used in standard &#64257;ctitious play) we derive a novel moderated &#64257;ctitious play algorithm and show that it is more likely than standard &#64257;ctitious play to converge to a payoff-dominant but risk-dominated nash equilibrium in a simple coordination game. furthermore we consider the converse case, and show how insights from game theory can be used to derive two improved mean &#64257;eld variational learning algorithms. we &#64257;rst show that the standard update rule of mean &#64257;eld variational learning is analogous to a cournot adjustment within game theory. by analogy with &#64257;ctitious play, we then suggest an improved update rule, and show that this results in &#64257;ctitious variational play, an improved mean &#64257;eld variational learning algorithm that exhibits better convergence in highly or strongly connected graphical models. second, we use a recent advance in &#64257;ctitious play, namely dynamic &#64257;ctitious play, to derive a derivative action variational learning algorithm, that exhibits superior convergence properties on a canonical machine learning problem (clustering a mixture distribution).",2008
computational logic foundations of kgp agents,"this paper presents the computational logic foundations of a model of agency called the kgp (knowledge, goals and plan model. this model allows the specification of heterogeneous agents that can interact with each other, and can exhibit both proactive and reactive behaviour allowing them to function in dynamic environments by adjusting their goals and plans when changes happen in such environments. kgp provides a highly modular agent architecture that integrates a collection of reasoning and physical capabilities, synthesised within transitions that update the agent's state in response to reasoning, sensing and acting. transitions are orchestrated by cycle theories that specify the order in which transitions are executed while taking into account the dynamic context and agent preferences, as well as selection operators for providing inputs to transitions.",2008
learning partially observable deterministic action models,"we present exact algorithms for identifying deterministic-actions' effects and preconditions in dynamic partially observable domains. they apply when one does not know the action model(the way actions affect the world) of a domain and must learn it from partial observations over time. such scenarios are common in real world applications. they are challenging for ai tasks because traditional domain structures that underly tractability (e.g., conditional independence) fail there (e.g., world features become correlated). our work departs from traditional assumptions about partial observations and action models. in particular, it focuses on problems in which actions are deterministic of simple logical structure and observation models have all features observed with some frequency. we yield tractable algorithms for the modified problem for such domains. our algorithms take sequences of partial observations over time as input, and output deterministic action models that could have lead to those observations. the algorithms output all or one of those models (depending on our choice), and are exact in that no model is misclassified given the observations. our algorithms take polynomial time in the number of time steps and state features for some traditional action classes examined in the ai-planning literature, e.g., strips actions. in contrast, traditional approaches for hmms and reinforcement learning are inexact and exponentially intractable for such domains. our experiments verify the theoretical tractability guarantees, and show that we identify action models exactly. several applications in planning, autonomous exploration, and adventure-game playing already use these results. they are also promising for probabilistic settings, partially observable reinforcement learning, and diagnosis.",2008
the computational complexity of dominance and consistency in cp-nets,"we investigate the computational complexity of testing dominance and consistency in cp-nets. previously, the complexity of dominance has been determined for restricted classes in which the dependency graph of the cp-net is acyclic. however, there are preferences of interest that define cyclic dependency graphs; these are modeled with general cp-nets. in our main results, we show here that both dominance and consistency for general cp-nets are pspace-complete. we then consider the concept of strong dominance, dominance equivalence and dominance incomparability, and several notions of optimality, and identify the complexity of the corresponding decision problems. the reductions used in the proofs are from strips planning, and thus reinforce the earlier established connections between both areas.",2008
an ordinal bargaining solution with fixed-point property,"shapley's impossibility result indicates that the two-person bargaining problem has no non-trivial ordinal solution with the traditional game-theoretic bargaining model. although the result is no longer true for bargaining problems with more than two agents, none of the well known bargaining solutions are ordinal. searching for meaningful ordinal solutions, especially for the bilateral bargaining problem, has been a challenging issue in bargaining theory for more than three decades. this paper proposes a logic-based ordinal solution to the bilateral bargaining problem. we argue that if a bargaining problem is modeled in terms of the logical relation of players' physical negotiation items, a meaningful bargaining solution can be constructed based on the ordinal structure of bargainers' preferences. we represent bargainers' demands in propositional logic and bargainers' preferences over their demands in total preorder. we show that the solution satisfies most desirable logical properties, such as individual rationality (logical version), consistency, collective rationality as well as a few typical game-theoretic properties, such as weak pareto optimality and contraction invariance. in addition, if all players' demand sets are logically closed, the solution satisfies a fixed-point condition, which says that the outcome of a negotiation is the result of mutual belief revision. finally, we define various decision problems in relation to our bargaining model and study their computational complexity.",2008
and/or multi-valued decision diagrams (aomdds) for graphical models,"inspired by the recently introduced framework of and/or search spaces for graphical models, we propose to augment multi-valued decision diagrams (mdd) with and nodes, in order to capture function decomposition structure and to extend these compiled data structures to general weighted graphical models (e.g., probabilistic models). we present the and/or multi-valued decision diagram (aomdd) which compiles a graphical model into a canonical form that supports polynomial (e.g., solution counting, belief updating) or constant time (e.g. equivalence of graphical models) queries. we provide two algorithms for compiling the aomdd of a graphical model. the first is search-based, and works by applying reduction rules to the trace of the memory intensive and/or search algorithm. the second is inference-based and uses a bucket elimination schedule to combine the aomdds of the input functions via the the apply operator. for both algorithms, the compilation time and the size of the aomdd are, in the worst case, exponential in the treewidth of the graphical model, rather than pathwidth as is known for ordered binary decision diagrams (obdds). we introduce the concept of semantic treewidth, which helps explain why the size of a decision diagram is often much smaller than the worst case bound. we provide an experimental evaluation that demonstrates the potential of aomdds.",2008
a multiagent reinforcement learning algorithm with non-linear dynamics,"several multiagent reinforcement learning (marl) algorithms have been proposed to optimize agents' decisions. due to the complexity of the problem, the majority of the previously developed marl algorithms assumed agents either had some knowledge of the underlying game (such as nash equilibria) and/or observed other agents actions and the rewards they received. we introduce a new marl algorithm called the weighted policy learner (wpl), which allows agents to reach a nash equilibrium (ne) in benchmark 2-player-2-action games with minimum knowledge. using wpl, the only feedback an agent needs is its own local reward (the agent does not observe other agents actions or rewards). furthermore, wpl does not assume that agents know the underlying game or the corresponding nash equilibrium a priori. we experimentally show that our algorithm converges in benchmark two-player-two-action games. we also show that our algorithm converges in the challenging shapley's game where previous marl algorithms failed to converge without knowing the underlying game or the ne. furthermore, we show that wpl outperforms the state-of-the-art algorithms in a more realistic setting of 100 agents interacting and learning concurrently. an important aspect of understanding the behavior of a marl algorithm is analyzing the dynamics of the algorithm: how the policies of multiple learning agents evolve over time as agents interact with one another. such an analysis not only verifies whether agents using a given marl algorithm will eventually converge, but also reveals the behavior of the marl algorithm prior to convergence. we analyze our algorithm in two-player-two-action games and show that symbolically proving wpl's convergence is difficult, because of the non-linear nature of wpl's dynamics, unlike previous marl algorithms that had either linear or piece-wise-linear dynamics. instead, we numerically solve wpl's dynamics differential equations and compare the solution to the dynamics of previous marl algorithms.",2008
learning to reach agreement in a continuous ultimatum game,"it is well-known that acting in an individually rational manner, according to the principles of classical game theory, may lead to sub-optimal solutions in a class of problems named social dilemmas. in contrast, humans generally do not have much difficulty with social dilemmas, as they are able to balance personal benefit and group benefit. as agents in multi-agent systems are regularly confronted with social dilemmas, for instance in tasks such as resource allocation, these agents may benefit from the inclusion of mechanisms thought to facilitate human fairness. although many of such mechanisms have already been implemented in a multi-agent systems context, their application is usually limited to rather abstract social dilemmas with a discrete set of available strategies (usually two). given that many real-world examples of social dilemmas are actually continuous in nature, we extend this previous work to more general dilemmas, in which agents operate in a continuous strategy space. the social dilemma under study here is the well-known ultimatum game, in which an optimal solution is achieved if agents agree on a common strategy. we investigate whether a scale-free interaction network facilitates agents to reach agreement, especially in the presence of fixed-strategy agents that represent a desired (e.g. human) outcome. moreover, we study the influence of rewiring in the interaction network. the agents are equipped with continuous-action learning automata and play a large number of random pairwise games in order to establish a common strategy. from our experiments, we may conclude that results obtained in discrete-strategy games can be generalized to continuous-strategy games to a certain extent: a scale-free interaction network structure allows agents to achieve agreement on a common strategy, and rewiring in the interaction network greatly enhances the agents' ability to reach agreement. however, it also becomes clear that some alternative mechanisms, such as reputation and volunteering, have many subtleties involved and do not have convincing beneficial effects in the continuous case.",2008
on the value of correlation,"correlated equilibrium generalizes nash equilibrium to allow correlation devices. correlated equilibrium captures the idea that in many systems there exists a trusted administrator who can recommend behavior to a set of agents, but can not enforce such behavior. this makes this solution concept most appropriate to the study of multi-agent systems in ai. aumann showed an example of a game, and of a correlated equilibrium in this game in which the agents' welfare (expected sum of players' utilities) is greater than their welfare in all mixed-strategy equilibria. following the idea initiated by the price of anarchy literature this suggests the study of two major measures for the value of correlation in a game with nonnegative payoffs: 1. the ratio between the maximal welfare obtained in a correlated equilibrium to the maximal welfare obtained in a mixed-strategy equilibrium. we refer to this ratio as the mediation value. 2. the ratio between the maximal welfare to the maximal welfare obtained in a correlated equilibrium. we refer to this ratio as the enforcement value. in this work we initiate the study of the mediation and enforcement values, providing several general results on the value of correlation as captured by these concepts. we also present a set of results for the more specialized case of congestion games, a class of games that received a lot of attention in the recent literature.",2008
the latent relation mapping engine: algorithm and experiments,"many ai researchers and cognitive scientists have argued that analogy is the core of cognition. the most influential work on computational modeling of analogy-making is structure mapping theory (smt) and its implementation in the structure mapping engine (sme). a limitation of sme is the requirement for complex hand-coded representations. we introduce the latent relation mapping engine (lrme), which combines ideas from sme and latent relational analysis (lra) in order to remove the requirement for hand-coded representations. lrme builds analogical mappings between lists of words, using a large corpus of raw text to automatically discover the semantic relations among the words. we evaluate lrme on a set of twenty analogical mapping problems, ten based on scientific analogies and ten based on common metaphors. lrme achieves human-level performance on the twenty problems. we compare lrme with a variety of alternative approaches and find that they are not able to reach the same level of performance.",2008
cooperative search with concurrent interactions,"in this paper we show how taking advantage of autonomous agents' capability to maintain parallel interactions with others, and incorporating it into the cooperative economic search model results in a new search strategy which outperforms current strategies in use. as a framework for our analysis we use the electronic marketplace, where buyer agents have the incentive to search cooperatively. the new search technique is quite intuitive, however its analysis and the process of extracting the optimal search strategy are associated with several significant complexities. these difficulties are derived mainly from the unbounded search space and simultaneous dual affects of decisions taken along the search. we provide a comprehensive analysis of the model, highlighting, demonstrating and proving important characteristics of the optimal search strategy. consequently, we manage to come up with an efficient modular algorithm for extracting the optimal cooperative search strategy for any given environment. a computational based comparative illustration of the system performance using the new search technique versus the traditional methods is given, emphasizing the main differences in the optimal strategy's structure and the advantage of using the proposed model.",2008
extended rdf as a semantic foundation of rule markup languages,"ontologies and automated reasoning are the building blocks of the semantic web initiative. derivation rules can be included in an ontology to define derived concepts, based on base concepts. for example, rules allow to define the extension of a class or property, based on a complex relation between the extensions of the same or other classes and properties. on the other hand, the inclusion of negative information both in the form of negation-as-failure and explicit negative information is also needed to enable various forms of reasoning. in this paper, we extend rdf graphs with weak and strong negation, as well as derivation rules. the erdf stable model semantics of the extended framework (extended rdf) is defined, extending rdf(s) semantics. a distinctive feature of our theory, which is based on partial logic, is that both truth and falsity extensions of properties and classes are considered, allowing for truth value gaps. our framework supports both closed-world and open-world reasoning through the explicit representation of the particular closed-world assumptions and the erdf ontological categories of total properties and total classes.",2008
graphical model inference in optimal control of stochastic multi-agent systems,"in this article we consider the issue of optimal control in collaborative multi-agent systems with stochastic dynamics. the agents have a joint task in which they have to reach a number of target states. the dynamics of the agents contains additive control and additive noise, and the autonomous part factorizes over the agents. full observation of the global state is assumed. the goal is to minimize the accumulated joint cost, which consists of integrated instantaneous costs and a joint end cost. the joint end cost expresses the joint task of the agents. the instantaneous costs are quadratic in the control and factorize over the agents. the optimal control is given as a weighted linear combination of single-agent to single-target controls. the single-agent to single-target controls are expressed in terms of diffusion processes. these controls, when not closed form expressions, are formulated in terms of path integrals, which are calculated approximately by metropolis-hastings sampling. the weights in the control are interpreted as marginals of a joint distribution over agent to target assignments. the structure of the latter is represented by a graphical model, and the marginals are obtained by graphical model inference. exact inference of the graphical model will break down in large systems, and so approximate inference methods are needed. we use naive mean field approximation and belief propagation to approximate the optimal control in systems with linear dynamics. we compare the approximate inference methods with the exact solution, and we show that they can accurately compute the optimal control. finally, we demonstrate the control method in multi-agent systems with nonlinear dynamics consisting of up to 80 agents that have to reach an equal number of target states.",2008
a constraint programming approach for solving a queueing control problem,"in a facility with front room and back room operations, it is useful to switch workers between the rooms in order to cope with changing customer demand. assuming stochastic customer arrival and service times, we seek a policy for switching workers such that the expected customer waiting time is minimized while the expected back room staffing is sufficient to perform all work. three novel constraint programming models and several shaving procedures for these models are presented. experimental results show that a model based on closed-form expressions together with a combination of shaving procedures is the most efficient. this model is able to find and prove optimal solutions for many problem instances within a reasonable run-time. previously, the only available approach was a heuristic algorithm. furthermore, a hybrid method combining the heuristic and the best constraint programming method is shown to perform as well as the heuristic in terms of solution quality over time, while achieving the same performance in terms of proving optimality as the pure constraint programming model. this is the first work of which we are aware that solves such queueing-based problems with constraint programming.",2008
communication-based decomposition mechanisms for decentralized mdps,"multi-agent planning in stochastic environments can be framed formally as a decentralized markov decision problem. many real-life distributed problems that arise in manufacturing, multi-robot coordination and information gathering scenarios can be formalized using this framework. however, finding the optimal solution in the general case is hard, limiting the applicability of recently developed algorithms. this paper provides a practical approach for solving decentralized control problems when communication among the decision makers is possible, but costly. we develop the notion of communication-based mechanism that allows us to decompose a decentralized mdp into multiple single-agent problems. in this framework, referred to as decentralized semi-markov decision process with direct communication (dec-smdp-com), agents operate separately between communications. we show that finding an optimal mechanism is equivalent to solving optimally a dec-smdp-com. we also provide a heuristic search algorithm that converges on the optimal decomposition. restricting the decomposition to some specific types of local behaviors reduces significantly the complexity of planning. in particular, we present a polynomial-time algorithm for the case in which individual agents perform goal-oriented behaviors between communications. the paper concludes with an additional tractable algorithm that enables the introduction of human knowledge, thereby reducing the overall problem to finding the best time to communicate. empirical results show that these approaches provide good approximate solutions.",2008
new islands of tractability of cost-optimal planning,"we study the complexity of cost-optimal classical planning over propositional state variables and unary-effect actions. we discover novel problem fragments for which such optimization is tractable, and identify certain conditions that differentiate between tractable and intractable problems. these results are based on exploiting both structural and syntactic characteristics of planning problems. specifically, following brafman and domshlak (2003), we relate the complexity of planning and the topology of the causal graph. the main results correspond to tractability of cost-optimal planning for propositional problems with polytree causal graphs that either have o(1)-bounded in-degree, or are induced by actions having at most one prevail condition each. almost all our tractability results are based on a constructive proof technique that connects between certain tools from planning and tractable constraint optimization, and we believe this technique is of interest on its own due to a clear evidence for its robustness.",2008
optimal and approximate q-value functions for decentralized pomdps,"decision-theoretic planning is a popular approach to sequential decision making problems, because it treats uncertainty in sensing and acting in a principled way. in single-agent frameworks like mdps and pomdps, planning can be carried out by resorting to q-value functions: an optimal q-value function q* is computed in a recursive manner by dynamic programming, and then an optimal policy is extracted from q*. in this paper we study whether similar q-value functions can be defined for decentralized pomdp models (dec-pomdps), and how policies can be extracted from such value functions. we define two forms of the optimal q-value function for dec-pomdps: one that gives a normative description as the q-value function of an optimal pure joint policy and another one that is sequentially rational and thus gives a recipe for computation. this computation, however, is infeasible for all but the smallest problems. therefore, we analyze various approximate q-value functions that allow for efficient computation. we describe how they relate, and we prove that they all provide an upper bound to the optimal q-value function q*. finally, unifying some previous approaches for solving dec-pomdps, we describe a family of algorithms for extracting policies from such q-value functions, and perform an experimental evaluation on existing test problems, including a new firefighting benchmark problem.",2008
spectrum of variable-random trees,"in this paper, we show that a continuous spectrum of randomisation exists, in which most existing tree randomisations are only operating around the two ends of the spectrum. that leaves a huge part of the spectrum largely unexplored. we propose a base learner vr-tree which generates trees with variable-randomness. vr-trees are able to span from the conventional deterministic trees to the complete-random trees using a probabilistic parameter. using vr-trees as the base models, we explore the entire spectrum of randomised ensembles, together with bagging and random subspace. we discover that the two halves of the spectrum have their distinct characteristics; and the understanding of which allows us to propose a new approach in building better decision tree ensembles. we name this approach coalescence, which coalesces a number of points in the random-half of the spectrum. coalescence acts as a committee of ``experts'' to cater for unforeseeable conditions presented in training data. coalescence is found to perform better than any single operating point in the spectrum, without the need to tune to a specific level of randomness. in our empirical study, coalescence ranks top among the benchmarking ensemble methods including random forests, random subspace and c5 boosting; and only coalescence is significantly better than bagging and max-diverse ensemble among all the methods in the comparison. although coalescence is not significantly better than random forests, we have identified conditions under which one will perform better than the other.",2008
on the qualitative comparison of decisions having positive and negative features,"making a decision is often a matter of listing and comparing positive and negative arguments. in such cases, the evaluation scale for decisions should be considered bipolar, that is, negative and positive values should be explicitly distinguished. that is what is done, for example, in cumulative prospect theory. however, contraryto the latter framework that presupposes genuine numerical assessments, human agents often decide on the basis of an ordinal ranking of the pros and the cons, and by focusing on the most salient arguments. in other terms, the decision process is qualitative as well as bipolar. in this article, based on a bipolar extension of possibility theory, we define and axiomatically characterize several decision rules tailored for the joint handling of positive and negative arguments in an ordinal setting. the simplest rules can be viewed as extensions of the maximin and maximax criteria to the bipolar case, and consequently suffer from poor decisive power. more decisive rules that refine the former are also proposed. these refinements agree both with principles of efficiency and with the spirit of order-of-magnitude reasoning, that prevails in qualitative decision theory. the most refined decision rule uses leximin rankings of the pros and the cons, and the ideas of counting arguments of equal strength and cancelling pros by cons. it is shown to come down to a special case of cumulative prospect theory, and to subsume the ``take the best'' heuristic studied by cognitive psychologists.",2008
dynamic control in real-time heuristic search,"real-time heuristic search is a challenging type of agent-centered search because the agent's planning time per action is bounded by a constant independent of problem size. a common problem that imposes such restrictions is pathfinding in modern computer games where a large number of units must plan their paths simultaneously over large maps. common search algorithms (e.g., a*, ida*, d*, ara*, ad*) are inherently not real-time and may lose completeness when a constant bound is imposed on per-action planning time. real-time search algorithms retain completeness but frequently produce unacceptably suboptimal solutions. in this paper, we extend classic and modern real-time search algorithms with an automated mechanism for dynamic depth and subgoal selection. the new algorithms remain real-time and complete. on large computer game maps, they find paths within 7% of optimal while on average expanding roughly a single state per action. this is nearly a three-fold improvement in suboptimality over the existing state-of-the-art algorithms and, at the same time, a 15-fold improvement in the amount of planning per action.",2008
adaptive stochastic resource control: a machine learning approach,"the paper investigates stochastic resource allocation problems with scarce, reusable resources and non-preemtive, time-dependent, interconnected tasks. this approach is a natural generalization of several standard resource management problems, such as scheduling and transportation problems. first, reactive solutions are considered and defined as control policies of suitably reformulated markov decision processes (mdps). we argue that this reformulation has several favorable properties, such as it has finite state and action spaces, it is aperiodic, hence all policies are proper and the space of control policies can be safely restricted. next, approximate dynamic programming (adp) methods, such as fitted q-learning, are suggested for computing an efficient control policy. in order to compactly maintain the cost-to-go function, two representations are studied: hash tables and support vector regression (svr), particularly, nu-svrs. several additional improvements, such as the application of limited-lookahead rollout algorithms in the initial phases, action space decomposition, task clustering and distributed sampling are investigated, too. finally, experimental results on both benchmark and industry-related data are presented.",2008
refining the execution of abstract actions with learned action models,"robots reason about abstract actions, such as ""go to position `l'"", in order to decide what to do or to generate plans for their intended course of action. the use of abstract actions enables robots to employ small action libraries, which reduces the search space for decision making. when executing the actions, however, the robot must tailor the abstract actions to the specific task and situation context at hand. in this article we propose a novel robot action execution system that learns success and performance models for possible specializations of abstract actions. at execution time, the robot uses these models to optimize the execution of abstract actions to the respective task contexts. the robot can so use abstract actions for efficient reasoning, without compromising the performance of action execution. we show the impact of our action execution model in three robotic domains and on two kinds of action execution problems: (1) the instantiation of free action parameters to optimize the expected performance of action sequences; (2) the automatic introduction of additional subgoals to make action sequences more reliable.",2008
efficiency and envy-freeness in fair division of indivisible goods: logical representation and complexity,"we consider the problem of allocating fairly a set of indivisible goods among agents from the point of view of compact representation and computational complexity. we start by assuming that agents have dichotomous preferences expressed by propositional formulae. we express efficiency and envy-freeness in a logical setting, which reveals unexpected connections to nonmonotonic reasoning. then we identify the complexity of determining whether there exists an efficient and envy-free allocation, for several notions of efficiency, when preferences are represented in a succinct way (as well as restrictions of this problem). we first study the problem under the assumption that preferences are dichotomous, and then in the general case.",2008
satzilla: portfolio-based algorithm selection for sat,"it has been widely observed that there is no single ""dominant"" sat solver; instead, different solvers perform best on different instances. rather than following the traditional approach of choosing the best solver for a given class of instances, we advocate making this decision online on a per-instance basis. building on previous work, we describe satzilla, an automated approach for constructing per-instance algorithm portfolios for sat that use so-called empirical hardness models to choose among their constituent solvers. this approach takes as input a distribution of problem instances and a set of component solvers, and constructs a portfolio optimizing a given objective function (such as mean runtime, percent of instances solved, or score in a competition). the excellent performance of satzilla was independently verified in the 2007 sat competition, where our satzilla07 solvers won three gold, one silver and one bronze medal. in this article, we go well beyond satzilla07 by making the portfolio construction scalable and completely automated, and improving it by integrating local search solvers as candidate solvers, by predicting performance score instead of runtime, and by using hierarchical hardness models that take into account different types of sat instances. we demonstrate the effectiveness of these new techniques in extensive experimental results on data sets including instances from the most recent sat competition.",2008
"a unifying framework for structural properties of csps: definitions, complexity, tractability","literature on constraint satisfaction exhibits the definition of several ``structural'' properties that can be possessed by csps, like (in)consistency, substitutability or interchangeability. current tools for constraint solving typically detect such properties efficiently by means of incomplete yet effective algorithms, and use them to reduce the search space and boost search. in this paper, we provide a unifying framework encompassing most of the properties known so far, both in csp and other fields' literature, and shed light on the semantical relationships among them. this gives a unified and comprehensive view of the topic, allows new, unknown, properties to emerge, and clarifies the computational complexity of the various detection problems. in particular, among the others, two new concepts, fixability and removability emerge, that come out to be the ideal characterisations of values that may be safely assigned or removed from a variable's domain, while preserving problem satisfiability. these two notions subsume a large number of known properties, including inconsistency, substitutability and others. because of the computational intractability of all the property-detection problems, by following the csp approach we then determine a number of relaxations which provide sufficient conditions for their tractability. in particular, we exploit forms of language restrictions and local reasoning.",2008
a general theory of additive state space abstractions,"informally, a set of abstractions of a state space s is additive if the distance between any two states in s is always greater than or equal to the sum of the corresponding distances in the abstract spaces. the first known additive abstractions, called disjoint pattern databases, were experimentally demonstrated to produce state of the art performance on certain state spaces. however, previous applications were restricted to state spaces with special properties, which precludes disjoint pattern databases from being defined for several commonly used testbeds, such as rubik's cube, topspin and the pancake puzzle. in this paper we give a general definition of additive abstractions that can be applied to any state space and prove that heuristics based on additive abstractions are consistent as well as admissible. we use this new definition to create additive abstractions for these testbeds and show experimentally that well chosen additive abstractions can reduce search time substantially for the (18,4)-topspin puzzle and by three orders of magnitude over state of the art methods for the 17-pancake puzzle. we also derive a way of testing if the heuristic value returned by additive abstractions is provably too low and show that the use of this test can reduce search time for the 15-puzzle and topspin by roughly a factor of two.",2008
online planning algorithms for pomdps,"partially observable markov decision processes (pomdps) provide a rich framework for sequential decision-making under uncertainty in stochastic domains. however, solving a pomdp is often intractable except for small problems due to their complexity. here, we focus on online approaches that alleviate the computational complexity by computing good local policies at each decision step during the execution. online algorithms generally consist of a lookahead search to find the best action to execute at each time step in an environment. our objectives here are to survey the various existing online pomdp methods, analyze their properties and discuss their advantages and disadvantages; and to thoroughly evaluate these online approaches in different environments under various metrics (return, error bound reduction, lower bound improvement). our experimental results indicate that state-of-the-art online heuristic search methods can handle large pomdp domains efficiently.",2008
m-dpop: faithful distributed implementation of efficient social choice problems,"in the efficient social choice problem, the goal is to assign values, subject to side constraints, to a set of variables to maximize the total utility across a population of agents, where each agent has private information about its utility function. in this paper we model the social choice problem as a distributed constraint optimization problem (dcop), in which each agent can communicate with other agents that share an interest in one or more variables. whereas existing dcop algorithms can be easily manipulated by an agent, either by misreporting private information or deviating from the algorithm, we introduce m-dpop, the first dcop algorithm that provides a faithful distributed implementation for efficient social choice. this provides a concrete example of how the methods of mechanism design can be unified with those of distributed optimization. faithfulness ensures that no agent can benefit by unilaterally deviating from any aspect of the protocol, neither information-revelation, computation, nor communication, and whatever the private information of other agents. we allow for payments by agents to a central bank, which is the only central authoritythat we require. to achieve faithfulness, we carefully integrate the vickrey-clarke-groves (vcg) mechanism with the dpop algorithm, such that each agent is only asked to perform computation, report information, and send messages that is in its own best interest. determining agent i's payment requires solving the social choice problem without agent i. here, we present a method to reuse computation performed in solving the main problem in a way that is robust against manipulation by the excluded agent. experimental results on structured problems show that as much as 87% of the computation required for solving the marginal problems can be avoided by re-use, providing very good scalability in the number of agents. on unstructured problems, we observe a sensitivity of m-dpop to the density of the problem, and we show that reusability decreases from almost 100% for very sparse problems to around 20% for highly connected problems. we close with a discussion of the features of dcop that enable faithful implementations in this problem, the challenge of reusing computation from the main problem to marginal problems in other algorithms such as adopt and optapo, and the prospect of methods to avoid the welfare loss that can occur because of the transfer of payments to the bank.",2008
compositional belief update,"in this paper we explore a class of belief update operators, in which the definition of the operator is compositional with respect to the sentence to be added. the goal is to provide an update operator that is intuitive, in that its definition is based on a recursive decomposition of the update sentence's structure, and that may be reasonably implemented. in addressing update, we first provide a definition phrased in terms of the models of a knowledge base. while this operator satisfies a core group of the benchmark katsuno-mendelzon update postulates, not all of the postulates are satisfied. other katsuno-mendelzon postulates can be obtained by suitably restricting the syntactic form of the sentence for update, as we show. in restricting the syntactic form of the sentence for update, we also obtain a hierarchy of update operators with winslett's standard semantics as the most basic interesting approach captured. we subsequently give an algorithm which captures this approach; in the general case the algorithm is exponential, but with some not-unreasonable assumptions we obtain an algorithm that is linear in the size of the knowledge base. hence the resulting approach has much better complexity characteristics than other operators in some situations. we also explore other compositional belief change operators: erasure is developed as a dual operator to update; we show that a forget operator is definable in terms of update; and we give a definition of the compositional revision operator. we obtain that compositional revision, under the most natural definition, yields the satoh revision operator.",2008
"analogical dissimilarity: definition, algorithms and two experiments in machine learning","this paper defines the notion of analogical dissimilarity between four objects, with a special focus on objects structured as sequences. firstly, it studies the case where the four objects have a null analogical dissimilarity, i.e. are in analogical proportion. secondly, when one of these objects is unknown, it gives algorithms to compute it. thirdly, it tackles the problem of defining analogical dissimilarity, which is a measure of how far four objects are from being in analogical proportion. in particular, when objects are sequences, it gives a definition and an algorithm based on an optimal alignment of the four sequences. it gives also learning algorithms, i.e. methods to find the triple of objects in a learning sample which has the least analogical dissimilarity with a given object. two practical experiments are described: the first is a classification problem on benchmarks of binary and nominal data, the second shows how the generation of sequences by solving analogical equations enables a handwritten character recognition system to rapidly be adapted to a new writer.",2008
qualitative system identification from imperfect data,"experience in the physical sciences suggests that the only realistic means of understanding complex systems is through the use of mathematical models. typically, this has come to mean the identification of quantitative models expressed as differential equations. quantitative modelling works best when the structure of the model (i.e., the form of the equations) is known; and the primary concern is one of estimating the values of the parameters in the model. for complex biological systems, the model-structure is rarely known and the modeler has to deal with both model-identification and parameter-estimation. in this paper we are concerned with providing automated assistance to the first of these problems. specifically, we examine the identification by machine of the structural relationships between experimentally observed variables. these relationship will be expressed in the form of qualitative abstractions of a quantitative model. such qualitative models may not only provide clues to the precise quantitative model, but also assist in understanding the essence of that model. our position in this paper is that background knowledge incorporating system modelling principles can be used to constrain effectively the set of good qualitative models. utilising the model-identification framework provided by inductive logic programming (ilp) we present empirical support for this position using a series of increasingly complex artificial datasets. the results are obtained with qualitative and quantitative data subject to varying amounts of noise and different degrees of sparsity. the results also point to the presence of a set of qualitative states, which we term kernel subsets, that may be necessary for a qualitative model-learner to learn correct models. we demonstrate scalability of the method to biological system modelling by identification of the glycolysis metabolic pathway from data.",2008
latent tree models and approximate inference in bayesian networks,"we propose a novel method for approximate inference in bayesian networks (bns). the idea is to sample data from a bn, learn a latent tree model (ltm) from the data offline, and when online, make inference with the ltm instead of the original bn. because ltms are tree-structured, inference takes linear time. in the meantime, they can represent complex relationship among leaf nodes and hence the approximation accuracy is often good. empirical evidence shows that our method can achieve good approximation accuracy at low online computational cost.",2008
the ultrametric constraint and its application to phylogenetics,"a phylogenetic tree shows the evolutionary relationships among species. internal nodes of the tree represent speciation events and leaf nodes correspond to species. a goal of phylogenetics is to combine such trees into larger trees, called supertrees, whilst respecting the relationships in the original trees. a rooted tree exhibits an ultrametric property; that is, for any three leaves of the tree it must be that one pair has a deeper most recent common ancestor than the other pairs, or that all three have the same most recent common ancestor. this inspires a constraint programming encoding for rooted trees. we present an efficient constraint that enforces the ultrametric property over a symmetric array of constrained integer variables, with the inevitable property that the lower bounds of any three variables are mutually supportive. we show that this allows an efficient constraint-based solution to the supertree construction problem. we demonstrate that the versatility of constraint programming can be exploited to allow solutions to variants of the supertree construction problem.",2008
optimal strategies for simultaneous vickrey auctions with perfect substitutes,"we derive optimal strategies for a bidding agent that participates in multiple, simultaneous second-price auctions with perfect substitutes. we prove that, if everyone else bids locally in a single auction, the global bidder should always place non-zero bids in all available auctions, provided there are no budget constraints. with a budget, however, the optimal strategy is to bid locally if this budget is equal or less than the valuation. furthermore, for a wide range of valuation distributions, we prove that the problem of finding the optimal bids reduces to two dimensions if all auctions are identical. finally, we address markets with both sequential and simultaneous auctions, non-identical auctions, and the allocative efficiency of the market.",2008
minimaxsat: an efficient weighted max-sat solver,"in this paper we introduce minimaxsat, a new max-sat solver that is built on top of minisat+. it incorporates the best current sat and max-sat techniques. it can handle hard clauses(clauses of mandatory satisfaction as in sat), soft clauses (clauses whose falsification is penalized by a cost as in max-sat) as well as pseudo-boolean objective functions and constraints. its main features are: learning and backjumping on hard clauses; resolution-based and substraction-based lower bounding; and lazy propagation with the two-watched literal scheme. our empirical evaluation comparing a wide set of solving alternatives on a broad set of optimization benchmarks indicates that the performance of minimaxsat is usually close to the best specialized alternative and, in some cases, even better.",2008
planning with durative actions in stochastic domains,"probabilistic planning problems are typically modeled as a markov decision process (mdp). mdps, while an otherwise expressive model, allow only for sequential, non-durative actions. this poses severe restrictions in modeling and solving a real world planning problem. we extend the mdp model to incorporate 1) simultaneous action execution, 2) durative actions, and 3) stochastic durations. we develop several algorithms to combat the computational explosion introduced by these features. the key theoretical ideas used in building these algorithms are -- modeling a complex problem as an mdp in extended state/action space, pruning of irrelevant actions, sampling of relevant actions, using informed heuristics to guide the search, hybridizing different planners to achieve benefits of both, approximating the problem and replanning. our empirical evaluation illuminates the different merits in using various algorithms, viz., optimality, empirical closeness to optimality, theoretical error bounds, and speed.",2008
cui networks: a graphical representation for conditional utility independence,"we introduce cui networks, a compact graphical representation of utility functions over multiple attributes. cui networks model multiattribute utility functions using the well-studied and widely applicable utility independence concept. we show how conditional utility independence leads to an effective functional decomposition that can be exhibited graphically, and how local, compact data at the graph nodes can be used to calculate joint utility. we discuss aspects of elicitation, network construction, and optimization, and contrast our new representation with previous graphical preference modeling.",2008
ctl model update for system modifications,"model checking is a promising technology, which has been applied for verification of many hardware and software systems. in this paper, we introduce the concept of model update towards the development of an automatic system modification tool that extends model checking functions. we define primitive update operations on the models of computation tree logic (ctl) and formalize the principle of minimal change for ctl model update. these primitive update operations, together with the underlying minimal change principle, serve as the foundation for ctl model update. essential semantic and computational characterizations are provided for our ctl model update approach. we then describe a formal algorithm that implements this approach. we also illustrate two case studies of ctl model updates for the well-known microwave oven example and the andrew file system 1, from which we further propose a method to optimize the update results in complex system modifications.",2008
conjunctive query answering for the description logic shiq,"conjunctive queries play an important role as an expressive query language for description logics (dls). although modern dls usually provide for transitive roles, conjunctive query answering over dl knowledge bases is only poorly understood if transitive roles are admitted in the query. in this paper, we consider unions of conjunctive queries over knowledge bases formulated in the prominent dl shiq and allow transitive roles in both the query and the knowledge base. we show decidability of query answering in this setting and establish two tight complexity bounds: regarding combined complexity, we prove that there is a deterministic algorithm for query answering that needs time single exponential in the size of the kb and double exponential in the size of the query, which is optimal. regarding data complexity, we prove containment in co-np.",2008
sound and complete inference rules for se-consequence,"the notion of strong equivalence on logic programs with answer set semantics gives rise to a consequence relation on logic program rules, called se-consequence. we present a sound and complete set of inference rules for se-consequence on disjunctive logic programs.",2008
loosely coupled formulations for automated planning: an integer programming perspective,"we represent planning as a set of loosely coupled network flow problems, where each network corresponds to one of the state variables in the planning domain. the network nodes correspond to the state variable values and the network arcs correspond to the value transitions. the planning problem is to find a path (a sequence of actions) in each network such that, when merged, they constitute a feasible plan. in this paper we present a number of integer programming formulations that model these loosely coupled networks with varying degrees of flexibility. since merging may introduce exponentially many ordering constraints we implement a so-called branch-and-cut algorithm, in which these constraints are dynamically generated and added to the formulation when needed. our results are very promising, they improve upon previous planning as integer programming approaches and lay the foundation for integer programming approaches for cost optimal planning.",2008
on the expressiveness of levesque's normal form,"levesque proposed a generalization of a database called a proper knowledge base (kb), which is equivalent to a possibly infinite consistent set of ground literals. in contrast to databases, proper kbs do not make the closed-world assumption and hence the entailment problem becomes undecidable. levesque then proposed a limited but efficient inference method v for proper kbs, which is sound and, when the query is in a certain normal form, also logically complete. he conjectured that for every first-order query there is an equivalent one in normal form. in this note, we show that this conjecture is false. in fact, we show that any class of formulas for which v is complete must be strictly less expressive than full first-order logic. moreover, in the propositional case it is very unlikely that a formula always has a polynomial-size normal form.",2008
modular reuse of ontologies: theory and practice,"in this paper, we propose a set of tasks that are relevant for the modular reuse of ontologies. in order to formalize these tasks as reasoning problems, we introduce the notions of conservative extension, safety and module for a very general class of logic-based ontology languages. we investigate the general properties of and relationships between these notions and study the relationships between the relevant reasoning problems we have previously identified. to study the computability of these problems, we consider, in particular, description logics (dls), which provide the formal underpinning of the w3c web ontology language (owl), and show that all the problems we consider are undecidable or algorithmically unsolvable for the description logic underlying owl dl. in order to achieve a practical solution, we identify conditions sufficient for an ontology to reuse a set of symbols ``safely''---that is, without changing their meaning. we provide the notion of a safety class, which characterizes any sufficient condition for safety, and identify a family of safety classes--called locality---which enjoys a collection of desirable properties. we use the notion of a safety class to extract modules from ontologies, and we provide various modularization algorithms that are appropriate to the properties of the particular safety class in use. finally, we show practical benefits of our safety checking and module extraction algorithms.",2008
the complexity of planning problems with simple causal graphs,"we present three new complexity results for classes of planning problems with simple causal graphs. first, we describe a polynomial-time algorithm that uses macros to generate plans for the class 3s of planning problems with binary state variables and acyclic causal graphs. this implies that plan generation may be tractable even when a planning problem has an exponentially long minimal solution. we also prove that the problem of plan existence for planning problems with multi-valued variables and chain causal graphs is np-hard. finally, we show that plan existence for planning problems with binary state variables and polytree causal graphs is np-complete.",2008
gesture salience as a hidden variable for coreference resolution and keyframe extraction,"gesture is a non-verbal modality that can contribute crucial information to the understanding of natural language. but not all gestures are informative, and non-communicative hand motions may confuse natural language processing (nlp) and impede learning. people have little diffculty ignoring irrelevant hand movements and focusing on meaningful gestures, suggesting that an automatic system could also be trained to perform this task. however, the informativeness of a gesture is context-dependent and labeling enough data to cover all cases would be expensive. we present conditional modality fusion, a conditional hidden-variable model that learns to predict which gestures are salient for coreference resolution, the task of determining whether two noun phrases refer to the same semantic entity. moreover, our approach uses only coreference annotations, and not annotations of gesture salience itself. we show that gesture features improve performance on coreference resolution, and that by attending only to gestures that are salient, our method achieves further significant gains. in addition, we show that the model of gesture salience learned in the context of coreference accords with human intuition, by demonstrating that gestures judged to be salient by our model can be used successfully to create multimedia keyframe summaries of video. these summaries are similar to those created by human raters, and significantly outperform summaries produced by baselines from the literature.",2008
global inference for sentence compression: an integer linear programming approach,sentence compression holds promise for many applications ranging from summarization to subtitle generation. our work views sentence compression as an optimization problem and uses integer linear programming (ilp) to infer globally optimal compressions in the presence of linguistically motivated constraints. we show how previous formulations of sentence compression can be recast as ilps and extend these models with novel global constraints. experimental results on written and spoken texts demonstrate improvements over state-of-the-art models.,2008
first order decision diagrams for relational mdps,"markov decision processes capture sequential decision making under uncertainty, where an agent must choose actions so as to optimize long term reward. the paper studies efficient reasoning mechanisms for relational markov decision processes (rmdp) where world states have an internal relational structure that can be naturally described in terms of objects and relations among them. two contributions are presented. first, the paper develops first order decision diagrams (fodd), a new compact representation for functions over relational structures, together with a set of operators to combine fodds, and novel reduction techniques to keep the representation small. second, the paper shows how fodds can be used to develop solutions for rmdps, where reasoning is performed at the abstract level and the resulting optimal policy is independent of domain size (number of objects) or instantiation. in particular, a variant of the value iteration algorithm is developed by using special operations over fodds, and the algorithm is shown to converge to the optimal policy.",2008
axiomatic foundations for ranking systems,"reasoning about agent preferences on a set of alternatives, and the aggregation of such preferences into some social ranking is a fundamental issue in reasoning about multi-agent systems. when the set of agents and the set of alternatives coincide, we get the ranking systems setting. a famous type of ranking systems are page ranking systems in the context of search engines. in this paper we present an extensive axiomatic study of ranking systems. in particular, we consider two fundamental axioms: transitivity, and ranked independence of irrelevant alternatives. surprisingly, we find that there is no general social ranking rule that satisfies both requirements. furthermore, we show that our impossibility result holds under various restrictions on the class of ranking problems considered. however, when transitivity is weakened, an interesting possibility result is obtained. in addition, we show a complete axiomatization of approval voting using ranked iia.",2008
exploiting subgraph structure in multi-robot path planning,"multi-robot path planning is difficult due to the combinatorial explosion of the search space with every new robot added. complete search of the combined state-space soon becomes intractable. in this paper we present a novel form of abstraction that allows us to plan much more efficiently. the key to this abstraction is the partitioning of the map into subgraphs of known structure with entry and exit restrictions which we can represent compactly. planning then becomes a search in the much smaller space of subgraph configurations. once an abstract plan is found, it can be quickly resolved into a correct (but possibly sub-optimal) concrete plan without the need for further search. we prove that this technique is sound and complete and demonstrate its practical effectiveness on a real map. a contending solution, prioritised planning, is also evaluated and shown to have similar performance albeit at the cost of completeness. the two approaches are not necessarily conflicting; we demonstrate how they can be combined into a single algorithm which outperforms either approach alone.",2008
creating relational data from unstructured and ungrammatical data sources,"in order for agents to act on behalf of users, they will have to retrieve and integrate vast amounts of textual data on the world wide web. however, much of the useful data on the web is neither grammatical nor formally structured, making querying difficult. examples of these types of data sources are online classifieds like craigslist and auction item listings like ebay. we call this unstructured, ungrammatical data ""posts."" the unstructured nature of posts makes query and integration difficult because the attributes are embedded within the text. also, these attributes do not conform to standardized values, which prevents queries based on a common attribute value. the schema is unknown and the values may vary dramatically making accurate search difficult. creating relational data for easy querying requires that we define a schema for the embedded attributes and extract values from the posts while standardizing these values. traditional information extraction (ie) is inadequate to perform this task because it relies on clues from the data, such as structure or natural language, neither of which are found in posts. furthermore, traditional information extraction does not incorporate data cleaning, which is necessary to accurately query and integrate the source. the two-step approach described in this paper creates relational data sets from unstructured and ungrammatical text by addressing both issues. to do this, we require a set of known entities called a ""reference set."" the first step aligns each post to each member of each reference set. this allows our algorithm to define a schema over the post and include standard values for the attributes defined by this schema. the second step performs information extraction for the attributes, including attributes not easily represented by reference sets, such as a price. in this manner we create a relational structure over previously unstructured data, supporting deep and accurate queries over the data as well as standard values for integration. our experimental results show that our technique matches the posts to the reference set accurately and efficiently and outperforms state-of-the-art extraction systems on the extraction task from posts.",2008
a multiagent approach to autonomous intersection management,"artificial intelligence research is ushering in a new era of sophisticated, mass-market transportation technology. while computers can already fly a passenger jet better than a trained human pilot, people are still faced with the dangerous yet tedious task of driving automobiles. intelligent transportation systems (its) is the field that focuses on integrating information technology with vehicles and transportation infrastructure to make transportation safer, cheaper, and more efficient. recent advances in its point to a future in which vehicles themselves handle the vast majority of the driving task. once autonomous vehicles become popular, autonomous interactions amongst multiple vehicles will be possible. current methods of vehicle coordination, which are all designed to work with human drivers, will be outdated. the bottleneck for roadway efficiency will no longer be the drivers, but rather the mechanism by which those drivers' actions are coordinated. while open-road driving is a well-studied and more-or-less-solved problem, urban traffic scenarios, especially intersections, are much more challenging. we believe current methods for controlling traffic, specifically at intersections, will not be able to take advantage of the increased sensitivity and precision of autonomous vehicles as compared to human drivers. in this article, we suggest an alternative mechanism for coordinating the movement of autonomous vehicles through intersections. drivers and intersections in this mechanism are treated as autonomous agents in a multiagent system. in this multiagent system, intersections use a new reservation-based approach built around a detailed communication protocol, which we also present. we demonstrate in simulation that our new mechanism has the potential to significantly outperform current intersection control technology -- traffic lights and stop signs. because our mechanism can emulate a traffic light or stop sign, it subsumes the most popular current methods of intersection control. this article also presents two extensions to the mechanism. the first extension allows the system to control human-driven vehicles in addition to autonomous vehicles. the second gives priority to emergency vehicles without significant cost to civilian vehicles. the mechanism, including both extensions, is implemented and tested in simulation, and we present experimental results that strongly attest to the efficacy of this approach.",2008
